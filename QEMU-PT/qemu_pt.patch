diff --new-file -ur qemu/accel/kvm/kvm-all.c QEMU-PT/accel/kvm/kvm-all.c
--- qemu/accel/kvm/kvm-all.c	2021-08-24 13:01:59.000000000 +0200
+++ QEMU-PT/accel/kvm/kvm-all.c	2021-08-24 21:54:55.938586449 +0200
@@ -47,6 +47,17 @@
 
 #include "hw/boards.h"
 
+#ifdef CONFIG_PROCESSOR_TRACE
+#include "pt.h"
+#include "pt/hypercall.h"
+#include "pt/nested_hypercalls.h"
+#include "pt/synchronization.h"
+#include "pt/debug.h"
+#include "pt/state.h"
+#include "pt/interface.h"
+
+#endif
+
 /* This check must be after config-host.h is included */
 #ifdef CONFIG_EVENTFD
 #include <sys/eventfd.h>
@@ -390,6 +401,11 @@
     cpu->kvm_state = s;
     cpu->vcpu_dirty = true;
 
+#ifdef CONFIG_PROCESSOR_TRACE
+    pt_kvm_init(cpu);
+    install_timeout_detector(&GET_GLOBAL_STATE()->timeout_detector);
+#endif
+
     mmap_size = kvm_ioctl(s, KVM_GET_VCPU_MMAP_SIZE, 0);
     if (mmap_size < 0) {
         ret = mmap_size;
@@ -411,6 +427,11 @@
     }
 
     ret = kvm_arch_init_vcpu(cpu);
+    
+#ifdef CONFIG_PROCESSOR_TRACE
+    unblock_signals();
+#endif
+
 err:
     return ret;
 }
@@ -1892,7 +1913,7 @@
 #endif
     QLIST_INIT(&s->kvm_parked_vcpus);
     s->vmfd = -1;
-    s->fd = qemu_open("/dev/kvm", O_RDWR);
+    s->fd = qemu_open("/dev/kvm-pt", O_RDWR);
     if (s->fd == -1) {
         fprintf(stderr, "Could not access KVM kernel module: %m\n");
         ret = -errno;
@@ -2298,6 +2319,7 @@
 
 int kvm_cpu_exec(CPUState *cpu)
 {
+    static bool timeout_reload_pending = false;
     struct kvm_run *run = cpu->kvm_run;
     int ret, run_ret;
 
@@ -2311,6 +2333,12 @@
     qemu_mutex_unlock_iothread();
     cpu_exec_start(cpu);
 
+    if(timeout_reload_pending){
+          synchronization_lock_timeout_found();
+    }
+
+    timeout_reload_pending = false;
+
     do {
         MemTxAttrs attrs;
 
@@ -2330,14 +2358,26 @@
             kvm_cpu_kick_self();
         }
 
+
+#ifdef CONFIG_PROCESSOR_TRACE
+        pt_pre_kvm_run(cpu);
+#endif
+
         /* Read cpu->exit_request before KVM_RUN reads run->immediate_exit.
          * Matching barrier in kvm_eat_signals.
          */
         smp_rmb();
 
+        if(arm_sigprof_timer(&GET_GLOBAL_STATE()->timeout_detector)){
+            assert(false);
+        }
         run_ret = kvm_vcpu_ioctl(cpu, KVM_RUN, 0);
+        if (disarm_sigprof_timer(&GET_GLOBAL_STATE()->timeout_detector)){
+            timeout_reload_pending = true; 
 
+        }
         attrs = kvm_arch_post_run(cpu, run);
+        pt_post_kvm_run(cpu);                                                                                                                                            
 
 #ifdef KVM_HAVE_MCE_INJECTION
         if (unlikely(have_sigbus_pending)) {
@@ -2356,8 +2396,19 @@
                 ret = EXCP_INTERRUPT;
                 break;
             }
-            fprintf(stderr, "error: kvm run failed %s\n",
+
+            if(run_ret == -EFAULT){
+                if(GET_GLOBAL_STATE()->protect_payload_buffer && GET_GLOBAL_STATE()->in_fuzzing_mode){
+                    /* Fuzzing is enabled at this point -> don't exit */
+                    synchronization_payload_buffer_write_detected();
+                    ret = 0;
+                    break;
+                }
+            }
+
+            fprintf(stderr, "QEMU-PT: error: kvm run failed %s\n",
                     strerror(-run_ret));
+            qemu_backtrace();
 #ifdef TARGET_PPC
             if (run_ret == -EBUSY) {
                 fprintf(stderr,
@@ -2398,33 +2449,226 @@
             break;
         case KVM_EXIT_SHUTDOWN:
             DPRINTF("shutdown\n");
-            qemu_system_reset_request(SHUTDOWN_CAUSE_GUEST_RESET);
-            ret = EXCP_INTERRUPT;
+            fprintf(stderr, "ATTEMPT TO SHUTDOWN MACHINE (KVM_EXIT_SHUTDOWN)!\n");
+            if(GET_GLOBAL_STATE()->in_fuzzing_mode){
+                //timeout_reload_pending = false;
+                /* Fuzzing is enabled at this point -> don't exit */
+                handle_hypercall_kafl_release(run, cpu);
+                ret = 0;
+            }
+            else{
+                qemu_system_reset_request(SHUTDOWN_CAUSE_GUEST_RESET);
+                ret = EXCP_INTERRUPT;
+            }
             break;
         case KVM_EXIT_UNKNOWN:
             fprintf(stderr, "KVM: unknown exit, hardware reason %" PRIx64 "\n",
                     (uint64_t)run->hw.hardware_exit_reason);
+            assert(false);
             ret = -1;
             break;
         case KVM_EXIT_INTERNAL_ERROR:
             ret = kvm_handle_internal_error(cpu, run);
             break;
+#ifdef CONFIG_PROCESSOR_TRACE
+        case KVM_EXIT_KAFL_ACQUIRE:
+            handle_hypercall_kafl_acquire(run, cpu);
+            ret = 0;
+            break;
+        case KVM_EXIT_KAFL_GET_PAYLOAD:
+            handle_hypercall_get_payload(run, cpu);
+            ret = 0;
+            break;
+        case KVM_EXIT_KAFL_GET_PROGRAM:
+            handle_hypercall_get_program(run, cpu);
+            ret = 0;
+            break;
+        case KVM_EXIT_KAFL_RELEASE:
+            handle_hypercall_kafl_release(run, cpu);
+            ret = 0;
+            break;
+        case KVM_EXIT_KAFL_SUBMIT_CR3:
+            handle_hypercall_kafl_cr3(run, cpu);
+            ret = 0;
+            break;
+        case KVM_EXIT_KAFL_SUBMIT_PANIC:
+            handle_hypercall_kafl_submit_panic(run, cpu);
+            ret = 0;
+            break;
+        case KVM_EXIT_KAFL_SUBMIT_KASAN:
+            handle_hypercall_kafl_submit_kasan(run, cpu);
+            ret = 0;
+            break;
+        case KVM_EXIT_KAFL_PANIC:
+            handle_hypercall_kafl_panic(run, cpu);
+            ret = 0;
+            break;
+        case KVM_EXIT_KAFL_KASAN:
+            handle_hypercall_kafl_kasan(run, cpu);
+            ret = 0;
+            break;
+        case KVM_EXIT_KAFL_LOCK:
+            handle_hypercall_kafl_lock(run, cpu);
+            ret = 0;
+            break;
+        case KVM_EXIT_KAFL_INFO:
+            handle_hypercall_kafl_info(run, cpu);
+            ret = 0;
+            break;
+        case KVM_EXIT_KAFL_NEXT_PAYLOAD:                                                                                                                                 
+            handle_hypercall_kafl_next_payload(run, cpu);                                                                                                                    
+            ret = 0;                                                                                                                                                         
+            break;      
+        case KVM_EXIT_KAFL_PRINTF:                                                                                                                                   
+            handle_hypercall_kafl_printf(run, cpu);                                                                                                                    
+            ret = 0;                                                                                                                                                         
+            break;       
+        case KVM_EXIT_KAFL_PRINTK_ADDR:   
+            handle_hypercall_kafl_printk_addr(run, cpu);                                                                                                                    
+            ret = 0;                                                                                                                                                         
+            break;   
+        case KVM_EXIT_KAFL_PRINTK:      
+            //timeout_reload_pending = false;                                                                                                                               
+            handle_hypercall_kafl_printk(run, cpu);                                                                                                                    
+            ret = 0;                                                                                                                                                         
+            break;
+
+        /* user space only exit reasons */
+        case KVM_EXIT_KAFL_USER_RANGE_ADVISE:
+            handle_hypercall_kafl_user_range_advise(run, cpu);
+            ret = 0;  
+            break;
+
+        case KVM_EXIT_KAFL_USER_SUBMIT_MODE:
+            handle_hypercall_kafl_user_submit_mode(run, cpu);
+            ret = 0;  
+            break;
+
+        case KVM_EXIT_KAFL_USER_FAST_ACQUIRE:
+            if(handle_hypercall_kafl_next_payload(run, cpu)){
+                handle_hypercall_kafl_cr3(run, cpu);   
+                handle_hypercall_kafl_acquire(run, cpu);
+            }
+            ret = 0;  
+            break;
+
+        case KVM_EXIT_KAFL_TOPA_MAIN_FULL:
+            pt_handle_overflow(cpu);
+            ret = 0;
+            break;
+
+        case KVM_EXIT_KAFL_USER_ABORT:
+            handle_hypercall_kafl_user_abort(run, cpu);
+            ret = 0;  
+            break;
+
+        case KVM_EXIT_KAFL_NESTED_CONFIG:
+            handle_hypercall_kafl_nested_config(run, cpu);
+            ret = 0;
+            break;
+
+        case KVM_EXIT_KAFL_NESTED_PREPARE:
+            handle_hypercall_kafl_nested_prepare(run, cpu);
+            ret = 0;
+            break;
+
+        case KVM_EXIT_KAFL_NESTED_ACQUIRE:
+            handle_hypercall_kafl_nested_acquire(run, cpu);
+            ret = 0;
+            break;
+
+        case KVM_EXIT_KAFL_NESTED_RELEASE:
+            handle_hypercall_kafl_nested_release(run, cpu);
+            ret = 0;
+            break;
+
+        case KVM_EXIT_KAFL_NESTED_HPRINTF:
+            handle_hypercall_kafl_nested_hprintf(run, cpu);
+            ret = 0;
+            break;
+
+        case KVM_EXIT_KAFL_PAGE_DUMP_BP:
+            handle_hypercall_kafl_page_dump_bp(run, cpu, run->debug.arch.pc);
+            ret = 0;
+            break;
+
+        case KVM_EXIT_KAFL_MTF:
+            handle_hypercall_kafl_mtf(run, cpu);
+            ret = 0;
+            break;
+
+        case KVM_EXIT_KAFL_RANGE_SUBMIT:
+            handle_hypercall_kafl_range_submit(run, cpu);
+            ret = 0;
+            break;
+
+        case HYPERCALL_KAFL_REQ_STREAM_DATA:
+            handle_hypercall_kafl_req_stream_data(run, cpu);
+            ret = 0;
+            break;
+
+        case KVM_EXIT_KAFL_NESTED_EARLY_RELEASE:
+            handle_hypercall_kafl_nested_early_release(run, cpu);
+            ret = 0;
+            break;
+
+        case KVM_EXIT_KAFL_PANIC_EXTENDED:
+            handle_hypercall_kafl_panic_extended(run, cpu);
+            ret = 0;
+            break;
+        
+#ifdef CONFIG_REDQUEEN                                                                                                                                                    
+        case KVM_EXIT_DEBUG:
+            kvm_arch_get_registers(cpu);                                                                                                                                     
+            if(!handle_hypercall_kafl_hook(run, cpu)){      
+                ret = kvm_arch_handle_exit(cpu, run);                                                                                                                        
+            }                                                                                                                                                      
+            else {     
+                ret = 0;                                                                                                                                                     
+            }                                                                                                                                                                
+            break;      
+#endif                                                                                                                                                     
+#endif     
+
         case KVM_EXIT_SYSTEM_EVENT:
             switch (run->system_event.type) {
             case KVM_SYSTEM_EVENT_SHUTDOWN:
-                qemu_system_shutdown_request(SHUTDOWN_CAUSE_GUEST_SHUTDOWN);
-                ret = EXCP_INTERRUPT;
+                if(GET_GLOBAL_STATE()->in_fuzzing_mode){
+                    /* Fuzzing is enabled at this point -> don't exit */
+                    handle_hypercall_kafl_release(run, cpu);
+                    ret = 0;
+                }
+                else{
+                    qemu_system_shutdown_request(SHUTDOWN_CAUSE_GUEST_SHUTDOWN);
+                    ret = EXCP_INTERRUPT;
+                }
                 break;
             case KVM_SYSTEM_EVENT_RESET:
-                qemu_system_reset_request(SHUTDOWN_CAUSE_GUEST_RESET);
-                ret = EXCP_INTERRUPT;
+                fprintf(stderr, "ATTEMPT TO SHUTDOWN MACHINE (KVM_SYSTEM_EVENT_RESET)!\n");
+                if(GET_GLOBAL_STATE()->in_fuzzing_mode){
+                    /* Fuzzing is enabled at this point -> don't exit */
+                    handle_hypercall_kafl_release(run, cpu);
+                    ret = 0;
+                }
+                else{
+                    qemu_system_reset_request(SHUTDOWN_CAUSE_GUEST_RESET);
+                    ret = EXCP_INTERRUPT;
+                }
                 break;
             case KVM_SYSTEM_EVENT_CRASH:
-                kvm_cpu_synchronize_state(cpu);
-                qemu_mutex_lock_iothread();
-                qemu_system_guest_panicked(cpu_get_crash_info(cpu));
-                qemu_mutex_unlock_iothread();
-                ret = 0;
+                fprintf(stderr, "ATTEMPT TO SHUTDOWN MACHINE (KVM_SYSTEM_EVENT_CRASH)!\n");
+                if(GET_GLOBAL_STATE()->in_fuzzing_mode){
+                    /* Fuzzing is enabled at this point -> don't exit */
+                    handle_hypercall_kafl_release(run, cpu);
+                    ret = 0;
+                }
+                else{
+                    kvm_cpu_synchronize_state(cpu);
+                    qemu_mutex_lock_iothread();
+                    qemu_system_guest_panicked(cpu_get_crash_info(cpu));
+                    qemu_mutex_unlock_iothread();
+                    ret = 0;
+                }
                 break;
             default:
                 DPRINTF("kvm_arch_handle_exit\n");
@@ -2433,16 +2677,42 @@
             }
             break;
         default:
-            DPRINTF("kvm_arch_handle_exit\n");
+            printf("kvm_arch_handle_exit => %d\n", run->exit_reason);
+            assert(false);
             ret = kvm_arch_handle_exit(cpu, run);
             break;
         }
+#ifdef CONFIG_PROCESSOR_TRACE   
+
+        if(GET_GLOBAL_STATE()->in_fuzzing_mode && GET_GLOBAL_STATE()->cow_cache_full){
+            synchronization_cow_full_detected();
+            GET_GLOBAL_STATE()->cow_cache_full = false;
+            ret = 0;
+        }
+        else{
+            if(GET_GLOBAL_STATE()->in_fuzzing_mode && cpu->halted){
+                cpu->halted = 0;
+                GET_GLOBAL_STATE()->shutdown_requested = true;
+            }
+
+            if(GET_GLOBAL_STATE()->in_fuzzing_mode && GET_GLOBAL_STATE()->shutdown_requested){
+                /* Fuzzing is enabled at this point -> don't exit */
+                fprintf(stderr, "shutdown_requested -> calling handle_hypercall_kafl_release\n");
+
+                synchronization_lock_crash_found();
+                GET_GLOBAL_STATE()->shutdown_requested = false;
+                ret = 0;
+            }
+        }
+
+#endif    
     } while (ret == 0);
 
     cpu_exec_end(cpu);
     qemu_mutex_lock_iothread();
 
     if (ret < 0) {
+        fprintf(stderr, "ATTEMPT TO SHUTDOWN MACHINE (ret < 0)!\n");
         cpu_dump_state(cpu, stderr, CPU_DUMP_CODE);
         vm_stop(RUN_STATE_INTERNAL_ERROR);
     }
@@ -2574,6 +2844,10 @@
     return err;
 }
 
+int kvm_has_vapic(void){
+    return !kvm_check_extension(kvm_state, KVM_CAP_VAPIC);
+}
+
 bool kvm_has_sync_mmu(void)
 {
     return kvm_state->sync_mmu;
@@ -2843,6 +3117,7 @@
     pthread_sigmask(SIG_SETMASK, &set, NULL);
 #endif
     sigdelset(&set, SIG_IPI);
+    sigdelset(&set, SIGALRM);
     if (kvm_immediate_exit) {
         r = pthread_sigmask(SIG_SETMASK, &set, NULL);
     } else {
@@ -3048,10 +3323,6 @@
     return kvm_state->kernel_irqchip_split == ON_OFF_AUTO_ON;
 }
 
-int kvm_has_vapic(void){
-    return !kvm_check_extension(kvm_state, KVM_CAP_VAPIC);
-}
-
 static void kvm_accel_instance_init(Object *obj)
 {
     KVMState *s = KVM_STATE(obj);
diff --new-file -ur qemu/block/block-backend.c QEMU-PT/block/block-backend.c
--- qemu/block/block-backend.c	2021-08-24 12:37:27.000000000 +0200
+++ QEMU-PT/block/block-backend.c	2021-08-24 21:54:56.038586388 +0200
@@ -27,6 +27,7 @@
 #include "qemu/option.h"
 #include "trace.h"
 #include "migration/misc.h"
+#include "pt/block_cow.h"
 
 /* Number of coroutines to reserve per attached device model */
 #define COROUTINE_POOL_RESERVATION 64
@@ -42,61 +43,6 @@
     QLIST_ENTRY(BlockBackendAioNotifier) list;
 } BlockBackendAioNotifier;
 
-struct BlockBackend {
-    char *name;
-    int refcnt;
-    BdrvChild *root;
-    AioContext *ctx;
-    DriveInfo *legacy_dinfo;    /* null unless created by drive_new() */
-    QTAILQ_ENTRY(BlockBackend) link;         /* for block_backends */
-    QTAILQ_ENTRY(BlockBackend) monitor_link; /* for monitor_block_backends */
-    BlockBackendPublic public;
-
-    DeviceState *dev;           /* attached device model, if any */
-    const BlockDevOps *dev_ops;
-    void *dev_opaque;
-
-    /* the block size for which the guest device expects atomicity */
-    int guest_block_size;
-
-    /* If the BDS tree is removed, some of its options are stored here (which
-     * can be used to restore those options in the new BDS on insert) */
-    BlockBackendRootState root_state;
-
-    bool enable_write_cache;
-
-    /* I/O stats (display with "info blockstats"). */
-    BlockAcctStats stats;
-
-    BlockdevOnError on_read_error, on_write_error;
-    bool iostatus_enabled;
-    BlockDeviceIoStatus iostatus;
-
-    uint64_t perm;
-    uint64_t shared_perm;
-    bool disable_perm;
-
-    bool allow_aio_context_change;
-    bool allow_write_beyond_eof;
-
-    NotifierList remove_bs_notifiers, insert_bs_notifiers;
-    QLIST_HEAD(, BlockBackendAioNotifier) aio_notifiers;
-
-    int quiesce_counter;
-    CoQueue queued_requests;
-    bool disable_request_queuing;
-
-    VMChangeStateEntry *vmsh;
-    bool force_allow_inactivate;
-
-    /* Number of in-flight aio requests.  BlockDriverState also counts
-     * in-flight requests but aio requests can exist even when blk->root is
-     * NULL, so we cannot rely on its counter for that case.
-     * Accessed with atomic ops.
-     */
-    unsigned int in_flight;
-};
-
 typedef struct BlockBackendAIOCB {
     BlockAIOCB common;
     BlockBackend *blk;
@@ -138,6 +84,8 @@
 static void blk_root_set_aio_ctx(BdrvChild *child, AioContext *ctx,
                                  GSList **ignore);
 
+int blk_check_byte_request(BlockBackend *blk, int64_t offset, size_t size);
+
 static char *blk_root_get_parent_desc(BdrvChild *child)
 {
     BlockBackend *blk = child->opaque;
@@ -335,6 +283,7 @@
     BlockBackend *blk;
 
     blk = g_new0(BlockBackend, 1);
+    blk->cow_cache = NULL;
     blk->refcnt = 1;
     blk->ctx = ctx;
     blk->perm = perm;
@@ -407,6 +356,8 @@
         return NULL;
     }
 
+    blk->cow_cache = cow_cache_new(filename);
+
     return blk;
 }
 
@@ -1109,7 +1060,7 @@
     blk->disable_request_queuing = disable;
 }
 
-static int blk_check_byte_request(BlockBackend *blk, int64_t offset,
+int blk_check_byte_request(BlockBackend *blk, int64_t offset,
                                   size_t size)
 {
     int64_t len;
@@ -1333,7 +1284,14 @@
     .aiocb_size         = sizeof(BlkAioEmAIOCB),
 };
 
-static void blk_aio_complete(BlkAioEmAIOCB *acb)
+void blk_aio_complete(BlkAioEmAIOCB *acb);
+BlockAIOCB *blk_aio_prwv(BlockBackend *blk, int64_t offset, int bytes,
+                                void *iobuf, CoroutineEntry co_entry,
+                                BdrvRequestFlags flags,
+                                BlockCompletionFunc *cb, void *opaque);
+void blk_aio_write_entry(void *opaque);
+
+void blk_aio_complete(BlkAioEmAIOCB *acb)
 {
     if (acb->has_returned) {
         acb->common.cb(acb->common.opaque, acb->rwco.ret);
@@ -1349,7 +1307,7 @@
     blk_aio_complete(acb);
 }
 
-static BlockAIOCB *blk_aio_prwv(BlockBackend *blk, int64_t offset, int bytes,
+BlockAIOCB *blk_aio_prwv(BlockBackend *blk, int64_t offset, int bytes,
                                 void *iobuf, CoroutineEntry co_entry,
                                 BdrvRequestFlags flags,
                                 BlockCompletionFunc *cb, void *opaque)
@@ -1399,7 +1357,7 @@
     blk_aio_complete(acb);
 }
 
-static void blk_aio_write_entry(void *opaque)
+void blk_aio_write_entry(void *opaque)
 {
     BlkAioEmAIOCB *acb = opaque;
     BlkRwCo *rwco = &acb->rwco;
@@ -1476,16 +1434,24 @@
                            QEMUIOVector *qiov, BdrvRequestFlags flags,
                            BlockCompletionFunc *cb, void *opaque)
 {
-    return blk_aio_prwv(blk, offset, qiov->size, qiov,
-                        blk_aio_read_entry, flags, cb, opaque);
+    if(blk->cow_cache->enabled){
+        return blk_aio_prwv(blk, offset, qiov->size, qiov, cow_cache_read_entry, flags, cb, opaque);
+    }
+    else{
+        return blk_aio_prwv(blk, offset, qiov->size, qiov, blk_aio_read_entry, flags, cb, opaque);
+    }
 }
 
 BlockAIOCB *blk_aio_pwritev(BlockBackend *blk, int64_t offset,
                             QEMUIOVector *qiov, BdrvRequestFlags flags,
                             BlockCompletionFunc *cb, void *opaque)
 {
-    return blk_aio_prwv(blk, offset, qiov->size, qiov,
-                        blk_aio_write_entry, flags, cb, opaque);
+    if(blk->cow_cache->enabled){
+        return blk_aio_prwv(blk, offset, qiov->size, qiov, cow_cache_write_entry, flags, cb, opaque);
+    }
+    else{
+        return blk_aio_prwv(blk, offset, qiov->size, qiov, blk_aio_write_entry, flags, cb, opaque);
+    }
 }
 
 static void blk_aio_flush_entry(void *opaque)
diff --new-file -ur qemu/block/file-posix.c QEMU-PT/block/file-posix.c
--- qemu/block/file-posix.c	2021-08-24 12:37:27.000000000 +0200
+++ QEMU-PT/block/file-posix.c	2021-08-24 21:54:56.038586388 +0200
@@ -781,37 +781,16 @@
 static int raw_check_lock_bytes(int fd, uint64_t perm, uint64_t shared_perm,
                                 Error **errp)
 {
-    int ret;
     int i;
 
     PERM_FOREACH(i) {
-        int off = RAW_LOCK_SHARED_BASE + i;
         uint64_t p = 1ULL << i;
         if (perm & p) {
-            ret = qemu_lock_fd_test(fd, off, 1, true);
-            if (ret) {
-                char *perm_name = bdrv_perm_names(p);
-                error_setg(errp,
-                           "Failed to get \"%s\" lock",
-                           perm_name);
-                g_free(perm_name);
-                return ret;
-            }
         }
     }
     PERM_FOREACH(i) {
-        int off = RAW_LOCK_PERM_BASE + i;
         uint64_t p = 1ULL << i;
         if (!(shared_perm & p)) {
-            ret = qemu_lock_fd_test(fd, off, 1, true);
-            if (ret) {
-                char *perm_name = bdrv_perm_names(p);
-                error_setg(errp,
-                           "Failed to get shared \"%s\" lock",
-                           perm_name);
-                g_free(perm_name);
-                return ret;
-            }
         }
     }
     return 0;
diff --new-file -ur qemu/compile.sh QEMU-PT/compile.sh
--- qemu/compile.sh	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/compile.sh	2021-08-24 21:54:56.042586385 +0200
@@ -0,0 +1,23 @@
+# Copyright (C) 2017 Sergej Schumilo
+# 
+# This file is part of QEMU-PT (kAFL).
+# 
+# QEMU-PT is free software: you can redistribute it and/or modify
+# it under the terms of the GNU General Public License as published by
+# the Free Software Foundation, either version 2 of the License, or
+# (at your option) any later version.
+# 
+# QEMU-PT is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+# GNU General Public License for more details.
+#
+# You should have received a copy of the GNU General Public License
+# along with QEMU-PT.  If not, see <http://www.gnu.org/licenses/>.
+
+#for release mode uncomment the following line:
+#export NM=`where gcc-nm-6` AR=`where gcc-ar-6` CC=`where gcc-6` RANLIB=`where gcc-ranlib-6` LD=`where gcc-6` CXX=`where g++-6` LDFLAGS="-flto" CFLAGS='-flto -std=gnu99'
+./configure --target-list=x86_64-softmmu --enable-gtk --enable-pt --enable-redqueen --disable-werror --disable-capstone --disable-libssh --disable-docs #-enable-sanitizers 
+make -j8
+
+# sudo apt-get install gtk+3.0  libgtk-3-dev
diff --new-file -ur qemu/configure QEMU-PT/configure
--- qemu/configure	2021-08-24 12:37:27.000000000 +0200
+++ QEMU-PT/configure	2021-08-24 21:54:55.686586600 +0200
@@ -950,6 +950,10 @@
   case "$opt" in
   --help|-h) show_help=yes
   ;;
+  --enable-pt) pt="yes"
+  ;;
+  --enable-redqueen) redqueen="yes"
+  ;;
   --version|-V) exec cat $source_path/VERSION
   ;;
   --prefix=*) prefix="$optarg"
@@ -6072,9 +6076,11 @@
   CFLAGS="-fprofile-arcs -ftest-coverage -g $CFLAGS"
   LDFLAGS="-fprofile-arcs -ftest-coverage $LDFLAGS"
 elif test "$fortify_source" = "yes" ; then
-  CFLAGS="-O2 -U_FORTIFY_SOURCE -D_FORTIFY_SOURCE=2 $CFLAGS"
-elif test "$debug" = "no"; then
-  CFLAGS="-O2 $CFLAGS"
+  #CFLAGS="-DNESTED_PATCH -O0 -frename-registers -frename-registers -mtune=native -march=core-avx2  -g -U_FORTIFY_SOURCE -D_FORTIFY_SOURCE=2 $CFLAGS"
+  CFLAGS="-DNESTED_PATCH -O3 -rdynamic -Wno-error=maybe-uninitialized -frename-registers -frename-registers -mtune=native -march=core-avx2  -g -U_FORTIFY_SOURCE -D_FORTIFY_SOURCE=2 $CFLAGS"
+  LIBS="-lcapstone $LIBS"
+#elif test "$debug" = "no"; then
+#  CFLAGS="-O2 $CFLAGS"
 fi
 
 if test "$have_asan" = "yes"; then
@@ -6817,6 +6823,12 @@
 if test "$splice" = "yes" ; then
   echo "CONFIG_SPLICE=y" >> $config_host_mak
 fi
+if test "$pt" = "yes" ; then
+  echo "CONFIG_PROCESSOR_TRACE=y" >> $config_host_mak
+  if test "$redqueen" = "yes" ; then
+    echo "CONFIG_REDQUEEN=y" >> $config_host_mak
+  fi
+fi
 if test "$eventfd" = "yes" ; then
   echo "CONFIG_EVENTFD=y" >> $config_host_mak
 fi
diff --new-file -ur qemu/cpus.c QEMU-PT/cpus.c
--- qemu/cpus.c	2021-08-24 12:37:27.000000000 +0200
+++ QEMU-PT/cpus.c	2021-08-24 21:54:55.938586449 +0200
@@ -1279,6 +1279,7 @@
 
     sigemptyset(&waitset);
     sigaddset(&waitset, SIG_IPI);
+    sigaddset(&waitset, SIGALRM);
 
     /* signal CPU creation */
     cpu->created = true;
diff --new-file -ur qemu/exec.c QEMU-PT/exec.c
--- qemu/exec.c	2021-08-24 12:37:27.000000000 +0200
+++ QEMU-PT/exec.c	2021-08-24 21:54:55.554586677 +0200
@@ -1357,6 +1357,62 @@
     return dirty;
 }
 
+#ifdef CONFIG_PROCESSOR_TRACE
+extern void fast_reload_qemu_user_fdl_set_dirty(void* self, MemoryRegion *mr, uint64_t addr, uint64_t length);
+extern void* get_fast_reload_snapshot(void);
+
+/* Note: start and end must be within the same ram block.  */
+bool cpu_physical_memory_test_dirty(ram_addr_t start,
+                                              ram_addr_t length,
+                                              unsigned client)
+{
+    DirtyMemoryBlocks *blocks;
+    unsigned long end, page;
+    bool dirty = false;
+    RAMBlock *ramblock;
+    uint64_t mr_offset, mr_size;
+
+    if (length == 0) {
+        return false;
+    }
+
+    end = TARGET_PAGE_ALIGN(start + length) >> TARGET_PAGE_BITS;
+    page = start >> TARGET_PAGE_BITS;
+
+    WITH_RCU_READ_LOCK_GUARD() {
+        blocks = atomic_rcu_read(&ram_list.dirty_memory[client]);
+        ramblock = qemu_get_ram_block(start);
+        /* Range sanity check on the ramblock */
+        assert(start >= ramblock->offset &&
+               start + length <= ramblock->offset + ramblock->used_length);
+
+        while (page < end) {
+            unsigned long idx = page / DIRTY_MEMORY_BLOCK_SIZE;
+            unsigned long offset = page % DIRTY_MEMORY_BLOCK_SIZE;
+            unsigned long num = MIN(end - page,
+                                    DIRTY_MEMORY_BLOCK_SIZE - offset);
+
+            /*
+            dirty |= bitmap_test_and_clear_atomic(blocks->blocks[idx],
+                                                  offset, num);
+            
+            if(dirty){
+                bitmap_set_atomic(blocks->blocks[idx],
+                                                  offset, num);
+            }
+            */
+            dirty |= bitmap_test_atomic(blocks->blocks[idx],
+                                                  offset, num);
+            
+
+            page += num;
+        }
+    }
+
+    return dirty;
+}
+#endif
+
 DirtyBitmapSnapshot *cpu_physical_memory_snapshot_and_clear_dirty
     (MemoryRegion *mr, hwaddr offset, hwaddr length, unsigned client)
 {
@@ -3063,6 +3119,9 @@
 static void invalidate_and_set_dirty(MemoryRegion *mr, hwaddr addr,
                                      hwaddr length)
 {
+#ifdef CONFIG_PROCESSOR_TRACE
+    fast_reload_qemu_user_fdl_set_dirty(get_fast_reload_snapshot(), mr, addr & 0xFFFFFFFFFFFFF000, length);
+#endif
     uint8_t dirty_log_mask = memory_region_get_dirty_log_mask(mr);
     addr += memory_region_get_ram_addr(mr);
 
diff --new-file -ur qemu/hmp-commands.hx QEMU-PT/hmp-commands.hx
--- qemu/hmp-commands.hx	2021-08-24 12:37:27.000000000 +0200
+++ QEMU-PT/hmp-commands.hx	2021-08-24 21:54:55.686586600 +0200
@@ -386,6 +386,24 @@
 ETEXI
 
     {
+        .name       = "fast_loadvm",
+        .args_type  = "name:s",
+        .params     = "tag",
+        .help       = "restore a VM snapshot from its tag",
+        .cmd        = hmp_loadvm_fast,
+        .command_completion = loadvm_completion,
+    },
+
+STEXI
+@item loadvm @var{tag}
+@findex loadvm
+Set the whole virtual machine to the snapshot identified by the tag
+@var{tag}.
+
+Since 4.0, loadvm stopped accepting snapshot id as parameter.
+ETEXI
+
+    {
         .name       = "delvm",
         .args_type  = "name:s",
         .params     = "tag",
diff --new-file -ur qemu/hw/char/serial.c QEMU-PT/hw/char/serial.c
--- qemu/hw/char/serial.c	2021-08-24 12:37:27.000000000 +0200
+++ QEMU-PT/hw/char/serial.c	2021-08-24 21:54:55.586586659 +0200
@@ -35,6 +35,7 @@
 #include "qemu/error-report.h"
 #include "trace.h"
 #include "hw/qdev-properties.h"
+#include "pt/state.h"
 
 //#define DEBUG_SERIAL
 
@@ -242,7 +243,9 @@
 static void serial_xmit(SerialState *s)
 {
     do {
+#ifndef CONFIG_PROCESSOR_TRACE
         assert(!(s->lsr & UART_LSR_TEMT));
+#endif
         if (s->tsr_retry == 0) {
             assert(!(s->lsr & UART_LSR_THRE));
 
@@ -344,6 +347,10 @@
 {
     SerialState *s = opaque;
 
+    if(GET_GLOBAL_STATE()->in_fuzzing_mode){
+        return;
+    }
+
     addr &= 7;
     trace_serial_ioport_write(addr, val);
     switch(addr) {
diff --new-file -ur qemu/hw/display/vga.c QEMU-PT/hw/display/vga.c
--- qemu/hw/display/vga.c	2021-08-24 12:37:27.000000000 +0200
+++ QEMU-PT/hw/display/vga.c	2021-08-24 21:54:55.662586615 +0200
@@ -153,6 +153,8 @@
     return vbe_enabled(s) ? s->sr_vbe[idx] : s->sr[idx];
 }
 
+bool dirty = false;
+
 static void vga_update_memory_access(VGACommonState *s)
 {
     hwaddr base, offset, size;
@@ -166,6 +168,7 @@
         object_unparent(OBJECT(&s->chain4_alias));
         s->has_chain4_alias = false;
         s->plane_updated = 0xf;
+        dirty = true;
     }
     if ((sr(s, VGA_SEQ_PLANE_WRITE) & VGA_SR02_ALL_PLANES) ==
         VGA_SR02_ALL_PLANES && sr(s, VGA_SEQ_MEMORY_MODE) & VGA_SR04_CHN_4M) {
@@ -2075,11 +2078,13 @@
 static int vga_common_post_load(void *opaque, int version_id)
 {
     VGACommonState *s = opaque;
-
-    /* force refresh */
-    s->graphic_mode = -1;
-    vbe_update_vgaregs(s);
-    vga_update_memory_access(s);
+    if(dirty){
+        /* force refresh */
+        s->graphic_mode = -1;
+        vbe_update_vgaregs(s);
+        vga_update_memory_access(s);
+        dirty = false;
+    }
     return 0;
 }
 
diff --new-file -ur qemu/hw/i386/pc_piix.c QEMU-PT/hw/i386/pc_piix.c
--- qemu/hw/i386/pc_piix.c	2021-08-24 12:37:27.000000000 +0200
+++ QEMU-PT/hw/i386/pc_piix.c	2021-08-24 21:54:55.614586643 +0200
@@ -957,3 +957,21 @@
 DEFINE_PC_MACHINE(xenfv, "xenfv", pc_xen_hvm_init,
                   xenfv_machine_options);
 #endif
+
+#ifdef CONFIG_PROCESSOR_TRACE
+
+static void pc_kAFL64_vmx_v1_0_machine_options(MachineClass *m)
+{
+    pc_i440fx_machine_options(m);
+    m->alias = "kAFL64";
+    m->desc = "kAFL64 PC (i440FX + PIIX, 1996)";
+}
+
+static void kAFL64_init(MachineState *machine)
+{
+    pc_init1(machine, TYPE_I440FX_PCI_HOST_BRIDGE, TYPE_I440FX_PCI_DEVICE);
+}
+
+DEFINE_PC_MACHINE(v1, "kAFL64-v1", kAFL64_init, pc_kAFL64_vmx_v1_0_machine_options);
+
+#endif
diff --new-file -ur qemu/hw/pci/pci.c QEMU-PT/hw/pci/pci.c
--- qemu/hw/pci/pci.c	2021-08-24 12:37:27.000000000 +0200
+++ QEMU-PT/hw/pci/pci.c	2021-08-24 21:54:55.574586666 +0200
@@ -546,6 +546,22 @@
     return 0;
 }
 
+#ifdef CONFIG_PROCESSOR_TRACE
+void fast_get_pci_config_device(void* data, size_t size, void* opaque){
+    PCIDevice *s = container_of(opaque, PCIDevice, config);
+    PCIDeviceClass *pc = PCI_DEVICE_GET_CLASS(s);
+    uint8_t *config = (uint8_t *) data;
+
+    memcpy(s->config, config, size);
+
+    pci_update_mappings(s);
+    if (pc->is_bridge) {
+        PCIBridge *b = PCI_BRIDGE(s);
+        pci_bridge_update_mappings(b);
+    }
+}
+#endif
+
 /* just put buffer */
 static int put_pci_config_device(QEMUFile *f, void *pv, size_t size,
                                  const VMStateField *field, QJSON *vmdesc)
@@ -585,6 +601,17 @@
     return 0;
 }
 
+#ifdef CONFIG_PROCESSOR_TRACE
+void fast_get_pci_irq_state(void* data, size_t size, void* opaque){
+    PCIDevice *s = container_of(opaque, PCIDevice, irq_state);
+    uint32_t* irq_state = (uint32_t*) data;
+
+    for (int i = 0; i < PCI_NUM_PINS; ++i) {
+        pci_set_irq_state(s, i, irq_state[i]);
+    }
+}
+#endif
+
 static int put_pci_irq_state(QEMUFile *f, void *pv, size_t size,
                              const VMStateField *field, QJSON *vmdesc)
 {
diff --new-file -ur qemu/include/exec/ram_addr.h QEMU-PT/include/exec/ram_addr.h
--- qemu/include/exec/ram_addr.h	2021-08-24 12:37:27.000000000 +0200
+++ QEMU-PT/include/exec/ram_addr.h	2021-08-24 21:54:55.862586494 +0200
@@ -456,6 +456,12 @@
 }
 #endif /* not _WIN32 */
 
+#ifdef CONFIG_PROCESSORE_TRACE
+bool cpu_physical_memory_test_dirty(ram_addr_t start,
+                                              ram_addr_t length,
+                                              unsigned client);
+#endif
+
 bool cpu_physical_memory_test_and_clear_dirty(ram_addr_t start,
                                               ram_addr_t length,
                                               unsigned client);
diff --new-file -ur qemu/include/hw/core/cpu.h QEMU-PT/include/hw/core/cpu.h
--- qemu/include/hw/core/cpu.h	2021-08-24 12:37:27.000000000 +0200
+++ QEMU-PT/include/hw/core/cpu.h	2021-08-24 21:54:55.842586506 +0200
@@ -409,6 +409,21 @@
      */
     uintptr_t mem_io_pc;
 
+#ifdef CONFIG_PROCESSOR_TRACE
+    volatile int pt_cmd;
+    volatile int pt_ret;
+    volatile bool pt_enabled;
+
+    int pt_fd;
+    void* pt_mmap;
+
+    void* pt_decoder_state;
+
+    bool reload_pending;
+    bool intel_pt_run_trashed;
+
+#endif
+
     int kvm_fd;
     struct KVMState *kvm_state;
     struct kvm_run *kvm_run;
diff --new-file -ur qemu/include/hw/pci/pci.h QEMU-PT/include/hw/pci/pci.h
--- qemu/include/hw/pci/pci.h	2021-08-24 12:37:27.000000000 +0200
+++ QEMU-PT/include/hw/pci/pci.h	2021-08-24 21:54:55.842586506 +0200
@@ -373,6 +373,8 @@
 
 uint8_t pci_find_capability(PCIDevice *pci_dev, uint8_t cap_id);
 
+void fast_get_pci_config_device(void* data, size_t size, void* opaque);
+void fast_get_pci_irq_state(void* data, size_t size, void* opaque);
 
 uint32_t pci_default_read_config(PCIDevice *d,
                                  uint32_t address, int len);
diff --new-file -ur qemu/include/monitor/hmp.h QEMU-PT/include/monitor/hmp.h
--- qemu/include/monitor/hmp.h	2021-08-24 12:37:27.000000000 +0200
+++ QEMU-PT/include/monitor/hmp.h	2021-08-24 21:54:55.834586511 +0200
@@ -67,6 +67,9 @@
 void hmp_drive_mirror(Monitor *mon, const QDict *qdict);
 void hmp_drive_backup(Monitor *mon, const QDict *qdict);
 void hmp_loadvm(Monitor *mon, const QDict *qdict);
+#ifdef CONFIG_PROCESSOR_TRACE
+void hmp_loadvm_fast(Monitor *mon, const QDict *qdict);
+#endif
 void hmp_savevm(Monitor *mon, const QDict *qdict);
 void hmp_delvm(Monitor *mon, const QDict *qdict);
 void hmp_info_snapshots(Monitor *mon, const QDict *qdict);
diff --new-file -ur qemu/include/qemu/bitmap.h QEMU-PT/include/qemu/bitmap.h
--- qemu/include/qemu/bitmap.h	2021-08-24 12:37:27.000000000 +0200
+++ QEMU-PT/include/qemu/bitmap.h	2021-08-24 21:54:55.862586494 +0200
@@ -253,6 +253,9 @@
 void bitmap_set_atomic(unsigned long *map, long i, long len);
 void bitmap_clear(unsigned long *map, long start, long nr);
 bool bitmap_test_and_clear_atomic(unsigned long *map, long start, long nr);
+#ifdef CONFIG_PROCESSOR_TRACE
+bool bitmap_test_atomic(unsigned long *map, long start, long nr);
+#endif
 void bitmap_copy_and_clear_atomic(unsigned long *dst, unsigned long *src,
                                   long nr);
 unsigned long bitmap_find_next_zero_area(unsigned long *map,
diff --new-file -ur qemu/include/sysemu/block-backend.h QEMU-PT/include/sysemu/block-backend.h
--- qemu/include/sysemu/block-backend.h	2021-08-24 12:37:27.000000000 +0200
+++ QEMU-PT/include/sysemu/block-backend.h	2021-08-24 21:54:55.858586497 +0200
@@ -15,6 +15,9 @@
 
 #include "qemu/iov.h"
 #include "block/throttle-groups.h"
+#include "sysemu/sysemu.h"
+#include "pt/block_cow.h"
+
 
 /*
  * TODO Have to include block/block.h for a bunch of block layer
@@ -265,4 +268,60 @@
 
 const BdrvChild *blk_root(BlockBackend *blk);
 
+struct BlockBackend {
+    cow_cache_t* cow_cache;
+    char *name;
+    int refcnt;
+    BdrvChild *root;
+    AioContext *ctx;
+    DriveInfo *legacy_dinfo;    /* null unless created by drive_new() */
+    QTAILQ_ENTRY(BlockBackend) link;         /* for block_backends */
+    QTAILQ_ENTRY(BlockBackend) monitor_link; /* for monitor_block_backends */
+    BlockBackendPublic public;
+
+    DeviceState *dev;           /* attached device model, if any */
+    const BlockDevOps *dev_ops;
+    void *dev_opaque;
+
+    /* the block size for which the guest device expects atomicity */
+    int guest_block_size;
+
+    /* If the BDS tree is removed, some of its options are stored here (which
+     * can be used to restore those options in the new BDS on insert) */
+    BlockBackendRootState root_state;
+
+    bool enable_write_cache;
+
+    /* I/O stats (display with "info blockstats"). */
+    BlockAcctStats stats;
+
+    BlockdevOnError on_read_error, on_write_error;
+    bool iostatus_enabled;
+    BlockDeviceIoStatus iostatus;
+
+    uint64_t perm;
+    uint64_t shared_perm;
+    bool disable_perm;
+
+    bool allow_aio_context_change;
+    bool allow_write_beyond_eof;
+
+    NotifierList remove_bs_notifiers, insert_bs_notifiers;
+    QLIST_HEAD(, BlockBackendAioNotifier) aio_notifiers;
+
+    int quiesce_counter;
+    CoQueue queued_requests;
+    bool disable_request_queuing;
+
+    VMChangeStateEntry *vmsh;
+    bool force_allow_inactivate;
+
+    /* Number of in-flight aio requests.  BlockDriverState also counts
+     * in-flight requests but aio requests can exist even when blk->root is
+     * NULL, so we cannot rely on its counter for that case.
+     * Accessed with atomic ops.
+     */
+    unsigned int in_flight;
+};
+
 #endif
diff --new-file -ur qemu/include/sysemu/kvm.h QEMU-PT/include/sysemu/kvm.h
--- qemu/include/sysemu/kvm.h	2021-08-24 12:37:27.000000000 +0200
+++ QEMU-PT/include/sysemu/kvm.h	2021-08-24 21:54:55.858586497 +0200
@@ -367,8 +367,16 @@
 /* full state set, modified during initialization or on vmload */
 #define KVM_PUT_FULL_STATE      3
 
+#ifdef CONFIG_PROCESSOR_TRACE
+#define KVM_PUT_FULL_STATE_FAST      4
+#endif
+
 int kvm_arch_put_registers(CPUState *cpu, int level);
 
+#ifdef CONFIG_PROCESSOR_TRACE
+int kvm_arch_get_registers_fast(CPUState *cpu);
+#endif
+
 int kvm_arch_init(MachineState *ms, KVMState *s);
 
 int kvm_arch_init_vcpu(CPUState *cpu);
diff --new-file -ur qemu/linux-headers/linux/kvm.h QEMU-PT/linux-headers/linux/kvm.h
--- qemu/linux-headers/linux/kvm.h	2021-08-24 12:37:27.000000000 +0200
+++ QEMU-PT/linux-headers/linux/kvm.h	2021-08-24 21:54:55.690586598 +0200
@@ -237,6 +237,62 @@
 #define KVM_EXIT_HYPERV           27
 #define KVM_EXIT_ARM_NISV         28
 
+#define HYPERCALL_KAFL_RAX_ID			0x01f
+#define KAFL_EXIT_OFFSET				100
+
+#define KVM_EXIT_KAFL_ACQUIRE			100
+#define KVM_EXIT_KAFL_GET_PAYLOAD		101
+#define KVM_EXIT_KAFL_GET_PROGRAM		102
+#define KVM_EXIT_KAFL_GET_ARGV			103
+#define KVM_EXIT_KAFL_RELEASE			104
+#define KVM_EXIT_KAFL_SUBMIT_CR3		105
+#define KVM_EXIT_KAFL_SUBMIT_PANIC		106
+#define KVM_EXIT_KAFL_SUBMIT_KASAN		107
+#define KVM_EXIT_KAFL_PANIC				108
+#define KVM_EXIT_KAFL_KASAN				109
+#define KVM_EXIT_KAFL_LOCK				110
+#define KVM_EXIT_KAFL_INFO				111
+#define KVM_EXIT_KAFL_NEXT_PAYLOAD		112
+#define KVM_EXIT_KAFL_PRINTF			113
+
+/* Kernel Printf Debugger */
+#define KVM_EXIT_KAFL_PRINTK_ADDR		114
+#define KVM_EXIT_KAFL_PRINTK			115
+
+/* user space only exit reasons */
+#define KVM_EXIT_KAFL_USER_RANGE_ADVISE	116
+#define KVM_EXIT_KAFL_USER_SUBMIT_MODE	117
+#define KVM_EXIT_KAFL_USER_FAST_ACQUIRE	118
+#define KVM_EXIT_KAFL_TOPA_MAIN_FULL	119
+#define KVM_EXIT_KAFL_USER_ABORT		120
+
+
+/* hypertrash only hypercalls */
+#define HYPERTRASH_HYPERCALL_MASK		0xAA000000
+
+#define HYPERCALL_KAFL_NESTED_PREPARE	(0 | HYPERTRASH_HYPERCALL_MASK)
+#define HYPERCALL_KAFL_NESTED_CONFIG	(1 | HYPERTRASH_HYPERCALL_MASK)
+#define HYPERCALL_KAFL_NESTED_ACQUIRE	(2 | HYPERTRASH_HYPERCALL_MASK)
+#define HYPERCALL_KAFL_NESTED_RELEASE	(3 | HYPERTRASH_HYPERCALL_MASK)
+
+#define KVM_EXIT_KAFL_NESTED_CONFIG		121
+#define KVM_EXIT_KAFL_NESTED_PREPARE	122
+#define KVM_EXIT_KAFL_NESTED_ACQUIRE	123
+#define KVM_EXIT_KAFL_NESTED_RELEASE	124
+
+#define KVM_EXIT_KAFL_PAGE_DUMP_BP	125
+#define KVM_EXIT_KAFL_TIMEOUT       126
+
+#define KVM_EXIT_KAFL_NESTED_HPRINTF	127
+#define KVM_EXIT_KAFL_MTF	128
+
+#define KVM_EXIT_KAFL_RANGE_SUBMIT	129
+#define HYPERCALL_KAFL_REQ_STREAM_DATA 130
+#define KVM_EXIT_KAFL_NESTED_EARLY_RELEASE	131
+#define KVM_EXIT_KAFL_PANIC_EXTENDED 132
+
+
+
 /* For KVM_EXIT_INTERNAL_ERROR */
 /* Emulate instruction failed. */
 #define KVM_INTERNAL_ERROR_EMULATION	1
@@ -1623,4 +1679,57 @@
 #define KVM_HYPERV_CONN_ID_MASK		0x00ffffff
 #define KVM_HYPERV_EVENTFD_DEASSIGN	(1 << 0)
 
+/*
+ * ioctls for vmx_pt fds
+ */
+#define KVM_VMX_PT_SETUP_FD                                     _IO(KVMIO,      0xd0)                   /* apply vmx_pt fd (via vcpu fd ioctl)*/
+#define KVM_VMX_PT_CONFIGURE_ADDR0                      _IOW(KVMIO,     0xd1, __u64)    /* configure IP-filtering for addr0_a & addr0_b */
+#define KVM_VMX_PT_CONFIGURE_ADDR1                      _IOW(KVMIO,     0xd2, __u64)    /* configure IP-filtering for addr1_a & addr1_b */
+#define KVM_VMX_PT_CONFIGURE_ADDR2                      _IOW(KVMIO,     0xd3, __u64)    /* configure IP-filtering for addr2_a & addr2_b */
+#define KVM_VMX_PT_CONFIGURE_ADDR3                      _IOW(KVMIO,     0xd4, __u64)    /* configure IP-filtering for addr3_a & addr3_b */
+
+#define KVM_VMX_PT_CONFIGURE_CR3                        _IOW(KVMIO,     0xd5, __u64)    /* setup CR3 filtering value */
+#define KVM_VMX_PT_ENABLE                                       _IO(KVMIO,      0xd6)                   /* enable and lock configuration */ 
+#define KVM_VMX_PT_GET_TOPA_SIZE                        _IOR(KVMIO,     0xd7, __u32)    /* get defined ToPA size */
+#define KVM_VMX_PT_DISABLE                                      _IO(KVMIO,      0xd8)                   /* enable and lock configuration */ 
+#define KVM_VMX_PT_CHECK_TOPA_OVERFLOW          _IO(KVMIO,      0xd9)                   /* check for ToPA overflow */
+
+#define KVM_VMX_PT_ENABLE_ADDR0                         _IO(KVMIO,      0xaa)                   /* enable IP-filtering for addr0 */
+#define KVM_VMX_PT_ENABLE_ADDR1                         _IO(KVMIO,      0xab)                   /* enable IP-filtering for addr1 */
+#define KVM_VMX_PT_ENABLE_ADDR2                         _IO(KVMIO,      0xac)                   /* enable IP-filtering for addr2 */
+#define KVM_VMX_PT_ENABLE_ADDR3                         _IO(KVMIO,      0xad)                   /* enable IP-filtering for addr3 */
+
+#define KVM_VMX_PT_DISABLE_ADDR0                        _IO(KVMIO,      0xae)                   /* disable IP-filtering for addr0 */
+#define KVM_VMX_PT_DISABLE_ADDR1                        _IO(KVMIO,      0xaf)                   /* disable IP-filtering for addr1 */
+#define KVM_VMX_PT_DISABLE_ADDR2                        _IO(KVMIO,      0xe0)                   /* disable IP-filtering for addr2 */
+#define KVM_VMX_PT_DISABLE_ADDR3                        _IO(KVMIO,      0xe1)                   /* disable IP-filtering for addr3 */
+
+#define KVM_VMX_PT_ENABLE_CR3                           _IO(KVMIO,      0xe2)                   /* enable CR3 filtering */
+#define KVM_VMX_PT_DISABLE_CR3                          _IO(KVMIO,      0xe3)                   /* disable CR3 filtering */
+
+#define KVM_VMX_PT_SUPPORTED                            _IO(KVMIO,      0xe4)
+
+#define KVM_VMX_FDL_SETUP_FD                            _IO(KVMIO,      0xe5)
+#define KVM_VMX_FDL_SET                                 _IOW(KVMIO,     0xe6, __u64)
+#define KVM_VMX_FDL_FLUSH                               _IO(KVMIO,      0xe7)
+#define KVM_VMX_FDL_GET_INDEX                           _IOR(KVMIO,     0xe8, __u64)
+
+#define KVM_VMX_PT_GET_ADDRN                            _IO(KVMIO,      0xe9)
+
+/* Multi CR3 Support */
+
+#define KVM_VMX_PT_CONFIGURE_MULTI_CR3			_IOW(KVMIO,	0xea, __u64)	/* setup CR3 filtering value */
+#define KVM_VMX_PT_ENABLE_MULTI_CR3				_IO(KVMIO,	0xeb)			/* enable CR3 filtering */
+#define KVM_VMX_PT_DISABLE_MULTI_CR3			_IO(KVMIO,	0xec)			/* disable CR3 filtering */
+
+/* Page Dump Support */
+
+#define KVM_VMX_PT_SET_PAGE_DUMP_CR3				_IOW(KVMIO,	0xed, __u64)
+#define KVM_VMX_PT_ENABLE_PAGE_DUMP_CR3			_IO(KVMIO,	0xee)	
+#define KVM_VMX_PT_DISABLE_PAGE_DUMP_CR3		_IO(KVMIO,	0xef)	
+
+#define KVM_VMX_PT_ENABLE_MTF			_IO(KVMIO,	0xf0)	
+#define KVM_VMX_PT_DISABLE_MTF		_IO(KVMIO,	0xf1)	
+
+
 #endif /* __LINUX_KVM_H */
diff --new-file -ur qemu/Makefile.target QEMU-PT/Makefile.target
--- qemu/Makefile.target	2021-08-24 12:37:27.000000000 +0200
+++ QEMU-PT/Makefile.target	2021-08-24 21:54:56.030586393 +0200
@@ -157,6 +157,8 @@
 obj-y += hw/
 obj-y += monitor/
 obj-y += qapi/
+obj-$(CONFIG_PROCESSOR_TRACE) += pt.o
+obj-$(CONFIG_PROCESSOR_TRACE) += pt/
 obj-y += memory.o
 obj-y += memory_mapping.o
 obj-y += migration/ram.o
diff --new-file -ur qemu/memory.c QEMU-PT/memory.c
--- qemu/memory.c	2021-08-24 12:37:27.000000000 +0200
+++ QEMU-PT/memory.c	2021-08-24 21:54:55.554586677 +0200
@@ -35,6 +35,11 @@
 #include "hw/boards.h"
 #include "migration/vmstate.h"
 
+#ifdef CONFIG_PROCESSOR_TRACE
+#include "pt/state.h"
+#include "pt/fast_vm_reload.h"
+#endif
+
 //#define DEBUG_UNASSIGNED
 
 static unsigned memory_region_transaction_depth;
@@ -2011,6 +2016,9 @@
                              hwaddr size)
 {
     assert(mr->ram_block);
+#ifdef CONFIG_PROCESSOR_TRACE
+    fast_reload_qemu_user_fdl_set_dirty(get_fast_reload_snapshot(), mr, addr & 0xFFFFFFFFFFFFF000, size);
+#endif
     cpu_physical_memory_set_dirty_range(memory_region_get_ram_addr(mr) + addr,
                                         size,
                                         memory_region_get_dirty_log_mask(mr));
diff --new-file -ur qemu/migration/savevm.c QEMU-PT/migration/savevm.c
--- qemu/migration/savevm.c	2021-08-24 12:37:27.000000000 +0200
+++ QEMU-PT/migration/savevm.c	2021-08-24 21:54:56.034586391 +0200
@@ -252,14 +252,14 @@
     QTAILQ_HEAD(, SaveStateEntry) handlers;
     int global_section_id;
     uint32_t len;
-    const char *name;
+    char *name;
     uint32_t target_page_bits;
     uint32_t caps_count;
     MigrationCapability *capabilities;
     QemuUUID uuid;
 } SaveState;
 
-static SaveState savevm_state = {
+SaveState savevm_state = {
     .handlers = QTAILQ_HEAD_INITIALIZER(savevm_state.handlers),
     .global_section_id = 0,
 };
@@ -289,6 +289,16 @@
     return result;
 }
 
+int vmstate_load(QEMUFile *f, SaveStateEntry *se);
+int vmstate_save(QEMUFile *f, SaveStateEntry *se, QJSON *vmdesc);
+void save_section_header(QEMUFile *f, SaveStateEntry *se, uint8_t section_type);
+void save_section_footer(QEMUFile *f, SaveStateEntry *se);
+bool should_send_vmdesc(void);
+int qemu_savevm_state(QEMUFile *f, Error **errp);
+bool check_section_footer(QEMUFile *f, SaveStateEntry *se);
+int qemu_loadvm_section_start_full(QEMUFile *f, MigrationIncomingState *mis);
+int qemu_loadvm_section_part_end(QEMUFile *f, MigrationIncomingState *mis);
+
 static int configuration_pre_save(void *opaque)
 {
     SaveState *state = opaque;
@@ -297,7 +307,10 @@
     int i, j;
 
     state->len = strlen(current_name);
-    state->name = current_name;
+    if(state->name){
+	free(state->name);
+    }
+    state->name = strdup(current_name);
     state->target_page_bits = qemu_target_page_bits();
 
     state->caps_count = get_validatable_capabilities_count();
@@ -508,7 +521,7 @@
     }
 };
 
-static const VMStateDescription vmstate_configuration = {
+const VMStateDescription vmstate_configuration = {
     .name = "configuration",
     .version_id = 1,
     .pre_load = configuration_pre_load,
@@ -848,7 +861,7 @@
     }
 }
 
-static int vmstate_load(QEMUFile *f, SaveStateEntry *se)
+int vmstate_load(QEMUFile *f, SaveStateEntry *se)
 {
     trace_vmstate_load(se->idstr, se->vmsd ? se->vmsd->name : "(old)");
     if (!se->vmsd) {         /* Old style */
@@ -877,7 +890,7 @@
     }
 }
 
-static int vmstate_save(QEMUFile *f, SaveStateEntry *se, QJSON *vmdesc)
+int vmstate_save(QEMUFile *f, SaveStateEntry *se, QJSON *vmdesc)
 {
     trace_vmstate_save(se->idstr, se->vmsd ? se->vmsd->name : "(old)");
     if (!se->vmsd) {
@@ -890,7 +903,7 @@
 /*
  * Write the header for device section (QEMU_VM_SECTION START/END/PART/FULL)
  */
-static void save_section_header(QEMUFile *f, SaveStateEntry *se,
+void save_section_header(QEMUFile *f, SaveStateEntry *se,
                                 uint8_t section_type)
 {
     qemu_put_byte(f, section_type);
@@ -912,7 +925,7 @@
  * Write a footer onto device sections that catches cases misformatted device
  * sections.
  */
-static void save_section_footer(QEMUFile *f, SaveStateEntry *se)
+void save_section_footer(QEMUFile *f, SaveStateEntry *se)
 {
     if (migrate_get_current()->send_section_footer) {
         qemu_put_byte(f, QEMU_VM_SECTION_FOOTER);
@@ -1262,7 +1275,7 @@
     return ret;
 }
 
-static bool should_send_vmdesc(void)
+bool should_send_vmdesc(void)
 {
     MachineState *machine = MACHINE(qdev_get_machine());
     bool in_postcopy = migration_in_postcopy();
@@ -1498,7 +1511,7 @@
     }
 }
 
-static int qemu_savevm_state(QEMUFile *f, Error **errp)
+int qemu_savevm_state(QEMUFile *f, Error **errp)
 {
     int ret;
     MigrationState *ms = migrate_get_current();
@@ -2200,7 +2213,7 @@
  * Returns: true if the footer was good
  *          false if there is a problem (and calls error_report to say why)
  */
-static bool check_section_footer(QEMUFile *f, SaveStateEntry *se)
+bool check_section_footer(QEMUFile *f, SaveStateEntry *se)
 {
     int ret;
     uint8_t read_mark;
@@ -2237,7 +2250,7 @@
     return true;
 }
 
-static int
+int
 qemu_loadvm_section_start_full(QEMUFile *f, MigrationIncomingState *mis)
 {
     uint32_t instance_id, version_id, section_id;
@@ -2302,7 +2315,7 @@
     return 0;
 }
 
-static int
+int
 qemu_loadvm_section_part_end(QEMUFile *f, MigrationIncomingState *mis)
 {
     uint32_t section_id;
diff --new-file -ur qemu/migration/vmstate.c QEMU-PT/migration/vmstate.c
--- qemu/migration/vmstate.c	2021-08-24 12:37:27.000000000 +0200
+++ QEMU-PT/migration/vmstate.c	2021-08-24 21:54:56.030586393 +0200
@@ -22,10 +22,15 @@
 
 static int vmstate_subsection_save(QEMUFile *f, const VMStateDescription *vmsd,
                                    void *opaque, QJSON *vmdesc);
-static int vmstate_subsection_load(QEMUFile *f, const VMStateDescription *vmsd,
+int vmstate_subsection_load(QEMUFile *f, const VMStateDescription *vmsd,
                                    void *opaque);
 
-static int vmstate_n_elems(void *opaque, const VMStateField *field)
+int vmstate_n_elems(void *opaque, const VMStateField *field);
+int vmstate_size(void *opaque, const VMStateField *field);
+void vmstate_handle_alloc(void *ptr, const VMStateField *field, void *opaque);
+const VMStateDescription * vmstate_get_subsection(const VMStateDescription **sub, char *idstr);
+
+int vmstate_n_elems(void *opaque, const VMStateField *field)
 {
     int n_elems = 1;
 
@@ -49,7 +54,7 @@
     return n_elems;
 }
 
-static int vmstate_size(void *opaque, const VMStateField *field)
+int vmstate_size(void *opaque, const VMStateField *field)
 {
     int size = field->size;
 
@@ -63,7 +68,7 @@
     return size;
 }
 
-static void vmstate_handle_alloc(void *ptr, const VMStateField *field,
+void vmstate_handle_alloc(void *ptr, const VMStateField *field,
                                  void *opaque)
 {
     if (field->flags & VMS_POINTER && field->flags & VMS_ALLOC) {
@@ -428,7 +433,7 @@
     return ret;
 }
 
-static const VMStateDescription *
+const VMStateDescription *
 vmstate_get_subsection(const VMStateDescription **sub, char *idstr)
 {
     while (sub && *sub) {
@@ -440,7 +445,7 @@
     return NULL;
 }
 
-static int vmstate_subsection_load(QEMUFile *f, const VMStateDescription *vmsd,
+int vmstate_subsection_load(QEMUFile *f, const VMStateDescription *vmsd,
                                    void *opaque)
 {
     trace_vmstate_subsection_load(vmsd->name);
diff --new-file -ur qemu/monitor/hmp-cmds.c QEMU-PT/monitor/hmp-cmds.c
--- qemu/monitor/hmp-cmds.c	2021-08-24 12:37:27.000000000 +0200
+++ QEMU-PT/monitor/hmp-cmds.c	2021-08-24 21:54:55.554586677 +0200
@@ -60,6 +60,11 @@
 #include <spice/enums.h>
 #endif
 
+#ifdef CONFIG_PROCESSOR_TRACE
+#include "pt/state.h"
+#include "pt/fast_vm_reload.h"
+#endif
+
 void hmp_handle_error(Monitor *mon, Error *err)
 {
     if (err) {
@@ -1456,6 +1461,22 @@
     hmp_handle_error(mon, err);
 }
 
+#ifdef CONFIG_PROCESSOR_TRACE
+#include "sysemu/kvm.h"
+
+
+void hmp_loadvm_fast(Monitor *mon, const QDict *qdict)
+{
+
+    //struct kvm_clock_data data = {.clock = 0, .flags = 0};
+
+    fast_reload_restore(get_fast_reload_snapshot());
+    	
+    //kvm_vm_ioctl(kvm_state, KVM_SET_CLOCK, &data);
+
+}
+#endif
+
 void hmp_savevm(Monitor *mon, const QDict *qdict)
 {
     Error *err = NULL;
diff --new-file -ur qemu/pt/auxiliary_buffer.c QEMU-PT/pt/auxiliary_buffer.c
--- qemu/pt/auxiliary_buffer.c	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/auxiliary_buffer.c	2021-08-24 21:54:55.938586449 +0200
@@ -0,0 +1,227 @@
+/*
+
+Copyright (C) 2019 Sergej Schumilo
+
+This file is part of QEMU-PT (HyperTrash / kAFL).
+
+QEMU-PT is free software: you can redistribute it and/or modify
+it under the terms of the GNU General Public License as published by
+the Free Software Foundation, either version 2 of the License, or
+(at your option) any later version.
+
+QEMU-PT is distributed in the hope that it will be useful,
+but WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with QEMU-PT.  If not, see <http://www.gnu.org/licenses/>.
+
+*/
+
+#include "pt/auxiliary_buffer.h"
+#include <string.h>
+#include <stdio.h>
+#include <stdbool.h>
+#include "pt/state.h"
+
+#define VOLATILE_WRITE_64(dst, src) *((volatile uint64_t*)&dst) = (uint64_t)src
+#define VOLATILE_WRITE_32(dst, src) *((volatile uint32_t*)&dst) = (uint32_t)src
+#define VOLATILE_WRITE_16(dst, src) *((volatile uint16_t*)&dst) = (uint16_t)src
+#define VOLATILE_WRITE_8(dst, src) *((volatile uint8_t*)&dst) = (uint8_t)src
+
+#define VOLATILE_READ_64(dst, src) dst = *((volatile uint64_t*)(&src)) 
+#define VOLATILE_READ_32(dst, src) dst = *((volatile uint32_t*)(&src)) 
+#define VOLATILE_READ_16(dst, src) dst = *((volatile uint16_t*)(&src)) 
+#define VOLATILE_READ_8(dst, src) dst = *((volatile uint8_t*)(&src)) 
+
+static void volatile_memset(void* dst, uint8_t ch, size_t count){
+  for (size_t i = 0; i < count; i++){
+    VOLATILE_WRITE_8(((uint8_t*)dst)[i], ch);
+  }
+}
+
+static void volatile_memcpy(void* dst, void* src, size_t size){
+  for (size_t i = 0; i < size; i++){
+    VOLATILE_WRITE_8(((uint8_t*)dst)[i], ((uint8_t*)src)[i]);
+  }
+}
+
+void init_auxiliary_buffer(auxilary_buffer_t* auxilary_buffer){
+  fprintf(stderr, "%s\n", __func__);
+  volatile_memset((void*) auxilary_buffer, 0, sizeof(auxilary_buffer_t));
+
+  VOLATILE_WRITE_16(auxilary_buffer->header.version, QEMU_PT_VERSION);
+
+  uint16_t hash = (sizeof(auxilary_buffer_header_t) + 
+                  sizeof(auxilary_buffer_cap_t) + 
+                  sizeof(auxilary_buffer_config_t) + 
+                  sizeof(auxilary_buffer_result_t) + 
+                  sizeof(auxilary_buffer_misc_t)) % 0xFFFF;
+
+  VOLATILE_WRITE_16(auxilary_buffer->header.hash, hash);
+
+  VOLATILE_WRITE_64(auxilary_buffer->header.magic, AUX_MAGIC);               
+}
+
+void check_auxiliary_config_buffer(auxilary_buffer_t* auxilary_buffer, auxilary_buffer_config_t* shadow_config){
+  uint8_t changed = 0;
+  VOLATILE_READ_8(changed, auxilary_buffer->configuration.changed);
+  if (changed){
+
+  
+    uint8_t aux_byte;
+
+    VOLATILE_READ_8(aux_byte, auxilary_buffer->configuration.redqueen_mode);
+    if(aux_byte){
+      /* enable redqueen mode */
+      if(aux_byte != shadow_config->redqueen_mode){
+        GET_GLOBAL_STATE()->in_redqueen_reload_mode = true;
+        GET_GLOBAL_STATE()->redqueen_enable_pending = true;
+	      GET_GLOBAL_STATE()->redqueen_instrumentation_mode = REDQUEEN_LIGHT_INSTRUMENTATION;
+      }
+    }
+    else{
+      /* disable redqueen mode */
+      if(aux_byte != shadow_config->redqueen_mode){
+        GET_GLOBAL_STATE()->in_redqueen_reload_mode = false;
+        GET_GLOBAL_STATE()->redqueen_disable_pending = true;
+	      GET_GLOBAL_STATE()->redqueen_instrumentation_mode = REDQUEEN_NO_INSTRUMENTATION;
+      }
+    }
+
+    VOLATILE_READ_8(aux_byte, auxilary_buffer->configuration.trace_mode);
+    if(aux_byte){
+      /* enable trace mode */
+      if(aux_byte != shadow_config->trace_mode && GET_GLOBAL_STATE()->redqueen_state){
+		      redqueen_set_trace_mode(GET_GLOBAL_STATE()->redqueen_state);
+      }
+    }
+    else {
+      /* disable trace mode */
+      if(aux_byte != shadow_config->trace_mode && GET_GLOBAL_STATE()->redqueen_state){
+		    redqueen_unset_trace_mode(GET_GLOBAL_STATE()->redqueen_state);
+      }
+    }
+
+    VOLATILE_READ_8(aux_byte, auxilary_buffer->configuration.page_dump_mode);
+    if(aux_byte){
+      GET_GLOBAL_STATE()->dump_page = true;
+      uint64_t data;
+      VOLATILE_READ_64(data, auxilary_buffer->configuration.page_addr);
+      GET_GLOBAL_STATE()->dump_page_addr = data;
+      //fprintf(stderr, "%s dump_page_addr => 0x%lx\n", __func__, GET_GLOBAL_STATE()->dump_page_addr);
+      VOLATILE_WRITE_8(auxilary_buffer->configuration.page_dump_mode, 0);
+      VOLATILE_WRITE_64(auxilary_buffer->configuration.page_addr, 0);
+    }
+
+    /* modify reload mode */
+    VOLATILE_READ_8(aux_byte, auxilary_buffer->configuration.reload_mode);
+    GET_GLOBAL_STATE()->in_reload_mode = aux_byte;
+
+    /* modify protect_payload_buffer */
+    VOLATILE_READ_8(aux_byte, auxilary_buffer->configuration.protect_payload_buffer);
+    GET_GLOBAL_STATE()->protect_payload_buffer = aux_byte;
+    
+    /* copy to shodow */
+    VOLATILE_READ_8(shadow_config->timeout_sec, auxilary_buffer->configuration.timeout_sec);
+    VOLATILE_READ_32(shadow_config->timeout_usec, auxilary_buffer->configuration.timeout_usec);
+
+    //if(shadow_config->timeout_sec || shadow_config->timeout_usec){
+      /* apply only non-zero values */
+      update_itimer(&(GET_GLOBAL_STATE()->timeout_detector), shadow_config->timeout_sec, shadow_config->timeout_usec);
+    //}
+
+    VOLATILE_READ_8(shadow_config->redqueen_mode, auxilary_buffer->configuration.redqueen_mode);
+    VOLATILE_READ_8(shadow_config->trace_mode, auxilary_buffer->configuration.trace_mode);
+    VOLATILE_READ_8(shadow_config->reload_mode, auxilary_buffer->configuration.reload_mode);
+
+    VOLATILE_READ_8(shadow_config->verbose_level, auxilary_buffer->configuration.verbose_level);
+
+    /* reset the 'changed' byte */
+    VOLATILE_WRITE_8(auxilary_buffer->configuration.changed, 0);
+  }
+}
+
+void set_crash_auxiliary_result_buffer(auxilary_buffer_t* auxilary_buffer){
+  VOLATILE_WRITE_8(auxilary_buffer->result.crash_found, 1);
+}
+
+void set_asan_auxiliary_result_buffer(auxilary_buffer_t* auxilary_buffer){
+  VOLATILE_WRITE_8(auxilary_buffer->result.asan_found, 1);
+}
+
+void set_timeout_auxiliary_result_buffer(auxilary_buffer_t* auxilary_buffer){
+  VOLATILE_WRITE_8(auxilary_buffer->result.timeout_found, 1);
+}
+
+void set_reload_auxiliary_result_buffer(auxilary_buffer_t* auxilary_buffer){
+  VOLATILE_WRITE_8(auxilary_buffer->result.reloaded, 1);
+}
+
+void set_pt_overflow_auxiliary_result_buffer(auxilary_buffer_t* auxilary_buffer){
+  VOLATILE_WRITE_8(auxilary_buffer->result.pt_overflow, 1);
+}
+
+void set_exec_done_auxiliary_result_buffer(auxilary_buffer_t* auxilary_buffer, uint8_t sec, uint32_t usec){
+  VOLATILE_WRITE_8(auxilary_buffer->result.exec_done, 1);
+
+  VOLATILE_WRITE_8(auxilary_buffer->result.runtime_sec, sec);
+  VOLATILE_WRITE_32(auxilary_buffer->result.runtime_usec, usec);
+}
+
+void flush_auxiliary_result_buffer(auxilary_buffer_t* auxilary_buffer){
+
+  VOLATILE_WRITE_8(auxilary_buffer->result.exec_done, 0);
+  VOLATILE_WRITE_8(auxilary_buffer->result.hprintf, 0);
+
+
+  VOLATILE_WRITE_8(auxilary_buffer->result.crash_found, 0);
+  VOLATILE_WRITE_8(auxilary_buffer->result.asan_found, 0);
+  VOLATILE_WRITE_8(auxilary_buffer->result.timeout_found, 0);
+  VOLATILE_WRITE_8(auxilary_buffer->result.reloaded, 0);
+  VOLATILE_WRITE_8(auxilary_buffer->result.pt_overflow, 0);
+
+  VOLATILE_WRITE_8(auxilary_buffer->result.runtime_sec, 0);
+  VOLATILE_WRITE_32(auxilary_buffer->result.runtime_usec, 0);
+
+  VOLATILE_WRITE_8(auxilary_buffer->result.page_not_found, 0);
+  VOLATILE_WRITE_64(auxilary_buffer->result.page_addr, 0);
+
+  VOLATILE_WRITE_8(auxilary_buffer->result.payload_buffer_write_attempt_found, 0);
+}
+
+void set_hprintf_auxiliary_buffer(auxilary_buffer_t* auxilary_buffer, char* msg, uint32_t len){
+  VOLATILE_WRITE_16(auxilary_buffer->misc.len, MIN(len, MISC_SIZE-2));
+  volatile_memcpy((void*)&auxilary_buffer->misc.data, (void*)msg, (size_t)MIN(len, MISC_SIZE-2));
+  VOLATILE_WRITE_8(auxilary_buffer->result.hprintf, 1);
+}
+
+void set_crash_reason_auxiliary_buffer(auxilary_buffer_t* auxilary_buffer, char* msg, uint32_t len){
+  VOLATILE_WRITE_16(auxilary_buffer->misc.len, MIN(len, MISC_SIZE-2));
+  volatile_memcpy((void*)&auxilary_buffer->misc.data, (void*)msg, (size_t) MIN(len, MISC_SIZE-2));
+  VOLATILE_WRITE_8(auxilary_buffer->result.crash_found, 1);
+}
+
+void flush_hprintf_auxiliary_buffer(auxilary_buffer_t* auxilary_buffer){
+  VOLATILE_WRITE_8(auxilary_buffer->result.hprintf, 0);
+}
+
+void set_state_auxiliary_result_buffer(auxilary_buffer_t* auxilary_buffer, uint8_t state){
+  VOLATILE_WRITE_8(auxilary_buffer->result.state, state);
+}
+
+void set_page_not_found_result_buffer(auxilary_buffer_t* auxilary_buffer, uint64_t page_addr){
+  VOLATILE_WRITE_8(auxilary_buffer->result.page_not_found, 1);
+  VOLATILE_WRITE_64(auxilary_buffer->result.page_addr, page_addr);
+}
+
+void set_success_auxiliary_result_buffer(auxilary_buffer_t* auxilary_buffer, uint8_t success){
+  VOLATILE_WRITE_8(auxilary_buffer->result.success, success);
+}
+
+void set_payload_buffer_write_reason_auxiliary_buffer(auxilary_buffer_t* auxilary_buffer, char* msg, uint32_t len){
+  VOLATILE_WRITE_16(auxilary_buffer->misc.len, MIN(len, MISC_SIZE-2));
+  volatile_memcpy((void*)&auxilary_buffer->misc.data, (void*)msg, (size_t) MIN(len, MISC_SIZE-2));
+  VOLATILE_WRITE_8(auxilary_buffer->result.payload_buffer_write_attempt_found, 1);
+}
\ No newline at end of file
diff --new-file -ur qemu/pt/auxiliary_buffer.h QEMU-PT/pt/auxiliary_buffer.h
--- qemu/pt/auxiliary_buffer.h	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/auxiliary_buffer.h	2021-08-26 11:21:04.088506002 +0200
@@ -0,0 +1,154 @@
+/*
+
+Copyright (C) 2019 Sergej Schumilo
+
+This file is part of QEMU-PT (HyperTrash / kAFL).
+
+QEMU-PT is free software: you can redistribute it and/or modify
+it under the terms of the GNU General Public License as published by
+the Free Software Foundation, either version 2 of the License, or
+(at your option) any later version.
+
+QEMU-PT is distributed in the hope that it will be useful,
+but WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with QEMU-PT.  If not, see <http://www.gnu.org/licenses/>.
+
+*/
+
+#pragma once
+#include <stdint.h>
+
+#define AUX_BUFFER_SIZE 4096
+
+#define AUX_MAGIC 0x54502d554d4551
+
+#define QEMU_PT_VERSION 1 /* let's start at 1 for the initial version using the aux buffer */
+
+#define HEADER_SIZE 128
+#define CAP_SIZE 256
+#define CONFIG_SIZE 512
+#define STATE_SIZE 512
+#define MISC_SIZE 4096-(HEADER_SIZE+CAP_SIZE+CONFIG_SIZE+STATE_SIZE)
+
+#define ADD_PADDING(max, type) uint8_t type ## _padding [max - sizeof(type)]
+
+typedef struct auxilary_buffer_header_s{
+  uint64_t magic;   /* 0x54502d554d4551 */
+  uint16_t version; 
+  uint16_t hash; 
+  /* more to come */
+} __attribute__((packed)) auxilary_buffer_header_t;
+
+typedef struct auxilary_buffer_cap_s{
+  uint8_t redqueen;
+  uint8_t agent_timeout_detection;
+  /* more to come */
+} __attribute__((packed)) auxilary_buffer_cap_t;
+
+typedef struct auxilary_buffer_config_s{
+  uint8_t changed;  /* set this byte to kick in a rescan of this buffer */
+
+  uint8_t timeout_sec;
+  uint32_t timeout_usec;
+
+  /* trigger to enable / disable different QEMU-PT modes */
+  uint8_t redqueen_mode; 
+  uint8_t trace_mode; 
+  uint8_t reload_mode;
+
+  uint8_t verbose_level;
+
+  uint8_t page_dump_mode;
+  uint64_t page_addr; 
+
+  /* nested mode only */
+  uint8_t protect_payload_buffer; 
+
+  /*  0 -> disabled
+      1 -> decoding
+      2 -> decoding + full disassembling
+  */
+  //uint8_t pt_processing_mode; 
+
+  /* more to come */
+} __attribute__((packed)) auxilary_buffer_config_t;
+
+typedef struct auxilary_buffer_result_s{
+  /*  0 -> booting, 
+      1 -> loader level 1, 
+      2 -> loader level 2, 
+      3 -> fuzzer is ready
+  */
+  uint8_t state; 
+
+  uint8_t hprintf;
+
+  uint8_t exec_done; 
+
+  uint8_t crash_found; 
+  uint8_t asan_found; 
+  uint8_t timeout_found; 
+  uint8_t reloaded;
+  uint8_t pt_overflow;
+
+  uint32_t runtime_usec;
+  uint8_t runtime_sec;
+
+  uint8_t page_not_found;
+  uint64_t page_addr; 
+
+  uint8_t success;
+
+  uint8_t payload_buffer_write_attempt_found; 
+
+  /* more to come */
+} __attribute__((packed)) auxilary_buffer_result_t;
+
+typedef struct auxilary_buffer_misc_s{
+  uint16_t len;
+  uint8_t data;
+  /* non yet */
+} __attribute__((packed)) auxilary_buffer_misc_t;
+
+typedef struct auxilary_buffer_s{
+  auxilary_buffer_header_t header;
+  ADD_PADDING(HEADER_SIZE, auxilary_buffer_header_t);
+
+  auxilary_buffer_cap_t capabilites;
+  ADD_PADDING(CAP_SIZE, auxilary_buffer_cap_t);
+
+  auxilary_buffer_config_t configuration;
+  ADD_PADDING(CONFIG_SIZE, auxilary_buffer_config_t);
+
+  auxilary_buffer_result_t result;
+  ADD_PADDING(STATE_SIZE, auxilary_buffer_result_t);
+
+  auxilary_buffer_misc_t misc;
+  ADD_PADDING(MISC_SIZE, auxilary_buffer_misc_t);
+
+} __attribute__((packed)) auxilary_buffer_t;
+
+void init_auxiliary_buffer(auxilary_buffer_t* auxilary_buffer);
+void check_auxiliary_config_buffer(auxilary_buffer_t* auxilary_buffer, auxilary_buffer_config_t* shadow_config);
+
+void flush_hprintf_auxiliary_buffer(auxilary_buffer_t* auxilary_buffer);
+
+void set_crash_auxiliary_result_buffer(auxilary_buffer_t* auxilary_buffer);
+void set_asan_auxiliary_result_buffer(auxilary_buffer_t* auxilary_buffer);
+void set_timeout_auxiliary_result_buffer(auxilary_buffer_t* auxilary_buffer);
+void set_reload_auxiliary_result_buffer(auxilary_buffer_t* auxilary_buffer);
+void set_pt_overflow_auxiliary_result_buffer(auxilary_buffer_t* auxilary_buffer);
+void flush_auxiliary_result_buffer(auxilary_buffer_t* auxilary_buffer);
+void set_exec_done_auxiliary_result_buffer(auxilary_buffer_t* auxilary_buffer, uint8_t sec, uint32_t usec);
+void set_state_auxiliary_result_buffer(auxilary_buffer_t* auxilary_buffer, uint8_t state);
+void set_hprintf_auxiliary_buffer(auxilary_buffer_t* auxilary_buffer, char* msg, uint32_t len);
+
+void set_page_not_found_result_buffer(auxilary_buffer_t* auxilary_buffer, uint64_t page_addr);
+void set_success_auxiliary_result_buffer(auxilary_buffer_t* auxilary_buffer, uint8_t success);
+void set_crash_reason_auxiliary_buffer(auxilary_buffer_t* auxilary_buffer, char* msg, uint32_t len);
+void set_payload_buffer_write_reason_auxiliary_buffer(auxilary_buffer_t* auxilary_buffer, char* msg, uint32_t len);
+
diff --new-file -ur qemu/pt/block_cow.c QEMU-PT/pt/block_cow.c
--- qemu/pt/block_cow.c	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/block_cow.c	2021-08-24 21:54:55.938586449 +0200
@@ -0,0 +1,419 @@
+#include <stdint.h>
+#include <sys/types.h>
+#include <sys/mman.h>
+#include "block_cow.h"
+#include "sysemu/block-backend.h"
+#include "fast_vm_reload.h"
+#include "pt/state.h"
+
+
+//#define COW_CACHE_DEBUG
+//#define COW_CACHE_VERBOSE
+
+#define CHUNK_SIZE 0x1000 
+//0x200
+#define PAGE_MASK 0xFFFFFFFFFFFFF000
+
+cow_cache_t* cow_cache_new(const char* filename){
+
+	printf("%s: \"%s\"\n", __func__, filename);
+
+	cow_cache_t* self = malloc(sizeof(cow_cache_t));
+	self->lookup_primary = kh_init(COW_CACHE);
+	self->lookup_secondary = kh_init(COW_CACHE);
+
+	self->data_primary = mmap(NULL, COW_CACHE_SIZE, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS, 0, 0);
+	assert(self->data_primary != MAP_FAILED);
+	//memset(self->data_primary, COW_CACHE_SIZE/CHUNK_SIZE, CHUNK_SIZE);
+
+	self->data_secondary = mmap(NULL, COW_CACHE_SECONDARY_SIZE, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS, 0, 0);
+	assert(self->data_secondary != MAP_FAILED);
+
+	self->filename = strdup(basename(filename));
+	self->offset_primary = 0;
+	self->offset_secondary = 0;
+	self->enabled = true;
+	self->enabled_fuzz = false;
+
+	return self;
+}
+
+static char* gen_file_name(cow_cache_t* self, const char* filename_prefix, const char* filename_postfix){
+	char* tmp1;
+	char* tmp2;
+
+	assert(asprintf(&tmp2, "%s", self->filename) != -1);
+
+	for(int i = 0; i < strlen(tmp2); i++){
+		if(tmp2[i] == '/'){
+			tmp2[i] = '_';
+		}
+	}
+
+	assert(asprintf(&tmp1, "%s_%s.%s", filename_prefix, tmp2, filename_postfix) != -1);
+
+	free(tmp2);
+
+	return tmp1;
+}
+
+void read_primary_buffer(cow_cache_t* self, const char* filename_prefix, bool switch_mode){
+	assert(!self->enabled_fuzz);
+
+	printf("%s: %s\n", __func__, self->filename);
+
+	char* tmp1;
+	char* tmp2;
+
+	//assert(asprintf(&tmp1, "%s_%s.khash", filename_prefix, self->filename) != -1);
+	//assert(asprintf(&tmp2, "%s_%s.pcow", filename_prefix, self->filename) != -1);
+
+	tmp1 = gen_file_name(self, filename_prefix, "khash");
+	tmp2 = gen_file_name(self, filename_prefix, "pcow");
+
+	printf("%s\n", tmp1);
+	kh_destroy(COW_CACHE, self->lookup_primary);
+
+	struct stat buffer;   
+  assert(stat (tmp2, &buffer) == 0);
+
+	if(buffer.st_size){
+		self->lookup_primary = kh_load(COW_CACHE, tmp1);
+	}
+	else {
+		self->lookup_primary = kh_init(COW_CACHE);
+	}
+
+	int fd = open(tmp2, O_RDONLY);
+	
+	if(switch_mode){
+		self->data_primary = mmap(0, COW_CACHE_SIZE, PROT_READ, MAP_SHARED, fd, 0);
+		assert(self->data_primary);
+	}
+	else{
+		void* ptr = mmap(0, COW_CACHE_SIZE, PROT_READ , MAP_SHARED, fd, 0);
+		assert(ptr);
+		memcpy(self->data_primary, ptr, buffer.st_size);
+		munmap(ptr, COW_CACHE_SIZE);
+	}
+	printf("self->data_primary  -> %p\n", self->data_primary );
+	close(fd);
+
+	self->offset_primary = buffer.st_size;
+	fprintf(stderr, "self->offset_primary: %lx\n", self->offset_primary);
+
+	if(switch_mode){
+    switch_to_fuzz_mode(self);
+	}
+
+	free(tmp1);
+	free(tmp2);
+
+}
+
+void dump_primary_buffer(cow_cache_t* self, const char* filename_prefix){
+	assert(self->enabled_fuzz);
+
+	printf("%s: %s\n", __func__, self->filename);
+
+	char* tmp1;
+	char* tmp2;
+
+	//assert(asprintf(&tmp1, "%s_%s.khash", filename_prefix, self->filename) != -1);
+	//assert(asprintf(&tmp2, "%s_%s.pcow", filename_prefix, self->filename) != -1);
+
+	tmp1 = gen_file_name(self, filename_prefix, "khash");
+	tmp2 = gen_file_name(self, filename_prefix, "pcow");
+
+	printf("%s\n", tmp1);
+	if(self->offset_primary){
+		kh_write(COW_CACHE, self->lookup_primary, tmp1);
+	}
+	else{
+		fclose(fopen(tmp1, "wb"));
+	}
+
+	FILE *fp = fopen(tmp2, "wb");
+	if(fp == NULL) {
+		fprintf(stderr, "[%s] Could not open file %s.\n", __func__, tmp2);
+		assert(false);
+		//exit(EXIT_FAILURE);
+	}
+
+	if(self->offset_primary){
+		fwrite(self->data_primary, CHUNK_SIZE, self->offset_primary/CHUNK_SIZE, fp);
+	}
+	fprintf(stderr, "self->offset_primary: %lx\n", self->offset_primary);
+
+
+	fclose(fp);
+
+	free(tmp1);
+	free(tmp2);
+
+}
+
+void cow_cache_reset(cow_cache_t* self){
+	if(!self->enabled_fuzz)
+		return;
+	/* TODO */
+	assert(self->enabled_fuzz);
+
+	//fprintf(stderr, "RESETING COW STUFF YO %s (%lx)\n", self->filename, self->offset_secondary);
+
+	self->offset_secondary = 0;
+	kh_clear(COW_CACHE, self->lookup_secondary);
+}
+
+
+void cow_cache_enable(cow_cache_t* self){
+	cow_cache_reset(self);
+	self->enabled = true;
+}
+
+
+void cow_cache_disable(cow_cache_t* self){
+	cow_cache_reset(self);
+	self->enabled = false;
+}
+
+typedef struct BlkRwCo {
+    BlockBackend *blk;
+    int64_t offset;
+    QEMUIOVector *qiov;
+    int ret;
+    BdrvRequestFlags flags;
+} BlkRwCo;
+
+typedef struct BlkAioEmAIOCB {
+    BlockAIOCB common;
+    BlkRwCo rwco;
+    int bytes;
+    bool has_returned;
+} BlkAioEmAIOCB;
+
+extern void blk_aio_write_entry(void *opaque);
+extern int blk_check_byte_request(BlockBackend *blk, int64_t offset, size_t size);
+extern void blk_aio_complete(BlkAioEmAIOCB *acb);
+
+/* read from primary buffer */
+static inline void read_from_primary_buffer(cow_cache_t* self, BlockBackend *blk, int64_t offset,  unsigned int bytes, QEMUIOVector *qiov, BdrvRequestFlags flags, uint64_t offset_addr, uint64_t iov_offset){
+	khiter_t k;
+
+	k = kh_get(COW_CACHE, self->lookup_primary, offset_addr); 
+	if(k != kh_end(self->lookup_primary)){
+		#ifdef COW_CACHE_DEBUG
+		printf("[PRE ] READ DIRTY COW PAGE: ADDR: %lx IOVEC OFFSET: %lx DATA OFFSET: %lx\n", offset_addr, iov_offset, self->offset_primary);
+		#endif
+		qemu_iovec_from_buf(qiov, iov_offset, self->data_primary + kh_value(self->lookup_primary, k), CHUNK_SIZE);
+	}
+	return; 
+}
+
+/* try to read from secondary buffer
+ * read from primary buffer if the data is not available yet */
+static inline void read_from_secondary_buffer(cow_cache_t* self, BlockBackend *blk, int64_t offset,  unsigned int bytes, QEMUIOVector *qiov, BdrvRequestFlags flags, uint64_t offset_addr, uint64_t iov_offset){
+	khiter_t k = kh_get(COW_CACHE, self->lookup_secondary, offset_addr); 
+	if(k != kh_end(self->lookup_secondary)){
+		#ifdef COW_CACHE_DEBUG
+		printf("[FUZZ] READ DIRTY COW PAGE: ADDR: %lx IOVEC OFFSET: %lx DATA OFFSET: %lx\n", offset_addr, iov_offset, self->offset_secondary);
+		#endif
+		qemu_iovec_from_buf(qiov, iov_offset, self->data_secondary + kh_value(self->lookup_secondary, k), CHUNK_SIZE);
+	}
+	else{
+		k = kh_get(COW_CACHE, self->lookup_primary, offset_addr);
+		if(k != kh_end(self->lookup_primary)){
+			#ifdef COW_CACHE_DEBUG
+			printf("[PRE ] READ DIRTY COW PAGE: ADDR: %lx IOVEC OFFSET: %lx DATA OFFSET: %lx\n", offset_addr, iov_offset, self->offset_primary);
+			#endif
+			qemu_iovec_from_buf(qiov, iov_offset, self->data_primary + kh_value(self->lookup_primary, k), CHUNK_SIZE);
+		}
+	}
+}
+
+/* read data from cow cache */
+static int coroutine_fn cow_cache_read(cow_cache_t* self, BlockBackend *blk, int64_t offset,  unsigned int bytes, QEMUIOVector *qiov, BdrvRequestFlags flags){
+	int ret;
+
+    ret = blk_check_byte_request(blk, offset, bytes);
+	if (ret < 0) {
+        return ret;
+    }
+
+    if ((qiov->size%CHUNK_SIZE)){
+#ifdef COW_CACHE_DEBUG
+			fprintf(stderr, "%s: FAILED %lx!\n", __func__, qiov->size);
+#endif
+    	return 0;
+    }
+    assert(!(qiov->size%CHUNK_SIZE));
+	
+	uint64_t iov_offset = 0;
+	for(uint64_t offset_addr = offset; offset_addr < (offset+(qiov->size)); offset_addr+= CHUNK_SIZE){
+
+		if(self->enabled_fuzz){
+			read_from_secondary_buffer(self, blk, offset, bytes, qiov, flags, offset_addr, iov_offset);
+		}
+		else{
+			read_from_primary_buffer(self, blk, offset, bytes, qiov, flags, offset_addr, iov_offset);
+		}
+
+		iov_offset+= CHUNK_SIZE;
+	}
+
+	return qiov->size;
+}
+
+
+/* write to primary buffer */
+static inline void write_to_primary_buffer(cow_cache_t* self, BlockBackend *blk, int64_t offset,  unsigned int bytes, QEMUIOVector *qiov, BdrvRequestFlags flags, uint64_t offset_addr, uint64_t iov_offset){
+	int ret;
+	khiter_t k;
+
+	k = kh_get(COW_CACHE, self->lookup_primary, offset_addr); 
+	if(unlikely(k == kh_end(self->lookup_primary))){
+		/* create page */
+
+		k = kh_put(COW_CACHE, self->lookup_primary, offset_addr, &ret);
+		#ifdef COW_CACHE_DEBUG
+		printf("ADD NEW COW PAGE: ADDR: %lx IOVEC OFFSET: %lx DATA OFFSET: %lx\n", offset_addr, iov_offset, self->offset_primary);
+		#endif
+
+
+		kh_value(self->lookup_primary, k) = self->offset_primary;
+
+		self->offset_primary += CHUNK_SIZE;
+
+		#ifdef COW_CACHE_VERBOSE
+		printf("COW CACHE IS 0x%lx BYTES (KB: %ld / MB: %ld / GB: %ld) IN SIZE!\n", self->offset, self->offset >> 10, self->offset >> 20, self->offset >> 30);
+		#endif		
+
+		/* IN CASE THE BUFFER IS FULL -> ABORT! */
+		assert(self->offset_primary < COW_CACHE_SIZE);
+	}
+
+	#ifdef COW_CACHE_DEBUG
+	printf("LOAD COW PAGE: ADDR: %lx IOVEC OFFSET: %lx DATA OFFSET: %lx (%s)\n", offset_addr, iov_offset, kh_value(self->lookup_primary, k), self->filename);
+	#endif
+
+	/* write to cached page */
+	qemu_iovec_to_buf(qiov, iov_offset, self->data_primary + kh_value(self->lookup_primary, k), CHUNK_SIZE);
+	
+}
+
+/* try to write to secondary buffer
+ * copy from primary buffer if the data is already cached */
+static inline void write_to_secondary_buffer(cow_cache_t* self, BlockBackend *blk, int64_t offset,  unsigned int bytes, QEMUIOVector *qiov, BdrvRequestFlags flags, uint64_t offset_addr, uint64_t iov_offset){
+	int ret;
+	khiter_t k_primary = kh_get(COW_CACHE, self->lookup_primary, offset_addr); 
+	khiter_t k_secondary = kh_get(COW_CACHE, self->lookup_secondary, offset_addr); 
+
+	/* IN CASE THE BUFFER IS FULL -> ABORT! */
+	//assert(self->offset_secondary <= COW_CACHE_SIZE);
+	if(self->offset_secondary >= COW_CACHE_SECONDARY_SIZE){
+		GET_GLOBAL_STATE()->cow_cache_full = true;
+		return;
+	}
+
+	if(unlikely(k_secondary == kh_end(self->lookup_secondary))){
+		/* if page is not cached in secondary buffer yet */
+		if(k_primary != kh_end(self->lookup_primary)){
+			/* copy page */
+			k_secondary = kh_put(COW_CACHE, self->lookup_secondary, offset_addr, &ret);
+			kh_value(self->lookup_secondary, k_secondary) = self->offset_secondary;
+			self->offset_secondary += CHUNK_SIZE;
+			memcpy(self->data_secondary + kh_value(self->lookup_secondary, k_secondary), self->data_primary + kh_value(self->lookup_primary, k_primary), CHUNK_SIZE);
+			//printf("[COPY] COW CACHE IS 0x%lx BYTES (KB: %ld / MB: %ld / GB: %ld) IN SIZE!\n", self->offset_secondary, self->offset_secondary >> 10, self->offset_secondary >> 20, self->offset_secondary >> 30);
+		}
+		else{
+			/* create page */
+			k_secondary = kh_put(COW_CACHE, self->lookup_secondary, offset_addr, &ret);
+			kh_value(self->lookup_secondary, k_secondary) = self->offset_secondary;
+			self->offset_secondary += CHUNK_SIZE;
+			//printf("[INIT] COW CACHE IS 0x%lx BYTES (KB: %ld / MB: %ld / GB: %ld) IN SIZE!\n", self->offset_secondary, self->offset_secondary >> 10, self->offset_secondary >> 20, self->offset_secondary >> 30);
+		}
+
+
+		/* IN CASE THE BUFFER IS FULL -> ABORT! */
+		assert(self->offset_secondary <= COW_CACHE_SECONDARY_SIZE);
+	}
+
+	/* write to cached page */
+	qemu_iovec_to_buf(qiov, iov_offset, self->data_secondary + kh_value(self->lookup_secondary, k_secondary), CHUNK_SIZE);
+}
+
+/* write data to cow cache */
+static int coroutine_fn cow_cache_write(cow_cache_t* self, BlockBackend *blk, int64_t offset,  unsigned int bytes, QEMUIOVector *qiov, BdrvRequestFlags flags){
+	//khiter_t k;
+	int ret;
+
+    ret = blk_check_byte_request(blk, offset, bytes);
+	if (ret < 0) {
+        return ret;
+    }
+
+	if ((qiov->size%CHUNK_SIZE)){
+#ifdef COW_CACHE_DEBUG
+		fprintf(stderr, "%s: FAILED %lx!\n", __func__, qiov->size);
+#endif
+  	return 0;
+  }
+	if((qiov->size%CHUNK_SIZE) && GET_GLOBAL_STATE()->in_fuzzing_mode){
+		GET_GLOBAL_STATE()->cow_cache_full = true;
+		fprintf(stderr, "WARNING: %s write in %lx CHUNKSIZE\n", __func__, qiov->size);
+		return 0;
+	}
+	else{
+		assert(!(qiov->size%CHUNK_SIZE));
+	}
+	
+	uint64_t iov_offset = 0;
+	for(uint64_t offset_addr = offset; offset_addr < (offset+(qiov->size)); offset_addr+= CHUNK_SIZE){
+		if(self->enabled_fuzz){
+			write_to_secondary_buffer(self, blk, offset, bytes, qiov, flags, offset_addr, iov_offset);
+		}
+		else{
+			write_to_primary_buffer(self, blk, offset, bytes, qiov, flags, offset_addr, iov_offset);
+		}
+
+		iov_offset+= CHUNK_SIZE;
+	}
+
+	return qiov->size;
+}
+
+void switch_to_fuzz_mode(cow_cache_t* self){
+	self->enabled_fuzz = true;
+	assert(!mprotect(self->data_primary, COW_CACHE_SIZE, PROT_READ));
+	printf("SWITCHED TO SECONDARY BUFFER\n");
+}
+
+void cow_cache_read_entry(void* opaque){
+
+    BlkAioEmAIOCB *acb = opaque;
+    BlkRwCo *rwco = &acb->rwco;
+
+#ifdef COW_CACHE_DEBUG
+        printf("%s %lx %lx\n", __func__,  rwco->offset, acb->bytes);
+#endif
+
+
+    rwco->ret = blk_co_preadv(rwco->blk, rwco->offset, acb->bytes, rwco->qiov, rwco->flags);
+
+    cow_cache_read( *((cow_cache_t**)(rwco->blk)), rwco->blk, rwco->offset, acb->bytes, rwco->qiov, rwco->flags);
+    blk_aio_complete(acb);
+}
+
+
+void cow_cache_write_entry(void* opaque){
+    BlkAioEmAIOCB *acb = opaque;
+    BlkRwCo *rwco = &acb->rwco;
+
+#ifdef COW_CACHE_DEBUG
+        printf("%s\n", __func__);
+#endif
+
+    rwco->ret = cow_cache_write( *((cow_cache_t**)(rwco->blk)), rwco->blk, rwco->offset, acb->bytes, rwco->qiov, rwco->flags);
+
+    blk_aio_complete(acb);
+}
diff --new-file -ur qemu/pt/block_cow.h QEMU-PT/pt/block_cow.h
--- qemu/pt/block_cow.h	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/block_cow.h	2021-08-24 21:54:55.938586449 +0200
@@ -0,0 +1,53 @@
+#pragma once 
+
+
+#include <stdint.h>
+#include <sys/types.h>
+#include "khash.h"
+
+#include "qemu/osdep.h"
+#include "block/block.h"
+
+
+/* 2GB Cache */
+//#define COW_CACHE_SIZE 0x80000000 
+
+// 3GB
+#define COW_CACHE_SIZE 0xC0000000
+
+// 512MB
+#define COW_CACHE_SECONDARY_SIZE 0x20000000 
+
+
+KHASH_MAP_INIT_INT64(COW_CACHE, uint64_t)
+
+typedef struct cow_cache_s{
+	khash_t(COW_CACHE) *lookup_primary;
+	khash_t(COW_CACHE) *lookup_secondary;
+	void* data_primary;
+	void* data_secondary;
+
+	char* filename;
+	uint64_t offset_primary;
+	uint64_t offset_secondary;
+
+	bool enabled;
+	bool enabled_fuzz;
+} cow_cache_t;
+
+cow_cache_t* cow_cache_new(const char* filename);
+void cow_cache_reset(cow_cache_t* self);
+//int coroutine_fn cow_cache_read(cow_cache_t* self, BlockBackend *blk, int64_t offset,  unsigned int bytes, QEMUIOVector *qiov, BdrvRequestFlags flags);
+//int coroutine_fn cow_cache_write(cow_cache_t* self, BlockBackend *blk, int64_t offset,  unsigned int bytes, QEMUIOVector *qiov, BdrvRequestFlags flags);
+
+
+void switch_to_fuzz_mode(cow_cache_t* self);
+
+void read_primary_buffer(cow_cache_t* self, const char* filename_prefix, bool switch_mode);
+void dump_primary_buffer(cow_cache_t* self, const char* filename_prefix);
+
+void cow_cache_read_entry(void* opaque);
+void cow_cache_write_entry(void* opaque);
+
+void cow_cache_enable(cow_cache_t* self);
+void cow_cache_disable(cow_cache_t* self);
\ No newline at end of file
diff --new-file -ur qemu/pt/debug.c QEMU-PT/pt/debug.c
--- qemu/pt/debug.c	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/debug.c	2021-08-24 21:54:55.938586449 +0200
@@ -0,0 +1,101 @@
+#include <execinfo.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <unistd.h>
+#include "pt/debug.h"
+#include "signal.h"
+
+#ifdef ENABLE_BACKTRACES
+#define BT_BUF_SIZE 100
+
+void qemu_backtrace(void){
+  void *buffer[BT_BUF_SIZE];
+  int nptrs = 0;
+  int j;
+
+  nptrs = backtrace(buffer, BT_BUF_SIZE);
+  fprintf(stderr, "backtrace() returned %d addresses\n", nptrs);
+
+
+  char **strings = backtrace_symbols(buffer, nptrs);
+  if (strings == NULL) {
+      fprintf(stderr, "backtrace_symbols failed!\n");
+      return;
+  }
+
+  for (j = 0; j < nptrs; j++)
+      fprintf(stderr, "%s\n", strings[j]);
+
+  free(strings);
+}
+
+static void sigsegfault_handler(int signo, siginfo_t *info, void *extra) {
+  fprintf(stderr, "CRASH DETECTED (PID: %d / SIGNAL: %d)\n", getpid(), signo);
+  qemu_backtrace();
+  fprintf(stderr, "WAITING FOR GDB TO ATTACH (PID: %d ...\n", getpid());
+  while(1){
+    sleep(1);
+  }
+}
+
+static void sigabrt_handler(int signo, siginfo_t *info, void *extra) {
+  fprintf(stderr, "CRASH DETECTED (PID: %d / SIGNAL: %d)\n", getpid(), signo);
+  qemu_backtrace();
+  fprintf(stderr, "WAITING FOR GDB TO ATTACH (PID: %d ...\n", getpid());
+  while(1){
+    sleep(1);
+  }
+}
+
+void init_crash_handler(void){
+
+  //qemu_backtrace();
+
+    struct sigaction action;
+    action.sa_flags = SA_SIGINFO;
+    action.sa_sigaction = sigsegfault_handler;
+
+    if (sigaction(SIGSEGV, &action, NULL) == -1) {
+        fprintf(stderr, "SIGSEGV: sigaction failed");
+        _exit(1);
+    }
+
+  
+    action.sa_sigaction = sigabrt_handler;
+
+    if (sigaction(SIGABRT, &action, NULL) == -1) {
+        fprintf(stderr, "SIGABRT: sigaction failed");
+        _exit(1);
+    }
+}
+
+void hexdump_kafl(const void* data, size_t size) {
+	char ascii[17];
+	size_t i, j;
+	ascii[16] = '\0';
+	for (i = 0; i < size; ++i) {
+		printf("%02X ", ((unsigned char*)data)[i]);
+		if (((unsigned char*)data)[i] >= ' ' && ((unsigned char*)data)[i] <= '~') {
+			ascii[i % 16] = ((unsigned char*)data)[i];
+		} else {
+			ascii[i % 16] = '.';
+		}
+		if ((i+1) % 8 == 0 || i+1 == size) {
+			printf(" ");
+			if ((i+1) % 16 == 0) {
+				printf("|  %s \n", ascii);
+			} else if (i+1 == size) {
+				ascii[(i+1) % 16] = '\0';
+				if ((i+1) % 16 <= 8) {
+					printf(" ");
+				}
+				for (j = (i+1) % 16; j < 16; ++j) {
+					printf("   ");
+				}
+				printf("|  %s \n", ascii);
+			}
+		}
+	}
+}
+
+#endif
diff --new-file -ur qemu/pt/debug.h QEMU-PT/pt/debug.h
--- qemu/pt/debug.h	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/debug.h	2021-08-24 21:54:55.938586449 +0200
@@ -0,0 +1,42 @@
+#pragma once
+
+#include <execinfo.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <unistd.h>
+
+#define ENABLE_BACKTRACES
+
+#define QEMU_PT_PRINT_PREFIX  "[QEMU-PT]\t"
+#define CORE_PREFIX           "Core:      "
+#define MEM_PREFIX            "Memory:    "
+#define RELOAD_PREFIX         "Reload:    "
+#define PT_PREFIX             "PT:        "
+#define INTERFACE_PREFIX      "Interface: "
+#define REDQUEEN_PREFIX       "Redqueen:  "
+#define DISASM_PREFIX         "Disasm:    "
+#define PAGE_CACHE_PREFIX     "PageCache: "
+#define INTERFACE_PREFIX      "Interface: "
+#define NESTED_VM_PREFIX      "Nested:    "
+
+
+#define DEBUG_VM_PREFIX       "Debug:     "
+
+#define COLOR	"\033[1;35m"
+#define ENDC	"\033[0m"
+
+
+#define QEMU_PT_PRINTF(PREFIX, format, ...) printf (QEMU_PT_PRINT_PREFIX COLOR PREFIX format ENDC "\n", ##__VA_ARGS__)
+#define QEMU_PT_PRINTF_DBG(PREFIX, format, ...) printf (QEMU_PT_PRINT_PREFIX PREFIX "(%s#:%d)\t"format, __BASE_FILE__, __LINE__, ##__VA_ARGS__)
+
+
+#define QEMU_PT_PRINTF_DEBUG(format, ...)  
+//fprintf (stderr, QEMU_PT_PRINT_PREFIX DEBUG_VM_PREFIX "(%s#:%d)\t"format "\n", __BASE_FILE__, __LINE__, ##__VA_ARGS__)
+
+#ifdef ENABLE_BACKTRACES
+
+void qemu_backtrace(void);
+void init_crash_handler(void);
+void hexdump_kafl(const void* data, size_t size);
+
+#endif
\ No newline at end of file
diff --new-file -ur qemu/pt/decoder.c QEMU-PT/pt/decoder.c
--- qemu/pt/decoder.c	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/decoder.c	2021-08-24 21:54:55.942586446 +0200
@@ -0,0 +1,996 @@
+/*
+
+Copyright (C) 2017 Sergej Schumilo
+
+This file is part of QEMU-PT (kAFL).
+
+QEMU-PT is free software: you can redistribute it and/or modify
+it under the terms of the GNU General Public License as published by
+the Free Software Foundation, either version 2 of the License, or
+(at your option) any later version.
+
+QEMU-PT is distributed in the hope that it will be useful,
+but WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with QEMU-PT.  If not, see <http://www.gnu.org/licenses/>.
+
+Note: 
+This Intel PT software decoder is partially inspired and based on Andi 
+Kleen's fastdecode.c (simple-pt). m
+See: https://github.com/andikleen/simple-pt/blob/master/fastdecode.c
+
+ * Simple PT dumper
+ *
+ * Copyright (c) 2015, Intel Corporation
+ * Author: Andi Kleen
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * 2. Redistributions in binary form must reproduce the above copyright
+ * notice, this list of conditions and the following disclaimer in the
+ * documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
+ * FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
+ * COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT,
+ * INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+ * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
+ * STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
+ * OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+
+int error_counter = 0;
+
+#define _GNU_SOURCE 1
+
+#define COMPUTED_GOTOS
+
+#ifdef STANDALONE_DECODER
+#include "decoder.h"
+#include <assert.h>
+#include "trace_cache.h"
+#define DEBUG
+
+#ifdef DEBUG
+//#define WRITE_SAMPLE_DECODED_DETAILED(format, ...) (sample_decoded_detailed(format, ##__VA_ARGS__))
+
+void sample_decoded_detailed(const char *format, ...){
+	va_list args;
+	va_start(args, format);
+	vprintf(format, args);
+	va_end(args);
+}
+#endif
+
+#else
+#include "pt/decoder.h"
+#include "pt/state.h"
+#endif
+
+//#define LEFT(x) ((end - p) >= (x))
+//#define BIT(x) (1U << (x))
+
+#define BENCHMARK 				1
+
+
+#define PT_TRACE_END			0b01010101
+
+#define PT_PKT_GENERIC_LEN		2
+#define PT_PKT_GENERIC_BYTE0	0b00000010
+
+#define PT_PKT_LTNT_LEN			8
+#define PT_PKT_LTNT_BYTE0		PT_PKT_GENERIC_BYTE0
+#define PT_PKT_LTNT_BYTE1		0b10100011
+
+#define PT_PKT_PIP_LEN			8
+#define PT_PKT_PIP_BYTE0		PT_PKT_GENERIC_BYTE0
+#define PT_PKT_PIP_BYTE1		0b01000011
+
+#define PT_PKT_CBR_LEN			4
+#define PT_PKT_CBR_BYTE0		PT_PKT_GENERIC_BYTE0
+#define PT_PKT_CBR_BYTE1		0b00000011
+
+#define PT_PKT_OVF_LEN			2
+#define PT_PKT_OVF_BYTE0		PT_PKT_GENERIC_BYTE0
+#define PT_PKT_OVF_BYTE1		0b11110011
+
+#define PT_PKT_PSB_LEN			16
+#define PT_PKT_PSB_BYTE0		PT_PKT_GENERIC_BYTE0
+#define PT_PKT_PSB_BYTE1		0b10000010
+
+#define PT_PKT_PSBEND_LEN		2
+#define PT_PKT_PSBEND_BYTE0		PT_PKT_GENERIC_BYTE0
+#define PT_PKT_PSBEND_BYTE1		0b00100011
+
+#define PT_PKT_MNT_LEN			11
+#define PT_PKT_MNT_BYTE0		PT_PKT_GENERIC_BYTE0
+#define PT_PKT_MNT_BYTE1		0b11000011
+#define PT_PKT_MNT_BYTE2		0b10001000
+
+#define PT_PKT_TMA_LEN			7
+#define PT_PKT_TMA_BYTE0		PT_PKT_GENERIC_BYTE0
+#define PT_PKT_TMA_BYTE1		0b01110011
+
+#define PT_PKT_VMCS_LEN			7
+#define PT_PKT_VMCS_BYTE0		PT_PKT_GENERIC_BYTE0
+#define PT_PKT_VMCS_BYTE1		0b11001000
+
+#define	PT_PKT_TS_LEN			2
+#define PT_PKT_TS_BYTE0			PT_PKT_GENERIC_BYTE0
+#define PT_PKT_TS_BYTE1			0b10000011
+
+#define PT_PKT_MODE_LEN			2
+#define PT_PKT_MODE_BYTE0		0b10011001
+
+#define PT_PKT_TIP_LEN			8
+#define PT_PKT_TIP_SHIFT		5
+#define PT_PKT_TIP_MASK			0b00011111
+#define PT_PKT_TIP_BYTE0		0b00001101
+#define PT_PKT_TIP_PGE_BYTE0	0b00010001
+#define PT_PKT_TIP_PGD_BYTE0	0b00000001
+#define PT_PKT_TIP_FUP_BYTE0	0b00011101
+
+
+#define TIP_VALUE_0				(0x0<<5)
+#define TIP_VALUE_1				(0x1<<5)
+#define TIP_VALUE_2				(0x2<<5)
+#define TIP_VALUE_3				(0x3<<5)
+#define TIP_VALUE_4				(0x4<<5)
+#define TIP_VALUE_5				(0x5<<5)
+#define TIP_VALUE_6				(0x6<<5)
+#define TIP_VALUE_7				(0x7<<5)
+
+//#define DEBUG
+
+static decoder_state_machine_t* decoder_statemachine_new(void);
+static void decoder_statemachine_reset(decoder_state_machine_t* self);
+
+static uint8_t psb[16] = {
+	0x02, 0x82, 0x02, 0x82, 0x02, 0x82, 0x02, 0x82,
+	0x02, 0x82, 0x02, 0x82, 0x02, 0x82, 0x02, 0x82
+};
+
+#ifdef DECODER_LOG
+static void flush_log(decoder_t* self){
+	self->log.tnt64 = 0;
+	self->log.tnt8 = 0;
+	self->log.pip = 0;
+	self->log.cbr = 0;
+	self->log.ts = 0;
+	self->log.ovf = 0;
+	self->log.psbc = 0;
+	self->log.psbend = 0;
+	self->log.mnt = 0;
+	self->log.tma = 0;
+	self->log.vmcs = 0;
+	self->log.pad = 0;
+	self->log.tip = 0;
+	self->log.tip_pge = 0;
+	self->log.tip_pgd = 0;
+	self->log.tip_fup = 0;
+	self->log.mode = 0;
+}
+#endif
+
+#ifndef STANDALONE_DECODER
+decoder_t* pt_decoder_init(uint64_t filter[4][2], int disassembler_word_width, redqueen_t *redqueen_state, page_cache_t* page_cache){
+#else
+decoder_t* pt_decoder_init(uint64_t filter[4][2], int disassembler_word_width, page_cache_t* page_cache){
+#endif
+	decoder_t* res = malloc(sizeof(decoder_t));
+
+	res->page_fault_found = false;
+	res->page_fault_addr = 0;
+
+	res->ovp_state = false;
+	res->last_tip = 0;
+  res->last_fup_src = 0;
+	res->fup_bind_pending = false;
+#ifdef DECODER_LOG
+	flush_log(res);
+#endif
+#ifndef STANDALONE_DECODER
+	res->disassembler_state = init_disassembler(filter, disassembler_word_width, redqueen_state, page_cache);	
+#else
+	res->disassembler_state = init_disassembler(filter, disassembler_word_width, page_cache);	
+#endif
+	res->tnt_cache_state = tnt_cache_init();
+		/* ToDo: Free! */
+	res->decoder_state = decoder_statemachine_new();
+	res->decoder_state_result = malloc(sizeof(should_disasm_t));
+	res->decoder_state_result->start = 0;
+	res->decoder_state_result->valid = 0;
+	res->decoder_state_result->valid = false;
+	res->mode = mode_64;
+
+	return res;
+}
+
+void pt_decoder_destroy(decoder_t* self){
+	if(self->tnt_cache_state){
+		destroy_disassembler(self->disassembler_state);
+		tnt_cache_destroy(self->tnt_cache_state);
+		self->tnt_cache_state = NULL;
+	}
+	free(self->decoder_state_result);
+	free(self->decoder_state);
+	free(self);
+}
+
+void pt_decoder_flush(decoder_t* self){
+	self->ovp_state = false;
+	self->last_tip = 0;
+	self->last_fup_src = 0;
+	self->fup_bind_pending = false;
+#ifdef DECODER_LOG
+	flush_log(self);
+#endif
+
+	tnt_cache_flush(self->tnt_cache_state);
+	disassembler_flush(self->disassembler_state);
+	decoder_statemachine_reset(self->decoder_state);
+	self->decoder_state_result->start = 0;
+	self->decoder_state_result->valid = 0;
+	self->decoder_state_result->valid = false;
+}	
+
+void pt_decoder_reset(decoder_t* self){
+	reset_disassembler(self->disassembler_state);
+}
+
+uint64_t pt_decoder_get_page_fault_addr(decoder_t* self){
+	return self->page_fault_addr;
+}
+
+static inline void _set_disasm(should_disasm_t* self, uint64_t from, uint64_t to){
+	self->valid = true;
+	self->start = from;
+	self->end = to;
+}
+
+static decoder_state_machine_t* decoder_statemachine_new(void){
+	decoder_state_machine_t * res = (decoder_state_machine_t*)malloc(sizeof(decoder_state_machine_t));
+	res->state = TraceDisabled;
+	res->last_ip = 0;
+	return res;
+}
+
+static void decoder_statemachine_reset(decoder_state_machine_t* self){
+	self->state = TraceDisabled;
+	self->last_ip = 0;
+}
+
+static inline void decoder_handle_tip(decoder_state_machine_t *self, uint64_t addr, should_disasm_t *res){
+	res->valid= false;
+	switch(self->state){
+		case TraceDisabled:
+			_set_disasm(res, addr, 0);
+			self->state = TraceEnabledWithLastIP;
+			self->last_ip = addr;
+			//assert(false);
+			break;
+		case TraceEnabledWithLastIP:
+			_set_disasm(res, self->last_ip, 0);
+			self->state = TraceEnabledWithLastIP;
+			self->last_ip = addr;
+			break;
+		case TraceEnabledWOLastIP:
+			self->state = TraceEnabledWithLastIP;
+			self->last_ip = addr;
+			break;
+	}
+}
+
+static inline void decoder_handle_pgd(decoder_state_machine_t *self, uint64_t addr, should_disasm_t *res){
+	res->valid= false;
+	switch(self->state){
+		case TraceDisabled:
+			//assert(false);
+			break;
+		case TraceEnabledWithLastIP:
+			_set_disasm(res, self->last_ip, addr);
+			self->state = TraceDisabled;
+			self->last_ip = 0;
+			break;
+		case TraceEnabledWOLastIP:
+			self->state = TraceDisabled;
+			break;
+	}
+}
+
+static inline void decoder_handle_pge(decoder_state_machine_t *self, uint64_t addr, should_disasm_t *res){
+	res->valid= false;
+	switch(self->state){
+		case TraceDisabled:
+			self->state = TraceEnabledWithLastIP;
+			self->last_ip = addr;
+			break;
+		case TraceEnabledWithLastIP:
+			//assert(false);
+			break;
+		case TraceEnabledWOLastIP:
+			self->state = TraceEnabledWithLastIP;
+			self->last_ip = addr;
+			break;
+	}
+}
+
+
+static inline void decoder_handle_fup(decoder_state_machine_t *self, uint64_t fup_src, should_disasm_t *res){
+	//assert(self->state);
+	res->valid= false;
+	switch(self->state){
+		case TraceDisabled:
+			self->state = TraceDisabled;
+			break;
+		case TraceEnabledWithLastIP:
+			_set_disasm(res, self->last_ip, fup_src);
+			//self->state = TraceEnabledWOLastIP;
+			//self->last_ip = 0;
+			self->last_ip = fup_src;
+		      break;
+		case TraceEnabledWOLastIP:
+			//assert(false);
+			break;
+	}
+}
+
+static inline uint64_t get_ip_val(uint8_t **pp, uint64_t *last_ip){
+	register uint8_t len = (*(*pp)++ >> PT_PKT_TIP_SHIFT);
+	if(unlikely(!len))
+		return 0;
+	uint64_t aligned_last_ip, aligned_pp;
+	memcpy(&aligned_pp, *pp, sizeof(uint64_t));
+	memcpy(&aligned_last_ip, last_ip, sizeof(uint64_t));
+	
+	aligned_last_ip = ((int64_t)((uint64_t)( 
+		((aligned_pp & (0xFFFFFFFFFFFFFFFF >> ((4-len)*16))) | (aligned_last_ip & (0xFFFFFFFFFFFFFFFF << ((len)*16))) ) 
+	)<< (64 - 48))) >> (64 - 48);
+	
+	memcpy(last_ip, &aligned_last_ip, sizeof(uint64_t));
+
+	*pp += (len*2);
+	return *last_ip;
+}
+
+static inline uint64_t get_val(uint8_t **pp, uint8_t len){
+	uint8_t*p = *pp;
+	uint64_t v = 0;
+	uint8_t i;
+	uint8_t shift = 0;
+
+	for (i = 0; i < len; i++, shift += 8)
+		v |= ((uint64_t)(*p++)) << shift;
+	*pp = p;
+	return v;
+}
+
+static inline void disasm(decoder_t* self){
+	static uint64_t failed_page = 0;
+	should_disasm_t* res = self->decoder_state_result;
+	if(res->valid){
+    	WRITE_SAMPLE_DECODED_DETAILED("\n\ndisasm(%lx,%lx)\tTNT: %ld\n", res->start, res->end, count_tnt(self->tnt_cache_state));
+			if(unlikely(trace_disassembler(self->disassembler_state, res->start, res->end, self->tnt_cache_state, &failed_page, self->mode) == disas_page_fault)){
+				self->page_fault_found = true;
+				self->page_fault_addr = failed_page;
+			}
+	}
+}
+
+
+static void tip_handler(decoder_t* self, uint8_t** p){
+	if(unlikely(self->fup_bind_pending)){
+		self->fup_bind_pending = false;
+		decoder_handle_fup(self->decoder_state, self->last_fup_src, self->decoder_state_result);
+    self->last_fup_src = 0;
+		disasm(self);
+    disassembler_flush(self->disassembler_state);
+		if(unlikely(self->page_fault_found)){
+			return;
+		}
+	}
+
+	get_ip_val(p, &self->last_tip);
+
+	WRITE_SAMPLE_DECODED_DETAILED("TIP    \t%lx (TNT: %d)\n", self->last_tip, count_tnt(self->tnt_cache_state));
+	decoder_handle_tip(self->decoder_state, self->last_tip, self->decoder_state_result);
+	disasm(self);
+#ifdef DECODER_LOG
+	self->log.tip++;
+#endif
+}
+
+static void tip_pge_handler(decoder_t* self, uint8_t** p){
+	self->ovp_state = false;
+
+	if(unlikely(self->fup_bind_pending)){
+		self->fup_bind_pending = false;
+		decoder_handle_fup(self->decoder_state, self->last_fup_src, self->decoder_state_result);
+    	self->last_fup_src = 0;
+		disasm(self);
+    disassembler_flush(self->disassembler_state);
+		if(unlikely(self->page_fault_found)){
+			return;
+		}
+	}
+
+	get_ip_val(p, &self->last_tip);
+
+	WRITE_SAMPLE_DECODED_DETAILED("PGE    \t%lx (TNT: %d)\n", self->last_tip, count_tnt(self->tnt_cache_state));
+	decoder_handle_pge(self->decoder_state, self->last_tip, self->decoder_state_result);
+ 	 assert(!self->decoder_state_result->valid);
+
+ 	disassembler_flush(self->disassembler_state);
+#ifndef STANDALONE_DECODER
+	if(unlikely(self->disassembler_state->redqueen_state->trace_mode)){
+		redqueen_trace_enabled(self->disassembler_state->redqueen_state, self->last_tip);
+	}
+#endif
+#ifdef DECODER_LOG
+	self->log.tip_pge++;
+#endif
+}
+
+static void tip_pgd_handler(decoder_t* self, uint8_t** p){
+	if(unlikely(self->fup_bind_pending)){
+		self->fup_bind_pending = false;
+		decoder_handle_fup(self->decoder_state, self->last_fup_src, self->decoder_state_result);
+    	self->last_fup_src = 0;
+		disasm(self);
+    disassembler_flush(self->disassembler_state);
+		if(unlikely(self->page_fault_found)){
+			return;
+		}
+	}
+
+	get_ip_val(p, &self->last_tip);
+	WRITE_SAMPLE_DECODED_DETAILED("PGD    \t%lx (TNT: %d)\n", self->last_tip, count_tnt(self->tnt_cache_state));
+	decoder_handle_pgd(self->decoder_state, self->last_tip, self->decoder_state_result);
+	disasm(self);
+
+	disassembler_flush(self->disassembler_state);
+#ifndef STANDALONE_DECODER
+	if(unlikely(self->disassembler_state->redqueen_state->trace_mode)){
+  	redqueen_trace_disabled(self->disassembler_state->redqueen_state, self->last_tip);
+  }
+#endif
+#ifdef DECODER_LOG
+	self->log.tip_pgd++;
+#endif
+}
+
+static void tip_fup_handler(decoder_t* self, uint8_t** p){
+	if(self->ovp_state){
+		self->decoder_state->state = TraceEnabledWithLastIP;
+		self->decoder_state->last_ip = get_ip_val(p, &self->last_tip);
+	
+		WRITE_SAMPLE_DECODED_DETAILED("FUP OVP   \t%lx (TNT: %d)\n", self->last_tip, count_tnt(self->tnt_cache_state));
+
+		self->ovp_state = false;
+		self->fup_bind_pending = false;
+
+		return;
+	}
+		
+	self->last_fup_src = get_ip_val(p, &self->last_tip);
+	WRITE_SAMPLE_DECODED_DETAILED("FUP    \t%lx (TNT: %d)\n", self->last_fup_src, count_tnt(self->tnt_cache_state));
+
+	self->fup_bind_pending = true;
+#ifdef DECODER_LOG
+	self->log.tip_fup++;
+#endif
+}
+
+static inline void pip_handler(decoder_t* self, uint8_t** p){
+#ifdef SAMPLE_DECODED_DETAILED
+	(*p) += PT_PKT_PIP_LEN-6;
+	WRITE_SAMPLE_DECODED_DETAILED("PIP\t%llx\n", (get_val(p, 6) >> 1) << 5);
+#else
+	//get_val(p, 6);
+	(*p) += PT_PKT_PIP_LEN;
+#endif
+#ifdef DECODER_LOG
+	self->log.pip++;
+#endif
+}
+
+__attribute__((hot)) decoder_result_t decode_buffer(decoder_t* self, uint8_t* map, size_t len){
+	static void* dispatch_table_level_1[] = {
+		&&handle_pt_pad,		// 00000000
+		&&handle_pt_tip_pgd,	// 00000001
+		&&handle_pt_level_2,	// 00000010
+		&&handle_pt_cyc,		// 00000011
+		&&handle_pt_tnt8,		// 00000100
+		&&handle_pt_error,		// 00000101
+		&&handle_pt_tnt8,		// 00000110
+		&&handle_pt_cyc,		// 00000111
+		&&handle_pt_tnt8,		// 00001000
+		&&handle_pt_error,		// 00001001
+		&&handle_pt_tnt8,		// 00001010
+		&&handle_pt_cyc,		// 00001011
+		&&handle_pt_tnt8,		// 00001100
+		&&handle_pt_tip,		// 00001101
+		&&handle_pt_tnt8,		// 00001110
+		&&handle_pt_cyc,		// 00001111
+		&&handle_pt_tnt8,		// 00010000
+		&&handle_pt_tip_pge,	// 00010001
+		&&handle_pt_tnt8,		// 00010010
+		&&handle_pt_cyc,		// 00010011
+		&&handle_pt_tnt8,		// 00010100
+		&&handle_pt_error,		// 00010101
+		&&handle_pt_tnt8,		// 00010110
+		&&handle_pt_cyc,		// 00010111
+		&&handle_pt_tnt8,		// 00011000
+		&&handle_pt_tsc,		// 00011001
+		&&handle_pt_tnt8,		// 00011010
+		&&handle_pt_cyc,		// 00011011
+		&&handle_pt_tnt8,		// 00011100
+		&&handle_pt_tip_fup,	// 00011101
+		&&handle_pt_tnt8,		// 00011110
+		&&handle_pt_cyc,		// 00011111
+		&&handle_pt_tnt8,		// 00100000
+		&&handle_pt_tip_pgd,	// 00100001
+		&&handle_pt_tnt8,		// 00100010
+		&&handle_pt_cyc,		// 00100011
+		&&handle_pt_tnt8,		// 00100100
+		&&handle_pt_error,		// 00100101
+		&&handle_pt_tnt8,		// 00100110
+		&&handle_pt_cyc,		// 00100111
+		&&handle_pt_tnt8,		// 00101000
+		&&handle_pt_error,		// 00101001
+		&&handle_pt_tnt8,		// 00101010
+		&&handle_pt_cyc,		// 00101011
+		&&handle_pt_tnt8,		// 00101100
+		&&handle_pt_tip,		// 00101101
+		&&handle_pt_tnt8,		// 00101110
+		&&handle_pt_cyc,		// 00101111
+		&&handle_pt_tnt8,		// 00110000
+		&&handle_pt_tip_pge,	// 00110001
+		&&handle_pt_tnt8,		// 00110010
+		&&handle_pt_cyc,		// 00110011
+		&&handle_pt_tnt8,		// 00110100
+		&&handle_pt_error,		// 00110101
+		&&handle_pt_tnt8,		// 00110110
+		&&handle_pt_cyc,		// 00110111
+		&&handle_pt_tnt8,		// 00111000
+		&&handle_pt_error,		// 00111001
+		&&handle_pt_tnt8,		// 00111010
+		&&handle_pt_cyc,		// 00111011
+		&&handle_pt_tnt8,		// 00111100
+		&&handle_pt_tip_fup,	// 00111101
+		&&handle_pt_tnt8,		// 00111110
+		&&handle_pt_cyc,		// 00111111
+		&&handle_pt_tnt8,		// 01000000
+		&&handle_pt_tip_pgd,	// 01000001
+		&&handle_pt_tnt8,		// 01000010
+		&&handle_pt_cyc,		// 01000011
+		&&handle_pt_tnt8,		// 01000100
+		&&handle_pt_error,		// 01000101
+		&&handle_pt_tnt8,		// 01000110
+		&&handle_pt_cyc,		// 01000111
+		&&handle_pt_tnt8,		// 01001000
+		&&handle_pt_error,		// 01001001
+		&&handle_pt_tnt8,		// 01001010
+		&&handle_pt_cyc,		// 01001011
+		&&handle_pt_tnt8,		// 01001100
+		&&handle_pt_tip,		// 01001101
+		&&handle_pt_tnt8,		// 01001110
+		&&handle_pt_cyc,		// 01001111
+		&&handle_pt_tnt8,		// 01010000
+		&&handle_pt_tip_pge,	// 01010001
+		&&handle_pt_tnt8,		// 01010010
+		&&handle_pt_cyc,		// 01010011
+		&&handle_pt_tnt8,		// 01010100
+		&&handle_pt_exit,		// 01010101
+		&&handle_pt_tnt8,		// 01010110
+		&&handle_pt_cyc,		// 01010111
+		&&handle_pt_tnt8,		// 01011000
+		&&handle_pt_mtc,		// 01011001
+		&&handle_pt_tnt8,		// 01011010
+		&&handle_pt_cyc,		// 01011011
+		&&handle_pt_tnt8,		// 01011100
+		&&handle_pt_tip_fup,	// 01011101
+		&&handle_pt_tnt8,		// 01011110
+		&&handle_pt_cyc,		// 01011111
+		&&handle_pt_tnt8,		// 01100000
+		&&handle_pt_tip_pgd,	// 01100001
+		&&handle_pt_tnt8,		// 01100010
+		&&handle_pt_cyc,		// 01100011
+		&&handle_pt_tnt8,		// 01100100
+		&&handle_pt_error,		// 01100101
+		&&handle_pt_tnt8,		// 01100110
+		&&handle_pt_cyc,		// 01100111
+		&&handle_pt_tnt8,		// 01101000
+		&&handle_pt_error,		// 01101001
+		&&handle_pt_tnt8,		// 01101010
+		&&handle_pt_cyc,		// 01101011
+		&&handle_pt_tnt8,		// 01101100
+		&&handle_pt_tip,		// 01101101
+		&&handle_pt_tnt8,		// 01101110
+		&&handle_pt_cyc,		// 01101111
+		&&handle_pt_tnt8,		// 01110000
+		&&handle_pt_tip_pge,	// 01110001
+		&&handle_pt_tnt8,		// 01110010
+		&&handle_pt_cyc,		// 01110011
+		&&handle_pt_tnt8,		// 01110100
+		&&handle_pt_error,		// 01110101
+		&&handle_pt_tnt8,		// 01110110
+		&&handle_pt_cyc,		// 01110111
+		&&handle_pt_tnt8,		// 01111000
+		&&handle_pt_error,		// 01111001
+		&&handle_pt_tnt8,		// 01111010
+		&&handle_pt_cyc,		// 01111011
+		&&handle_pt_tnt8,		// 01111100
+		&&handle_pt_tip_fup,	// 01111101
+		&&handle_pt_tnt8,		// 01111110
+		&&handle_pt_cyc,		// 01111111
+		&&handle_pt_tnt8,		// 10000000
+		&&handle_pt_tip_pgd,	// 10000001
+		&&handle_pt_tnt8,		// 10000010
+		&&handle_pt_cyc,		// 10000011
+		&&handle_pt_tnt8,		// 10000100
+		&&handle_pt_error,		// 10000101
+		&&handle_pt_tnt8,		// 10000110
+		&&handle_pt_cyc,		// 10000111
+		&&handle_pt_tnt8,		// 10001000
+		&&handle_pt_error,		// 10001001
+		&&handle_pt_tnt8,		// 10001010
+		&&handle_pt_cyc,		// 10001011
+		&&handle_pt_tnt8,		// 10001100
+		&&handle_pt_tip,		// 10001101
+		&&handle_pt_tnt8,		// 10001110
+		&&handle_pt_cyc,		// 10001111
+		&&handle_pt_tnt8,		// 10010000
+		&&handle_pt_tip_pge,	// 10010001
+		&&handle_pt_tnt8,		// 10010010
+		&&handle_pt_cyc,		// 10010011
+		&&handle_pt_tnt8,		// 10010100
+		&&handle_pt_error,		// 10010101
+		&&handle_pt_tnt8,		// 10010110
+		&&handle_pt_cyc,		// 10010111
+		&&handle_pt_tnt8,		// 10011000
+		&&handle_pt_mode,		// 10011001
+		&&handle_pt_tnt8,		// 10011010
+		&&handle_pt_cyc,		// 10011011
+		&&handle_pt_tnt8,		// 10011100
+		&&handle_pt_tip_fup,	// 10011101
+		&&handle_pt_tnt8,		// 10011110
+		&&handle_pt_cyc,		// 10011111
+		&&handle_pt_tnt8,		// 10100000
+		&&handle_pt_tip_pgd,	// 10100001
+		&&handle_pt_tnt8,		// 10100010
+		&&handle_pt_cyc,		// 10100011
+		&&handle_pt_tnt8,		// 10100100
+		&&handle_pt_error,		// 10100101
+		&&handle_pt_tnt8,		// 10100110
+		&&handle_pt_cyc,		// 10100111
+		&&handle_pt_tnt8,		// 10101000
+		&&handle_pt_error,		// 10101001
+		&&handle_pt_tnt8,		// 10101010
+		&&handle_pt_cyc,		// 10101011
+		&&handle_pt_tnt8,		// 10101100
+		&&handle_pt_tip,		// 10101101
+		&&handle_pt_tnt8,		// 10101110
+		&&handle_pt_cyc,		// 10101111
+		&&handle_pt_tnt8,		// 10110000
+		&&handle_pt_tip_pge,	// 10110001
+		&&handle_pt_tnt8,		// 10110010
+		&&handle_pt_cyc,		// 10110011
+		&&handle_pt_tnt8,		// 10110100
+		&&handle_pt_error,		// 10110101
+		&&handle_pt_tnt8,		// 10110110
+		&&handle_pt_cyc,		// 10110111
+		&&handle_pt_tnt8,		// 10111000
+		&&handle_pt_error,		// 10111001
+		&&handle_pt_tnt8,		// 10111010
+		&&handle_pt_cyc,		// 10111011
+		&&handle_pt_tnt8,		// 10111100
+		&&handle_pt_tip_fup,	// 10111101
+		&&handle_pt_tnt8,		// 10111110
+		&&handle_pt_cyc,		// 10111111
+		&&handle_pt_tnt8,		// 11000000
+		&&handle_pt_tip_pgd,	// 11000001
+		&&handle_pt_tnt8,		// 11000010
+		&&handle_pt_cyc,		// 11000011
+		&&handle_pt_tnt8,		// 11000100
+		&&handle_pt_error,		// 11000101
+		&&handle_pt_tnt8,		// 11000110
+		&&handle_pt_cyc,		// 11000111
+		&&handle_pt_tnt8,		// 11001000
+		&&handle_pt_error,		// 11001001
+		&&handle_pt_tnt8,		// 11001010
+		&&handle_pt_cyc,		// 11001011
+		&&handle_pt_tnt8,		// 11001100
+		&&handle_pt_tip,		// 11001101
+		&&handle_pt_tnt8,		// 11001110
+		&&handle_pt_cyc,		// 11001111
+		&&handle_pt_tnt8,		// 11010000
+		&&handle_pt_tip_pge,	// 11010001
+		&&handle_pt_tnt8,		// 11010010
+		&&handle_pt_cyc,		// 11010011
+		&&handle_pt_tnt8,		// 11010100
+		&&handle_pt_error,		// 11010101
+		&&handle_pt_tnt8,		// 11010110
+		&&handle_pt_cyc,		// 11010111
+		&&handle_pt_tnt8,		// 11011000
+		&&handle_pt_error,		// 11011001
+		&&handle_pt_tnt8,		// 11011010
+		&&handle_pt_cyc,		// 11011011
+		&&handle_pt_tnt8,		// 11011100
+		&&handle_pt_tip_fup,	// 11011101
+		&&handle_pt_tnt8,		// 11011110
+		&&handle_pt_cyc,		// 11011111
+		&&handle_pt_tnt8,		// 11100000
+		&&handle_pt_tip_pgd,	// 11100001
+		&&handle_pt_tnt8,		// 11100010
+		&&handle_pt_cyc,		// 11100011
+		&&handle_pt_tnt8,		// 11100100
+		&&handle_pt_error,		// 11100101
+		&&handle_pt_tnt8,		// 11100110
+		&&handle_pt_cyc,		// 11100111
+		&&handle_pt_tnt8,		// 11101000
+		&&handle_pt_error,		// 11101001
+		&&handle_pt_tnt8,		// 11101010
+		&&handle_pt_cyc,		// 11101011
+		&&handle_pt_tnt8,		// 11101100
+		&&handle_pt_tip,		// 11101101
+		&&handle_pt_tnt8,		// 11101110
+		&&handle_pt_cyc,		// 11101111
+		&&handle_pt_tnt8,		// 11110000
+		&&handle_pt_tip_pge,	// 11110001
+		&&handle_pt_tnt8,		// 11110010
+		&&handle_pt_cyc,		// 11110011
+		&&handle_pt_tnt8,		// 11110100
+		&&handle_pt_error,		// 11110101
+		&&handle_pt_tnt8,		// 11110110
+		&&handle_pt_cyc,		// 11110111
+		&&handle_pt_tnt8,		// 11111000
+		&&handle_pt_error,		// 11111001
+		&&handle_pt_tnt8,		// 11111010
+		&&handle_pt_cyc,		// 11111011
+		&&handle_pt_tnt8,		// 11111100
+		&&handle_pt_tip_fup,	// 11111101
+		&&handle_pt_tnt8,		// 11111110
+		&&handle_pt_error,		// 11111111
+	};
+
+	#define DISPATCH_L1() /*printf("-> %p -> %x\n", p, p[0]);*/ goto *dispatch_table_level_1[p[0]]
+
+	bool pt_overflowed = false;
+	self->page_fault_found = false;
+
+	uint8_t *end = map + len;
+	uint8_t *p = map;
+
+#ifdef DECODER_LOG
+	flush_log(self);
+#endif
+
+#ifdef STANDALONE_DECODER
+	map[len] = PT_TRACE_END;
+#endif
+
+	p = memmem(p, end - p, psb, PT_PKT_PSB_LEN);
+	if (!p) {
+		p = end;
+		goto handle_pt_exit;
+	}
+	
+	DISPATCH_L1();
+	handle_pt_mode:
+
+		
+		switch (p[1] >> 5) {
+			case 0:
+				switch (p[1] & 3) {
+					case 0:
+						self->mode = mode_16;
+						break;
+					case 1:
+						self->mode = mode_64;
+						break;
+					case 2:
+						self->mode = mode_32;
+						break;
+				}
+			default:
+				break;
+		}
+		
+		p += PT_PKT_MODE_LEN;
+		WRITE_SAMPLE_DECODED_DETAILED("MODE\n");
+		#ifdef DECODER_LOG
+		self->log.mode++;
+		#endif
+		DISPATCH_L1();
+	handle_pt_tip:
+		tip_handler(self, &p);
+		if(unlikely(self->page_fault_found)){
+			pt_decoder_flush(self);
+			return decoder_page_fault;
+		}
+		DISPATCH_L1();
+	handle_pt_tip_pge:
+		tip_pge_handler(self, &p);
+		if(unlikely(self->page_fault_found)){
+			pt_decoder_flush(self);
+			return decoder_page_fault;
+		}
+		DISPATCH_L1();
+	handle_pt_tip_pgd:
+		tip_pgd_handler(self, &p);
+		if(unlikely(self->page_fault_found)){
+			pt_decoder_flush(self);
+			return decoder_page_fault;
+		}
+		DISPATCH_L1();
+	handle_pt_tip_fup:
+		tip_fup_handler(self, &p);
+		DISPATCH_L1();
+	handle_pt_pad:
+		while(unlikely(!(*(++p)))){}
+		//p++;
+		#ifdef DECODER_LOG
+		self->log.pad++;
+		#endif
+		DISPATCH_L1();
+	handle_pt_level_2:
+		switch(p[1]){
+			case 0b00000011:	/* CBR */
+				p += PT_PKT_CBR_LEN;
+				#ifdef DECODER_LOG
+				self->log.cbr++;
+				#endif
+				DISPATCH_L1();
+				
+			case 0b00100011:	/* PSBEND */
+				p += PT_PKT_PSBEND_LEN;
+				WRITE_SAMPLE_DECODED_DETAILED("PSBEND\n");
+				#ifdef DECODER_LOG
+				self->log.psbend++;
+				#endif
+				DISPATCH_L1();
+
+			case 0b01000011:	/* PIP */
+				pip_handler(self, &p);
+				DISPATCH_L1();
+
+			case 0b10000010:	/* PSB */
+				p += PT_PKT_PSB_LEN;
+				WRITE_SAMPLE_DECODED_DETAILED("PSB\n");
+				#ifdef DECODER_LOG
+				self->log.psbc++;
+				#endif
+				DISPATCH_L1();
+
+			case 0b10000011:	/* TS  */
+				abort();
+				fprintf(stderr, "\n\n===========> TS\n" );
+				p += PT_PKT_TS_LEN;
+
+				return decoder_error;
+				DISPATCH_L1();
+
+			case 0b10100011:	/* LTNT */
+				WRITE_SAMPLE_DECODED_DETAILED("LTNT\n");
+				append_tnt_cache_ltnt(self->tnt_cache_state, (uint64_t)*p);
+				p += PT_PKT_LTNT_LEN;
+				#ifdef DECODER_LOG
+				self->log.tnt64++;
+				#endif
+				DISPATCH_L1();
+
+			case 0b11001000:	/* VMCS */
+				WRITE_SAMPLE_DECODED_DETAILED("VMCS\n");
+				p += PT_PKT_VMCS_LEN;
+				#ifdef DECODER_LOG
+				self->log.vmcs++;
+				#endif
+				DISPATCH_L1();
+
+			case 0b11110011:	/* OVF */
+				//fprintf(stderr, "OVERFLOW\n");
+				WRITE_SAMPLE_DECODED_DETAILED("OVERFLOW\n");
+				p += PT_PKT_OVF_LEN;
+				self->ovp_state = true;
+				self->last_tip = 0;
+				decoder_statemachine_reset(self->decoder_state);
+				pt_overflowed = true;
+
+				DISPATCH_L1();
+
+			case 0b11000011:	/* MNT */
+			case 0b01110011:	/* TMA */
+			default:
+				printf("unkown packet (level2): %x\n", p[1]);
+				abort();
+
+		}
+	handle_pt_tnt8:
+		WRITE_SAMPLE_DECODED_DETAILED("TNT %x\n", *p);
+		append_tnt_cache(self->tnt_cache_state, (uint64_t)(*p));
+		p++;
+		#ifdef DECODER_LOG
+		self->log.tnt8++;
+		#endif
+		DISPATCH_L1();
+	handle_pt_mtc:
+	handle_pt_tsc:
+	handle_pt_error:
+	handle_pt_cyc:
+		printf("unkown packet: %x\n", p[0]);
+		abort();
+
+	handle_pt_exit:
+
+	if (count_tnt(self->tnt_cache_state) != 0){
+			fprintf(stderr, "\nERR: \tTNT %d\n", count_tnt(self->tnt_cache_state));//, self->pge_enabled);
+
+		//return true;
+		return decoder_error;
+#ifndef STANDALONE_DECODER
+		fprintf(stderr, "\nERR: \tTNT %d\n", count_tnt(self->tnt_cache_state));//, self->pge_enabled);
+			char* tmp;
+		assert(asprintf(&tmp, "/tmp/failed_trace_%d_%d", getpid(), error_counter++) != -1);
+		FILE* f = fopen(tmp, "wb");
+							fwrite(map, len, 1, f);
+							fclose(f);
+		free(tmp);
+
+
+		return decoder_error;
+		
+#else
+		printf("ERROR DETECTED...FLUSHING DISASSEMBLER %d!\n", count_tnt(self->tnt_cache_state));
+#endif
+		return decoder_error;
+	}
+
+	if(self->disassembler_state->infinite_loop_found){
+
+		char* tmp;
+		assert(asprintf(&tmp, "/tmp/loop_trace_%d_%d", getpid(), error_counter++) != -1);
+		FILE* f = fopen(tmp, "wb");
+							fwrite(map, len, 1, f);
+							fclose(f);
+		free(tmp);
+
+		self->disassembler_state->infinite_loop_found = false;
+	}
+		
+	pt_decoder_flush(self);
+
+#ifdef STANDALONE_DECODER
+
+#ifndef PERFORMANCE
+	//printf("HASH: 0x%lx\n", fuzz_bitmap_get_hash());
+	//printf("\tFIN: TNT %d \n", count_tnt(self->tnt_cache_state));
+	/*
+	if(count_tnt(self->tnt_cache_state))
+		WRITE_SAMPLE_DECODED_DETAILED("\tFIN: TNT %d \n", count_tnt(self->tnt_cache_state));
+	else{
+		WRITE_SAMPLE_DECODED_DETAILED("\tFIN: TNT %d \n", count_tnt(self->tnt_cache_state));
+	}
+	*/
+#endif
+#endif
+
+	if(pt_overflowed){
+		return decoder_success_pt_overflow;
+	}
+	return decoder_success;
+}
\ No newline at end of file
diff --new-file -ur qemu/pt/decoder.h QEMU-PT/pt/decoder.h
--- qemu/pt/decoder.h	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/decoder.h	2021-08-24 21:54:55.938586449 +0200
@@ -0,0 +1,132 @@
+/*
+
+Copyright (C) 2017 Sergej Schumilo
+
+This file is part of QEMU-PT (kAFL).
+
+QEMU-PT is free software: you can redistribute it and/or modify
+it under the terms of the GNU General Public License as published by
+the Free Software Foundation, either version 2 of the License, or
+(at your option) any later version.
+
+QEMU-PT is distributed in the hope that it will be useful,
+but WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with QEMU-PT.  If not, see <http://www.gnu.org/licenses/>.
+
+*/
+
+#ifndef DECODER_H
+#define DECODER_H
+
+#include <sys/mman.h>
+#include <sys/fcntl.h>
+#include <sys/stat.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+#include <stdint.h>
+#include <unistd.h>
+#include <stddef.h>
+#include <sys/time.h>
+#include <stdbool.h>
+#ifdef STANDALONE_DECODER
+#include "tnt_cache.h"
+#include "disassembler.h"
+#include "logger.h"
+#include "page_cache.h"
+#else
+#include "pt/tnt_cache.h"
+#include "pt/disassembler.h"
+#include "pt/logger.h"
+#include "pt/redqueen.h"
+#include "pt/page_cache.h"
+#endif
+
+
+//#define DECODER_LOG
+
+typedef enum decoder_state { 
+	TraceDisabled=1, 
+	TraceEnabledWithLastIP, 
+	TraceEnabledWOLastIP} 
+decoder_state_e;
+
+typedef struct DecoderStateMachine{
+  decoder_state_e state;
+  uint64_t last_ip;
+} decoder_state_machine_t;
+
+/*
+Used as return type for statemachine updates, start and end are undefined unless valid is true
+*/
+typedef struct ShouldDisasm{
+  uint64_t start;
+  uint64_t end;
+  bool valid;
+} should_disasm_t;
+
+
+typedef struct decoder_s{
+	bool page_fault_found;
+	uint64_t page_fault_addr;
+	bool ovp_state; 
+	uint64_t last_tip;
+	uint64_t last_tip_tmp;
+	uint64_t last_fup_src;
+	bool fup_bind_pending;
+	disassembler_t* disassembler_state;
+	tnt_cache_t* tnt_cache_state;
+	decoder_state_machine_t* decoder_state;
+	should_disasm_t* decoder_state_result;
+	disassembler_mode_t mode;
+	//uint64_t filter[4][2];
+
+#ifdef DECODER_LOG
+	struct decoder_log_s{
+		uint64_t tnt64;
+		uint64_t tnt8;
+		uint64_t pip;
+		uint64_t cbr;
+		uint64_t ts;
+		uint64_t ovf;
+		uint64_t psbc;
+		uint64_t psbend;
+		uint64_t mnt;
+		uint64_t tma;
+		uint64_t vmcs;
+		uint64_t pad;
+		uint64_t tip;
+		uint64_t tip_pge;
+		uint64_t tip_pgd;
+		uint64_t tip_fup;
+		uint64_t mode;
+	} log;
+#endif
+} decoder_t;
+
+
+#ifndef STANDALONE_DECODER
+decoder_t* pt_decoder_init(uint64_t filter[4][2], int disassembler_word_width, redqueen_t *redqueen_state, page_cache_t* page_cache);
+#else
+decoder_t* pt_decoder_init(uint64_t filter[4][2], int disassembler_word_width, page_cache_t* page_cache);
+#endif
+
+typedef enum decoder_result_s { 
+	decoder_success, 
+	decoder_success_pt_overflow,
+	decoder_page_fault, 
+	decoder_error,
+} decoder_result_t;
+
+/* returns false if the CPU trashed our tracing run ... thank you Intel btw ... */
+ __attribute__((hot)) decoder_result_t decode_buffer(decoder_t* self, uint8_t* map, size_t len);
+void pt_decoder_destroy(decoder_t* self);
+void pt_decoder_flush(decoder_t* self);
+
+uint64_t pt_decoder_get_page_fault_addr(decoder_t* self);
+void pt_decoder_reset(decoder_t* self);
+#endif
diff --new-file -ur qemu/pt/disassembler.c QEMU-PT/pt/disassembler.c
--- qemu/pt/disassembler.c	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/disassembler.c	2021-08-24 21:54:55.938586449 +0200
@@ -0,0 +1,993 @@
+/*
+
+Copyright (C) 2017 Sergej Schumilo
+
+This file is part of QEMU-PT (kAFL).
+
+QEMU-PT is free software: you can redistribute it and/or modify
+it under the terms of the GNU General Public License as published by
+the Free Software Foundation, either version 2 of the License, or
+(at your option) any later version.
+
+QEMU-PT is distributed in the hope that it will be useful,
+but WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with QEMU-PT.  If not, see <http://www.gnu.org/licenses/>.
+
+*/
+
+#include "debug.h"
+#ifdef STANDALONE_DECODER
+#include "disassembler.h"
+#include "logger.h"
+#include <assert.h>
+#else
+#include "pt/disassembler.h"
+#include "pt/redqueen.h"
+#endif
+
+
+#if CS_API_MAJOR < 4
+#error Unsupported capstone version (capstone engine v4 is required)!
+#endif
+
+//#define WRITE_SAMPLE_DECODED_DETAILED(format, ...) (sample_decoded_detailed(format, ##__VA_ARGS__))
+
+
+#ifdef STANDALONE_DECODER
+#define likely(x)      __builtin_expect(!!(x), 1) 
+#define unlikely(x)    __builtin_expect(!!(x), 0) 
+#endif
+
+#define LOOKUP_TABLES		5
+#define IGN_MOD_RM			0
+#define IGN_OPODE_PREFIX	0
+#define MODRM_REG(x)		(x << 3)
+#define MODRM_AND			0b00111000
+
+#define limit_check(a, b, c, d) ((!(((c) >= (a)) && ((c) <= (b)))) || (c == d))
+#define out_of_bounds(self, addr) ((addr < self->min_addr) || (addr > self->max_addr))
+
+#define in_range(self, addr) (((addr > self->min_addr_0) && (addr < self->max_addr_0)) ||  ((addr > self->min_addr_1) && (addr < self->max_addr_1)) || ((addr > self->min_addr_2) && (addr < self->max_addr_2)) || ((addr > self->min_addr_3) && (addr < self->max_addr_3)))
+#define in_range_specific(addr, min, max) ((addr >= min) && (addr < max))
+
+/* conditional branch */
+cofi_ins cb_lookup[] = {
+	{X86_INS_JAE,			IGN_MOD_RM,	IGN_OPODE_PREFIX},
+	{X86_INS_JA,			IGN_MOD_RM,	IGN_OPODE_PREFIX},
+	{X86_INS_JBE,			IGN_MOD_RM,	IGN_OPODE_PREFIX},
+	{X86_INS_JB,			IGN_MOD_RM,	IGN_OPODE_PREFIX},
+	{X86_INS_JCXZ,		IGN_MOD_RM,	IGN_OPODE_PREFIX},
+	{X86_INS_JECXZ,		IGN_MOD_RM,	IGN_OPODE_PREFIX},
+	{X86_INS_JE,			IGN_MOD_RM,	IGN_OPODE_PREFIX},
+	{X86_INS_JGE,			IGN_MOD_RM,	IGN_OPODE_PREFIX},
+  {X86_INS_JG,			IGN_MOD_RM,	IGN_OPODE_PREFIX},
+  {X86_INS_JLE,			IGN_MOD_RM,	IGN_OPODE_PREFIX},
+	{X86_INS_JL,			IGN_MOD_RM,	IGN_OPODE_PREFIX},
+  {X86_INS_JNE,			IGN_MOD_RM,	IGN_OPODE_PREFIX},
+  {X86_INS_JNO,			IGN_MOD_RM,	IGN_OPODE_PREFIX},
+  {X86_INS_JNP,			IGN_MOD_RM,	IGN_OPODE_PREFIX},
+  {X86_INS_JNS,			IGN_MOD_RM,	IGN_OPODE_PREFIX},
+  {X86_INS_JO,			IGN_MOD_RM,	IGN_OPODE_PREFIX},
+  {X86_INS_JP,			IGN_MOD_RM,	IGN_OPODE_PREFIX},
+  {X86_INS_JRCXZ,		IGN_MOD_RM,	IGN_OPODE_PREFIX},
+  {X86_INS_JS,			IGN_MOD_RM,	IGN_OPODE_PREFIX},
+	{X86_INS_LOOP,		IGN_MOD_RM,	IGN_OPODE_PREFIX},
+	{X86_INS_LOOPE,		IGN_MOD_RM,	IGN_OPODE_PREFIX},
+	{X86_INS_LOOPNE,	IGN_MOD_RM,	IGN_OPODE_PREFIX},
+};
+
+/* unconditional direct branch */
+cofi_ins udb_lookup[] = {
+	{X86_INS_JMP,		IGN_MOD_RM,	0xe9},
+	{X86_INS_JMP,		IGN_MOD_RM, 0xeb},
+	{X86_INS_CALL,	IGN_MOD_RM,	0xe8},	
+};
+
+/* indirect branch */
+cofi_ins ib_lookup[] = {
+	{X86_INS_JMP,		MODRM_REG(4),	0xff},
+	{X86_INS_CALL,	MODRM_REG(2),	0xff},	
+};
+
+/* near ret */
+cofi_ins nr_lookup[] = {
+	{X86_INS_RET,		IGN_MOD_RM,	0xc3},
+	{X86_INS_RET,		IGN_MOD_RM,	0xc2},
+};
+ 
+/* far transfers */ 
+cofi_ins ft_lookup[] = {
+	{X86_INS_INT3,			IGN_MOD_RM,	IGN_OPODE_PREFIX},
+	{X86_INS_INT,				IGN_MOD_RM,	IGN_OPODE_PREFIX},
+	{X86_INS_INT1,			IGN_MOD_RM,	IGN_OPODE_PREFIX},
+	{X86_INS_INTO,			IGN_MOD_RM,	IGN_OPODE_PREFIX},
+	{X86_INS_IRET,			IGN_MOD_RM,	IGN_OPODE_PREFIX},
+	{X86_INS_IRETD,			IGN_MOD_RM,	IGN_OPODE_PREFIX},
+	{X86_INS_IRETQ,			IGN_MOD_RM,	IGN_OPODE_PREFIX},
+	{X86_INS_JMP,				IGN_MOD_RM,		0xea},
+	{X86_INS_JMP,				MODRM_REG(5),	0xff},
+	{X86_INS_CALL,			IGN_MOD_RM,		0x9a},
+	{X86_INS_CALL,			MODRM_REG(3),	0xff},
+	{X86_INS_RET,				IGN_MOD_RM,		0xcb},
+	{X86_INS_RET,				IGN_MOD_RM,		0xca},
+	{X86_INS_SYSCALL,		IGN_MOD_RM,	IGN_OPODE_PREFIX},
+	{X86_INS_SYSENTER,	IGN_MOD_RM,	IGN_OPODE_PREFIX},
+	{X86_INS_SYSEXIT,		IGN_MOD_RM,	IGN_OPODE_PREFIX},
+	{X86_INS_SYSRET,		IGN_MOD_RM,	IGN_OPODE_PREFIX},
+	{X86_INS_VMLAUNCH,	IGN_MOD_RM,	IGN_OPODE_PREFIX},
+	{X86_INS_VMRESUME,	IGN_MOD_RM,	IGN_OPODE_PREFIX},
+	{X86_INS_UD0, 			IGN_MOD_RM, IGN_OPODE_PREFIX},
+	{X86_INS_UD2, 			IGN_MOD_RM, IGN_OPODE_PREFIX},
+	{X86_INS_UD2B, 			IGN_MOD_RM, IGN_OPODE_PREFIX},
+
+};
+
+uint16_t cmp_lookup[] = {
+	X86_INS_CMP,
+	X86_INS_CMPPD,
+	X86_INS_CMPPS,
+	X86_INS_CMPSB,
+	X86_INS_CMPSD,
+	X86_INS_CMPSQ,
+	X86_INS_CMPSS,
+	X86_INS_CMPSW,
+	X86_INS_CMPXCHG16B,
+	X86_INS_CMPXCHG,
+	X86_INS_CMPXCHG8B,
+};
+
+
+cofi_ins* lookup_tables[] = {
+	cb_lookup,
+	udb_lookup,
+	ib_lookup,
+	nr_lookup,
+	ft_lookup,
+};
+
+uint8_t lookup_table_sizes[] = {
+	22,
+	3,
+	2,
+	2,
+	19+3
+};
+
+/* ===== kAFL disassembler cofi list ===== */
+
+
+cofi_list pending_obj = {.cofi.type=DISASSEMBLY_PENDING};
+cofi_list oob_obj = {.cofi.type=OUT_OF_BOUNDS};
+cofi_list page_cache_failed_obj = {.cofi.type=PAGE_CACHE_FAILED};
+
+
+//cofi_header pending_obj = {.type=DISASSEMBLY_PENDING};
+
+static cofi_list* create_list_head(void){
+	cofi_list* head = malloc(sizeof(cofi_list));
+	if (head != NULL){
+		head->list_ptr = NULL;
+		head->cofi_ptr = &pending_obj;
+		head->cofi_target_ptr = &pending_obj;
+		//head->cofi = NULL;
+		head->cofi.type = DISASSEMBLY_PENDING;
+		return head;
+	}
+	return NULL;
+}
+
+static void free_list(cofi_list* head){
+	cofi_list *tmp1, *tmp2;
+	tmp1 = head;
+	while (1){
+		tmp2 = tmp1;
+		if(tmp1 == NULL){
+			break;
+		}
+		tmp1 = tmp1->list_ptr;
+		free(tmp2);
+	}
+}
+
+static cofi_list* new_list_element(cofi_list* predecessor){ //, cofi_header* cofi){
+	if(predecessor){
+		cofi_list* next = malloc(sizeof(cofi_list));
+		if (next){
+			predecessor->list_ptr = next;
+			next->list_ptr = NULL;
+			next->cofi_ptr = &pending_obj;
+			next->cofi_target_ptr = &pending_obj;
+			//next->cofi = cofi;
+			next->cofi.type = DISASSEMBLY_PENDING;
+			return next;
+		}
+	}
+	return NULL;
+}
+
+static void edit_cofi_ptr(cofi_list* element, cofi_list* target){
+	if (element){
+		element->cofi_ptr = target;
+	}
+}
+
+static void map_put(disassembler_t* self, uint64_t addr, uint64_t ref){
+	int ret;
+	khiter_t k;
+	k = kh_put(ADDR0, self->map, addr, &ret); 
+	kh_value(self->map, k) = ref;
+}
+
+static int map_exist(disassembler_t* self, uint64_t addr){
+	khiter_t k;
+	k = kh_get(ADDR0, self->map, addr); 
+	if(k != kh_end(self->map)){
+		return 1;
+	}
+	return 0;
+}
+
+static int map_get(disassembler_t* self, uint64_t addr, uint64_t* ref){
+	khiter_t k;
+	k = kh_get(ADDR0, self->map, addr); 
+	if(k != kh_end(self->map)){
+		*ref = kh_value(self->map, k); 
+		return 0;
+	} 
+	return 1;
+}
+
+/* ===== kAFL disassembler engine ===== */
+
+static inline uint64_t fast_strtoull(const char *hexstring){
+	uint64_t result = 0;
+	uint8_t i = 0;
+	if (hexstring[1] == 'x' || hexstring[1] == 'X')
+		i = 2;
+	for (; hexstring[i]; i++)
+		result = (result << 4) + (9 * (hexstring[i] >> 6) + (hexstring[i] & 017));
+	return result;
+}
+
+static inline uint64_t hex_to_bin(char* str){
+	//return (uint64_t)strtoull(str, NULL, 16);
+	return fast_strtoull(str);
+}
+
+#ifndef STANDALONE_DECODER
+static bool is_interessting_lea_at(disassembler_t* self, cs_insn *ins){
+  bool res = false;
+
+	assert(ins);
+	cs_x86 *x86 = &(ins->detail->x86);
+
+	assert(x86->op_count == 2);
+	cs_x86_op *op2 = &(x86->operands[1]);
+
+	assert(op2->type == X86_OP_MEM);
+		
+	x86_reg reg = op2->mem.index;
+	int64_t disp = (int64_t)op2->mem.disp;
+  res = disp < 0 && (-disp) > 0xff && op2->mem.scale == 1 && op2->mem.base == X86_REG_INVALID && reg != X86_REG_INVALID;
+
+	if(res){
+		x86_reg reg = op2->mem.index;
+		if(reg == X86_REG_EIP || reg == X86_REG_RIP || reg == X86_REG_EBP || reg == X86_REG_RBP){
+      QEMU_PT_PRINTF(REDQUEEN_PREFIX, "got boring index");
+      res = false;
+    } //don't instrument local stack offset computations
+  }
+  return res;
+}
+
+static bool uses_register(cs_x86_op* op, x86_reg reg){
+
+	if (op->type == X86_OP_REG && op->reg == reg){
+		return true;
+	}
+
+	if (op->type == X86_OP_MEM && op->mem.base == reg){
+		return true;
+	}
+
+	return false;
+}
+
+static bool uses_stack_access(cs_x86_op* op){
+	if (uses_register(op, X86_REG_RBP) || uses_register(op, X86_REG_EBP)){
+		return true;
+	} 
+
+	if (uses_register(op, X86_REG_RSP) || uses_register(op, X86_REG_ESP)){
+		return true;
+	} 
+
+	return false;
+}
+
+static bool is_interessting_add_at(disassembler_t* self, cs_insn *ins){
+	assert(ins);
+	cs_x86 *x86 = &(ins->detail->x86);
+
+	assert(x86->op_count == 2);
+	cs_x86_op *op1 = &(x86->operands[0]);
+	cs_x86_op *op2 = &(x86->operands[1]);
+
+	if(op2->type == X86_OP_IMM && (op1->type == X86_OP_REG || op1->type == X86_OP_MEM)){
+		//offsets needs to be negative, < -0xff to ensure we only look at multi byte substractions
+		if((op2->imm > 0x7fff && (((op2->imm>>8)&0xff) != 0xff))){
+			if (!uses_stack_access(op1)){
+				return true;
+			}
+		}
+	}
+	return false; 
+}
+
+
+static bool is_interessting_sub_at(disassembler_t* self, cs_insn *ins){
+	assert(ins);
+	cs_x86 *x86 = &(ins->detail->x86);
+
+	assert(x86->op_count == 2);
+	cs_x86_op *op1 = &(x86->operands[0]);
+	cs_x86_op *op2 = &(x86->operands[1]);
+
+	if(op2->type == X86_OP_IMM && (op1->type == X86_OP_REG || op1->type == X86_OP_MEM)){
+		if(op2->imm > 0xFF){
+			if (!uses_stack_access(op1)){
+				return true;
+			}
+		}
+	}
+	return false; 
+}
+
+static bool is_interessting_xor_at(disassembler_t* self, cs_insn *ins){
+	assert(ins);
+	cs_x86 *x86 = &(ins->detail->x86);
+
+	assert(x86->op_count == 2);
+	cs_x86_op *op1 = &(x86->operands[0]);
+	cs_x86_op *op2 = &(x86->operands[1]);
+
+	if(op1->type == X86_OP_REG && op2->type == X86_OP_REG){
+		if(op1->reg != op2->reg){
+			return true;
+		}
+	}
+	return false; 
+}
+
+#endif
+
+static cofi_type opcode_analyzer(disassembler_t* self, cs_insn *ins){
+	uint8_t i, j;
+	cs_x86 details = ins->detail->x86;
+	//printf("SELF %p\n", self->redqueen_state);
+	//printf("INS %lx\n", ins->address);
+#ifndef STANDALONE_DECODER
+	if(self->redqueen_mode){
+		  if(ins->id == X86_INS_CMP){
+			  set_rq_instruction(self->redqueen_state, ins->address);
+				//QEMU_PT_PRINTF(REDQUEEN_PREFIX, "hooking cmp %lx %s %s", ins->address, ins->mnemonic, ins->op_str);
+      }
+		  if(ins->id == X86_INS_LEA && is_interessting_lea_at(self, ins)){
+		    //QEMU_PT_PRINTF(REDQUEEN_PREFIX, "hooking lea %lx", ins->address);
+			  set_rq_instruction(self->redqueen_state, ins->address);
+      }
+		  if(ins->id == X86_INS_SUB && is_interessting_sub_at(self, ins)){
+		    //QEMU_PT_PRINTF(REDQUEEN_PREFIX, "hooking sub %lx", ins->address);
+			  set_rq_instruction(self->redqueen_state, ins->address);
+      }
+		  if(ins->id == X86_INS_ADD && is_interessting_add_at(self, ins)){
+		    //QEMU_PT_PRINTF(REDQUEEN_PREFIX, "hooking add %lx", ins->address);
+			  set_rq_instruction(self->redqueen_state, ins->address);
+      }
+		  if(ins->id == X86_INS_XOR && is_interessting_xor_at(self, ins)){
+		    //QEMU_PT_PRINTF(REDQUEEN_PREFIX, "hooking xor %lx %s %s", ins->address, ins->mnemonic, ins->op_str);
+			  set_rq_instruction(self->redqueen_state, ins->address);
+      }
+      if(ins->id ==X86_INS_CALL || ins->id == X86_INS_LCALL){
+				//QEMU_PT_PRINTF(REDQUEEN_PREFIX, "hooking call %lx %s %s", ins->address, ins->mnemonic, ins->op_str);
+		  	set_rq_instruction(self->redqueen_state, ins->address);
+      }
+	}
+#endif
+	
+	//		printf("%lx (%d)\t%s\t%s\t\t\n", ins->address, i, ins->mnemonic, ins->op_str);
+	for (i = 0; i < LOOKUP_TABLES; i++){
+		for (j = 0; j < lookup_table_sizes[i]; j++){
+			if (ins->id == lookup_tables[i][j].opcode){
+				
+				/* check MOD R/M */
+				if (lookup_tables[i][j].modrm != IGN_MOD_RM && lookup_tables[i][j].modrm != (details.modrm & MODRM_AND))
+						continue;	
+						
+				/* check opcode prefix byte */
+				if (lookup_tables[i][j].opcode_prefix != IGN_OPODE_PREFIX && lookup_tables[i][j].opcode_prefix != details.opcode[0])
+						continue;
+#ifdef DEBUG
+				/* found */
+				//printf("%lx (%d)\t%s\t%s\t\t", ins->address, i, ins->mnemonic, ins->op_str);
+				//print_string_hex("      \t", ins->bytes, ins->size);
+#endif
+				return i;
+				
+			}
+		}
+	}
+	return NO_COFI_TYPE;
+}
+
+int get_capstone_mode(int word_width_in_bits){
+	switch(word_width_in_bits){
+		case 64: 
+			return CS_MODE_64;
+		case 32: 
+			return CS_MODE_32;
+		default:
+			assert(false);
+	}
+}
+
+#ifdef STANDALONE_DECODER
+void pt_bitmap(uint64_t from, uint64_t to){
+#ifdef STANDALONE_VERBOSE
+	printf("%s %lx-%lx\n", __func__, from, to);
+#endif
+}
+#endif
+
+static cofi_list* analyse_assembly(disassembler_t* self, uint64_t base_address, uint64_t limit, uint64_t* failed_page, disassembler_mode_t mode){
+	cs_insn *insn;
+	cofi_type type;
+	uint64_t tmp_list_element = 0;
+	cofi_list* tmp_list_element2 = NULL;
+	bool last_nop = false;
+	uint64_t total = 0;
+	uint64_t cofi = 0;
+	uint64_t address = base_address;
+	cofi_list* predecessor = NULL;
+	cofi_list* first = NULL;
+  bool abort_disassembly = false;
+
+	
+	insn = page_cache_cs_malloc(self->page_cache, mode);
+
+	uint64_t last_no_cofi_addr = 0xFFFFFFFFFFFFFFFFULL;
+
+	while(page_cache_disassemble_iter(self->page_cache, &address, insn, failed_page, mode)){
+	
+		if (insn->address > limit){
+			break;
+		}
+			
+		type = opcode_analyzer(self, insn);
+		total++;
+		
+		if (self->debug){
+			printf("%lx:\t(%d)\t%s\t%s\t\t\n", insn->address, type, insn->mnemonic, insn->op_str);
+		}
+		
+		if (!last_nop){
+			
+	
+			edit_cofi_ptr(predecessor, self->list_element);
+			predecessor = self->list_element;
+			self->list_element = new_list_element(self->list_element);
+
+			self->list_element->cofi.type = NO_COFI_TYPE;
+			self->list_element->cofi.ins_addr = insn->address;
+      		self->list_element->cofi.ins_size = insn->size;
+			self->list_element->cofi.target_addr = 0;
+		}
+		
+		if (map_exist(self, insn->address)){
+				map_get(self, insn->address, &tmp_list_element);
+				edit_cofi_ptr(self->list_element, (cofi_list*)tmp_list_element);
+
+				
+		
+				edit_cofi_ptr(predecessor, self->list_element);
+				predecessor = self->list_element;
+				self->list_element = new_list_element(self->list_element);
+
+				self->list_element->cofi.type = NO_COFI_TYPE;
+				self->list_element->cofi.ins_addr = insn->address;
+				self->list_element->cofi.target_addr = insn->size;
+				self->list_element->cofi.target_addr = 0;
+
+			break;
+		}
+		
+		if (type != NO_COFI_TYPE){
+			if(!(type == COFI_TYPE_INDIRECT_BRANCH || type == COFI_TYPE_NEAR_RET || type == COFI_TYPE_FAR_TRANSFERS)){
+
+				cofi++;
+				last_nop = false;
+				self->list_element->cofi.type = type;
+				self->list_element->cofi.ins_addr = insn->address;
+				self->list_element->cofi.ins_size = insn->size;
+				if (type == COFI_TYPE_CONDITIONAL_BRANCH || type == COFI_TYPE_UNCONDITIONAL_DIRECT_BRANCH){
+					self->list_element->cofi.target_addr = hex_to_bin(insn->op_str);	
+				} 
+				else {
+					self->list_element->cofi.target_addr = 0;
+				}
+				map_put(self, self->list_element->cofi.ins_addr, (uint64_t)(self->list_element));
+			}
+			else{
+				//don't disassembly through ret and similar instructions to avoid disassembly inline data
+				//however we need to finish the cofi ptr datatstructure therefore we take a second loop iteration and abort
+				//after last_nop = false ist handeled
+				if(last_no_cofi_addr != 0xFFFFFFFFFFFFFFFFULL){
+					self->list_element->cofi.type = NO_COFI_TYPE;
+					self->list_element->cofi.ins_addr = last_no_cofi_addr;
+					self->list_element->cofi.ins_size = 0;
+					self->list_element->cofi.target_addr = insn->address;
+
+					predecessor = self->list_element;
+					edit_cofi_ptr(predecessor, self->list_element);
+					self->list_element = new_list_element(self->list_element);
+				}
+				self->list_element->cofi.type = type;
+				self->list_element->cofi.ins_addr = insn->address;
+				self->list_element->cofi.ins_size = insn->size;
+				self->list_element->cofi.target_addr = 0;
+
+				map_put(self, self->list_element->cofi.ins_addr, (uint64_t)(self->list_element));
+
+
+				if(last_no_cofi_addr != 0xFFFFFFFFFFFFFFFFULL){
+					map_get(self, last_no_cofi_addr, (uint64_t*)&tmp_list_element2);
+					tmp_list_element2->cofi_ptr = self->list_element;
+				}
+				abort_disassembly = true;
+			}
+			last_no_cofi_addr = 0xFFFFFFFFFFFFFFFFULL;
+		}
+		else {
+			if(last_no_cofi_addr == 0xFFFFFFFFFFFFFFFFULL){
+				last_no_cofi_addr = insn->address;
+			}
+			last_nop = true;
+			map_put(self, insn->address, (uint64_t)(self->list_element));
+		}
+		
+		if (!first){
+			first = self->list_element;
+		}
+
+		if (abort_disassembly){
+			break;
+		}
+	}
+	
+	cs_free(insn, 1);
+
+	if(first == NULL){
+		return &page_cache_failed_obj;
+	}
+	else{
+		if (*failed_page != 0xFFFFFFFFFFFFFFFFULL && self->list_element->cofi.type == 5){
+			return &page_cache_failed_obj;
+		}
+	}
+
+	return first;
+}
+
+#ifndef STANDALONE_DECODER
+disassembler_t* init_disassembler(uint64_t filter[4][2], int disassembler_word_width, redqueen_t *redqueen_state, page_cache_t* page_cache){
+#else
+disassembler_t* init_disassembler(uint64_t filter[4][2], int disassembler_word_width, page_cache_t* page_cache){
+#endif
+	disassembler_t* res = malloc(sizeof(disassembler_t));
+
+#ifndef STANDALONE_DECODER
+	assert(redqueen_state != NULL);
+#endif
+	res->infinite_loop_found = false;
+	res->debug = false;
+	res->map = kh_init(ADDR0);
+	res->list_head = create_list_head();
+	res->word_width = disassembler_word_width;
+	res->list_element = res->list_head;
+  	res->has_pending_indirect_branch = false;
+  	res->pending_indirect_branch_src = 0;
+
+	res->min_addr_0 = filter[0][0];
+	res->max_addr_0 = filter[0][1];
+	res->min_addr_1 = filter[1][0];
+	res->max_addr_1 = filter[1][1];
+	res->min_addr_2 = filter[2][0];
+	res->max_addr_2 = filter[2][1];
+	res->min_addr_3 = filter[3][0];
+	res->max_addr_3 = filter[3][1];
+
+	/* fml */
+	assert(!in_range_specific(res->min_addr_0, res->min_addr_1, res->max_addr_1));
+	assert(!in_range_specific(res->min_addr_0, res->min_addr_2, res->max_addr_2));
+	assert(!in_range_specific(res->min_addr_0, res->min_addr_3, res->max_addr_3));
+	assert(!in_range_specific(res->max_addr_0-1, res->min_addr_1, res->max_addr_1));
+	assert(!in_range_specific(res->max_addr_0-1, res->min_addr_2, res->max_addr_2));
+	assert(!in_range_specific(res->max_addr_0-1, res->min_addr_3, res->max_addr_3));
+
+	assert(!in_range_specific(res->min_addr_1, res->min_addr_0, res->max_addr_0));
+	assert(!in_range_specific(res->min_addr_1, res->min_addr_2, res->max_addr_2));
+	assert(!in_range_specific(res->min_addr_1, res->min_addr_3, res->max_addr_3));
+	assert(!in_range_specific(res->max_addr_1-1, res->min_addr_0, res->max_addr_0));
+	assert(!in_range_specific(res->max_addr_1-1, res->min_addr_2, res->max_addr_2));
+	assert(!in_range_specific(res->max_addr_1-1, res->min_addr_3, res->max_addr_3));
+
+	assert(!in_range_specific(res->min_addr_2, res->min_addr_1, res->max_addr_1));
+	assert(!in_range_specific(res->min_addr_2, res->min_addr_0, res->max_addr_0));
+	assert(!in_range_specific(res->min_addr_2, res->min_addr_3, res->max_addr_3));
+	assert(!in_range_specific(res->max_addr_2-1, res->min_addr_1, res->max_addr_1));
+	assert(!in_range_specific(res->max_addr_2-1, res->min_addr_0, res->max_addr_0));
+	assert(!in_range_specific(res->max_addr_2-1, res->min_addr_3, res->max_addr_3));
+
+	assert(!in_range_specific(res->min_addr_3, res->min_addr_1, res->max_addr_1));
+	assert(!in_range_specific(res->min_addr_3, res->min_addr_2, res->max_addr_2));
+	assert(!in_range_specific(res->min_addr_3, res->min_addr_0, res->max_addr_0));
+	assert(!in_range_specific(res->max_addr_3-1, res->min_addr_1, res->max_addr_1));
+	assert(!in_range_specific(res->max_addr_3-1, res->min_addr_2, res->max_addr_2));
+	assert(!in_range_specific(res->max_addr_3-1, res->min_addr_0, res->max_addr_0));
+
+
+#ifndef STANDALONE_DECODER
+	if (redqueen_state != NULL){
+		res->redqueen_mode = true;
+		res->redqueen_state = redqueen_state;
+	}
+	else{
+		res->redqueen_mode = false;
+	}
+#endif
+
+	res->page_cache = page_cache;
+	res->trace_cache = trace_cache_new(fuzz_bitmap_get_size());
+
+	return res;
+}
+
+void destroy_disassembler(disassembler_t* self){
+	kh_destroy(ADDR0, self->map);
+	free_list(self->list_head);
+	trace_cache_destroy(self->trace_cache);
+	free(self);
+}
+
+void reset_disassembler(disassembler_t* self){
+
+	kh_destroy(ADDR0, self->map);
+	free_list(self->list_head);
+
+	self->map = kh_init(ADDR0);
+	self->list_head = create_list_head();
+	self->list_element = self->list_head;
+
+	trace_cache_destroy(self->trace_cache);
+	self->trace_cache = trace_cache_new(fuzz_bitmap_get_size());
+}
+
+static inline cofi_list* get_obj(disassembler_t* self, uint64_t entry_point, tnt_cache_t* tnt_cache_state, uint64_t* failed_page, disassembler_mode_t mode){
+	uint64_t tmp_list_element;
+
+	if(map_get(self, entry_point, &tmp_list_element)){
+		
+		if(in_range_specific(entry_point, self->min_addr_0, self->max_addr_0)){
+			return analyse_assembly(self, entry_point, self->max_addr_0, failed_page, mode);
+		}
+		else if(in_range_specific(entry_point, self->min_addr_1, self->max_addr_1)){
+			return analyse_assembly(self, entry_point, self->max_addr_1, failed_page, mode);
+		}
+		else if(in_range_specific(entry_point, self->min_addr_2, self->max_addr_2)){
+			return analyse_assembly(self, entry_point, self->max_addr_2, failed_page, mode);
+		}
+		else if(in_range_specific(entry_point, self->min_addr_3, self->max_addr_3)){
+			return analyse_assembly(self, entry_point, self->max_addr_3, failed_page, mode);
+		}
+		else{
+			*failed_page = 0;
+			return &oob_obj;
+		}
+	}
+	*failed_page = 0;
+	return (cofi_list*)tmp_list_element;
+}
+
+void disassembler_flush(disassembler_t* self){
+
+}
+
+
+static inline void inform_disassembler_target_ip(disassembler_t* self, uint64_t target_ip, bool trace_mode){
+  if(self->has_pending_indirect_branch){
+		self->has_pending_indirect_branch = false;
+
+#ifndef STANDALONE_DECODER
+		if(trace_mode){
+			WRITE_SAMPLE_DECODED_DETAILED("** %lx -rq-> %lx \n", self->pending_indirect_branch_src, target_ip);
+      redqueen_register_transition(self->redqueen_state, self->pending_indirect_branch_src, target_ip);
+		}
+		if(!trace_mode){
+			add_result_tracelet_cache(self->trace_cache->trace_cache, self->pending_indirect_branch_src, target_ip);
+		}
+#endif
+  disassembler_flush(self);
+  }
+}
+
+#define MAX_LOOP_COUNT 80000
+
+__attribute__((hot)) static disas_result_t trace_disassembler_loop(disassembler_t* self, uint64_t* entry_point, uint64_t limit, tnt_cache_t* tnt_cache_state, tracelet_cache_t** new_tracelet, trace_cache_key_t* key, uint64_t* failed_page, disassembler_mode_t mode, bool trace_mode);
+
+__attribute__((hot)) disas_result_t trace_disassembler(disassembler_t* self, uint64_t entry_point, uint64_t limit, tnt_cache_t* tnt_cache_state, uint64_t* failed_page, disassembler_mode_t mode){
+
+	*failed_page = 0;
+
+#ifndef STANDALONE_DECODER
+	if(self->redqueen_mode && self->redqueen_state->trace_mode){
+			return trace_disassembler_loop(self, &entry_point, limit, tnt_cache_state, NULL, NULL, failed_page, mode, true);
+	}
+#endif
+
+	uint64_t entry_point_tmp = entry_point;
+	tracelet_cache_t* new_tracelet = NULL;
+
+	trace_cache_key_t key;
+
+	while(true){
+		key.entry = entry_point_tmp;
+		key.limit = limit;
+		key.tnt_hash = get_tnt_hash(tnt_cache_state);
+
+		new_tracelet = trace_cache_fetch(self->trace_cache, key);
+		if(new_tracelet){
+			entry_point_tmp = apply_trace_cache_to_bitmap(new_tracelet, tnt_cache_state, true);
+			
+			if(!new_tracelet->cont_exec){
+				return disas_success;
+			}
+		}
+		else{
+			disas_result_t cont = trace_disassembler_loop(self, &entry_point_tmp, limit, tnt_cache_state, &new_tracelet, &key, failed_page, mode, false);
+			if(unlikely(cont == disas_page_fault)){
+				/* Early exit at this point. Don't apply preliminary results to the trace cache! */
+				return disas_page_fault;
+			}
+			set_next_entry_addres_tracelet_cache(new_tracelet, entry_point_tmp);
+			apply_trace_cache_to_bitmap(new_tracelet, tnt_cache_state, false);
+
+			trace_cache_add(self->trace_cache, key, new_tracelet);
+			if(cont != disas_success){
+				return disas_success;
+			}
+		}
+	}
+ }
+
+ __attribute__((hot))  static disas_result_t trace_disassembler_loop(disassembler_t* self, uint64_t* entry_point, uint64_t limit, tnt_cache_t* tnt_cache_state, tracelet_cache_t** new_tracelet, trace_cache_key_t* key, uint64_t* failed_page, disassembler_mode_t mode, bool trace_mode){
+ //__attribute__((hot)) static bool trace_disassembler_loop(disassembler_t* self, uint64_t* entry_point, uint64_t limit, tnt_cache_t* tnt_cache_state){
+
+ 	static void* dispatch_table[] = {
+		&&do_conditional_branch,		// COFI_TYPE_CONDITIONAL_BRANCH, 
+		&&do_unconditional_branch,	// COFI_TYPE_UNCONDITIONAL_DIRECT_BRANCH
+		&&do_indirect_branch,				// COFI_TYPE_INDIRECT_BRANCH
+		&&do_near_ret,							// COFI_TYPE_NEAR_RET
+		&&do_far_transfers,					// COFI_TYPE_FAR_TRANSFERS
+		&&do_no_cofi,								// NO_COFI_TYPE
+		&&do_disassemble_t,					// DISASSEMBLY_PENDING
+		&&do_out_of_bounds,					// OUT_OF_BOUNDS
+		&&do_infinite_loop,
+		&&do_page_cache_failed,
+	};
+
+	static void* dispatch_table_t[] = {
+		&&do_branch_check_t,		// COFI_TYPE_CONDITIONAL_BRANCH, 
+		&&do_branch_check_t,		// COFI_TYPE_UNCONDITIONAL_DIRECT_BRANCH
+		&&do_branch_check_t,		// COFI_TYPE_INDIRECT_BRANCH
+		&&do_branch_check_t,		// COFI_TYPE_NEAR_RET
+		&&do_branch_check_t,		// COFI_TYPE_FAR_TRANSFERS
+		&&do_branch_check_t,		// NO_COFI_TYPE
+		&&do_disassemble_t,			// DISASSEMBLY_PENDING
+		&&do_out_of_bounds,			// OUT_OF_BOUNDS
+		&&do_infinite_loop,
+		&&do_page_cache_failed,
+	};
+
+	static void* dispatch_table_nt[] = {
+		&&do_branch_check_nt,		// COFI_TYPE_CONDITIONAL_BRANCH, 
+		&&do_branch_check_nt,		// COFI_TYPE_UNCONDITIONAL_DIRECT_BRANCH
+		&&do_branch_check_nt,		// COFI_TYPE_INDIRECT_BRANCH
+		&&do_branch_check_nt,		// COFI_TYPE_NEAR_RET
+		&&do_branch_check_nt,		// COFI_TYPE_FAR_TRANSFERS
+		&&do_branch_check_nt,		// NO_COFI_TYPE
+		&&do_disassemble_nt,		// DISASSEMBLY_PENDING
+		&&do_out_of_bounds,			// OUT_OF_BOUNDS
+		&&do_infinite_loop,
+		&&do_page_cache_failed,
+	};
+
+
+	
+
+	int loop = 0;
+
+	/* max 58 iterations */
+	//int counter = 0;
+
+
+	#define BRANCH_CHECK_T() dispatch_type = !limit_check(last_obj->cofi.target_addr, obj->cofi.ins_addr, limit, *entry_point) && is_empty_tnt_cache(tnt_cache_state) ? OUT_OF_BOUNDS: obj->cofi.type ;
+	#define BRANCH_CHECK_NT() dispatch_type = !limit_check(last_obj->cofi.ins_addr+last_obj->cofi.ins_size, obj->cofi.ins_addr, limit, *entry_point) && is_empty_tnt_cache(tnt_cache_state) ? OUT_OF_BOUNDS: obj->cofi.type ;
+
+	#define DISPATCH_T_LOOP_PRE() dispatch_type = (loop < MAX_LOOP_COUNT) ? dispatch_type : INFINITE_LOOP //; goto *dispatch_table_t[dispatch_type]
+
+
+	#define DISPATCH_T_LOOP() dispatch_type = (*entry_point == limit && is_empty_tnt_cache(tnt_cache_state)) ? INFINITE_LOOP: dispatch_type; goto *dispatch_table_t[dispatch_type]
+
+
+	#define DISPATCH() goto *dispatch_table[dispatch_type]
+	#define DISPATCH_T() goto *dispatch_table_t[obj->cofi.type]
+	#define DISPATCH_NT() goto *dispatch_table_nt[obj->cofi.type]
+
+	#define FETCH(fetch_target) obj = get_obj(self, *entry_point, tnt_cache_state, failed_page, mode); dispatch_type = obj->cofi.type; goto *dispatch_table[dispatch_type]
+
+	uint8_t dispatch_type = 0;
+	cofi_list *obj;
+	cofi_list *last_obj = NULL;
+
+	inform_disassembler_target_ip(self, *entry_point, trace_mode);
+
+	if(likely(!trace_mode)){
+		reset_tracelet_tmp_cache(self->trace_cache->trace_cache);
+	}
+
+	FETCH(*entry_point);
+	do_conditional_branch:
+		
+
+		if(likely(!trace_mode) && self->trace_cache->trace_cache->cache.tnt_bits == MAX_RESULTS_PER_CACHE-1){
+			*entry_point = obj->cofi.ins_addr;// + obj->cofi.ins_size;;
+			if(!trace_mode){
+				*new_tracelet = new_from_tracelet_cache_tmp(self->trace_cache->trace_cache, true);
+			}
+			if(trace_mode)
+			WRITE_SAMPLE_DECODED_DETAILED("disas_success\n");
+			return disas_success;
+		}
+		
+				
+		switch(process_tnt_cache(tnt_cache_state)){
+			case TNT_EMPTY:
+			if(trace_mode)
+			WRITE_SAMPLE_DECODED_DETAILED("(%d)\t%lx\tCACHE EMPTY\n", COFI_TYPE_CONDITIONAL_BRANCH, obj->cofi.ins_addr);
+				if(!trace_mode){
+					*new_tracelet = new_from_tracelet_cache_tmp(self->trace_cache->trace_cache, false);
+				}
+				return disas_tnt_empty;
+
+			case TAKEN:
+				WRITE_SAMPLE_DECODED_DETAILED("(%d)\t%lx\t(Taken)\n", COFI_TYPE_CONDITIONAL_BRANCH, obj->cofi.ins_addr);			
+#ifndef STANDALONE_DECODER
+				if(unlikely(trace_mode)){
+					redqueen_register_transition(self->redqueen_state, obj->cofi.ins_addr, obj->cofi.target_addr);
+				}
+#endif
+				if(!trace_mode){
+					add_result_tracelet_cache(self->trace_cache->trace_cache, obj->cofi.ins_addr, obj->cofi.target_addr);
+				}
+				last_obj = obj;
+				obj = get_obj(self, obj->cofi.target_addr, tnt_cache_state, failed_page, mode);
+				loop = 0;
+				DISPATCH_T();
+
+			case NOT_TAKEN:
+				WRITE_SAMPLE_DECODED_DETAILED("(%d)\t%lx\t(Not Taken)\n", COFI_TYPE_CONDITIONAL_BRANCH ,obj->cofi.ins_addr);
+#ifndef STANDALONE_DECODER
+				if(unlikely(trace_mode)){
+					redqueen_register_transition(self->redqueen_state, obj->cofi.ins_addr, obj->cofi.ins_addr + obj->cofi.ins_size);
+				}
+#endif
+				if(!trace_mode){
+					add_result_tracelet_cache(self->trace_cache->trace_cache, obj->cofi.ins_addr, obj->cofi.ins_addr + obj->cofi.ins_size);
+				}
+				last_obj = obj;
+				/* obj->cofi_ptr is broken af (fix me later) */
+				//obj = obj->cofi_ptr;
+				obj = get_obj(self, obj->cofi.ins_addr + obj->cofi.ins_size, tnt_cache_state, failed_page, mode);
+				loop = 0;
+				DISPATCH_NT();
+}
+
+	do_unconditional_branch:
+		WRITE_SAMPLE_DECODED_DETAILED("(%d)\t%lx\n", COFI_TYPE_UNCONDITIONAL_DIRECT_BRANCH ,obj->cofi.ins_addr);
+		last_obj = obj;
+		obj = obj->cofi_target_ptr;
+
+#ifndef STANDALONE_DECODER
+		if(unlikely(trace_mode)){
+      redqueen_register_transition(self->redqueen_state, last_obj->cofi.ins_addr, last_obj->cofi.target_addr);
+		}
+#endif
+
+		loop++;
+		DISPATCH_T_LOOP();
+		DISPATCH_T();
+
+	do_indirect_branch:
+	do_near_ret:
+			if(trace_mode)
+		WRITE_SAMPLE_DECODED_DETAILED("(3)\t%lx\n",obj->cofi.ins_addr);
+		if(!trace_mode){
+			*new_tracelet = new_from_tracelet_cache_tmp(self->trace_cache->trace_cache, false);
+		}
+
+		if(unlikely(trace_mode)){
+    	self->has_pending_indirect_branch = true;
+    	self->pending_indirect_branch_src = obj->cofi.ins_addr;
+		}
+		return disas_tip_pending;
+
+	do_far_transfers:
+	if(trace_mode)
+		WRITE_SAMPLE_DECODED_DETAILED("(4)\t%lx\n",obj->cofi.ins_addr);
+		if(!trace_mode){
+			*new_tracelet = new_from_tracelet_cache_tmp(self->trace_cache->trace_cache, false);
+		}
+	if(trace_mode){
+    	self->has_pending_indirect_branch = true;
+    	self->pending_indirect_branch_src = obj->cofi.ins_addr;
+		}
+		return disas_tip_pending;
+
+	do_no_cofi:
+		WRITE_SAMPLE_DECODED_DETAILED("(5)\t%lx\n",obj->cofi.ins_addr);
+    last_obj = obj;
+    obj = obj->cofi_ptr;
+		DISPATCH_NT();
+
+	do_disassemble_t:
+		last_obj->cofi_target_ptr = get_obj(self, last_obj->cofi.target_addr, tnt_cache_state, failed_page, mode);
+		obj = last_obj->cofi_target_ptr;
+		DISPATCH_T();
+
+	do_branch_check_t:
+		BRANCH_CHECK_T();
+		DISPATCH();
+
+	do_disassemble_nt:
+		last_obj->cofi_ptr = get_obj(self, obj->cofi.ins_addr+obj->cofi.ins_size, tnt_cache_state, failed_page, mode);
+		obj = last_obj->cofi_ptr;
+		DISPATCH_NT();
+
+	do_branch_check_nt:
+		BRANCH_CHECK_NT();
+		DISPATCH();
+
+	do_out_of_bounds:
+		if(likely(!trace_mode)){
+			*new_tracelet = new_from_tracelet_cache_tmp(self->trace_cache->trace_cache, false);
+		}
+		WRITE_SAMPLE_DECODED_DETAILED("OUT OF BOUNDS %lx %lx %lx %lx (%d)\n", last_obj->cofi.ins_addr + last_obj->cofi.ins_size, last_obj->cofi.target_addr, obj->cofi.ins_addr, limit, is_empty_tnt_cache(tnt_cache_state));
+		return disas_out_of_bounds;	
+
+	do_infinite_loop:
+		if(likely(!trace_mode)){
+			*new_tracelet = new_from_tracelet_cache_tmp(self->trace_cache->trace_cache, false);
+		}
+		WRITE_SAMPLE_DECODED_DETAILED("INFINITE LOOP\n");
+		return disas_infinite_loop;
+	
+	do_page_cache_failed:
+		WRITE_SAMPLE_DECODED_DETAILED("PAGE_CACHE FAULT\n");
+		return disas_page_fault;
+}
+
+
diff --new-file -ur qemu/pt/disassembler.h QEMU-PT/pt/disassembler.h
--- qemu/pt/disassembler.h	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/disassembler.h	2021-08-24 21:54:55.938586449 +0200
@@ -0,0 +1,139 @@
+/*
+
+Copyright (C) 2017 Sergej Schumilo
+
+This file is part of QEMU-PT (kAFL).
+
+QEMU-PT is free software: you can redistribute it and/or modify
+it under the terms of the GNU General Public License as published by
+the Free Software Foundation, either version 2 of the License, or
+(at your option) any later version.
+
+QEMU-PT is distributed in the hope that it will be useful,
+but WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with QEMU-PT.  If not, see <http://www.gnu.org/licenses/>.
+
+*/
+
+#ifndef DISASSEMBLER_H
+#define DISASSEMBLER_H
+
+#include <stdint.h>
+#include <stdbool.h>
+#include <unistd.h>
+#include <sys/time.h>
+#include <inttypes.h>
+#include <capstone/capstone.h>
+#include <capstone/x86.h>
+#include <stdbool.h>
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <fcntl.h>
+#ifdef STANDALONE_DECODER
+#include "tnt_cache.h"
+#include "khash.h"
+#include "page_cache.h"
+#else
+#include "pt/tnt_cache.h"
+#include "pt/logger.h"
+#include "pt/khash.h"
+#include "qemu/osdep.h"
+#include "pt/redqueen.h"
+#include "pt/page_cache.h"
+#endif
+#include "trace_cache.h"
+
+#define unlikely(x)   __builtin_expect(!!(x), 0)
+
+KHASH_MAP_INIT_INT(ADDR0, uint64_t)
+
+typedef struct{
+	uint16_t opcode;
+	uint8_t modrm;
+	uint8_t opcode_prefix;
+} cofi_ins;
+
+typedef enum cofi_types{
+	COFI_TYPE_CONDITIONAL_BRANCH, 
+	COFI_TYPE_UNCONDITIONAL_DIRECT_BRANCH, 
+	COFI_TYPE_INDIRECT_BRANCH, 
+	COFI_TYPE_NEAR_RET, 
+	COFI_TYPE_FAR_TRANSFERS,
+	NO_COFI_TYPE,
+	DISASSEMBLY_PENDING,
+	OUT_OF_BOUNDS,
+	INFINITE_LOOP,
+	PAGE_CACHE_FAILED,
+} cofi_type;
+
+
+typedef struct {
+	uint64_t ins_addr;
+	uint64_t target_addr;
+	uint16_t ins_size;
+	cofi_type type;
+} cofi_header;
+
+typedef struct cofi_list {
+	struct cofi_list *list_ptr;
+	struct cofi_list *cofi_ptr;
+	struct cofi_list *cofi_target_ptr;
+	cofi_header cofi;
+} cofi_list;
+
+typedef struct disassembler_s{
+	bool infinite_loop_found;
+
+	uint8_t* code;
+	uint64_t min_addr_0;
+	uint64_t max_addr_0;
+	uint64_t min_addr_1;
+	uint64_t max_addr_1;
+	uint64_t min_addr_2;
+	uint64_t max_addr_2;
+	uint64_t min_addr_3;
+	uint64_t max_addr_3;
+
+
+	khash_t(ADDR0) *map;
+	cofi_list* list_head;
+	cofi_list* list_element;
+	bool debug;
+	bool has_pending_indirect_branch;
+  int word_width;
+	uint64_t pending_indirect_branch_src;
+	bool redqueen_mode;
+#ifndef STANDALONE_DECODER
+	redqueen_t* redqueen_state;
+#endif
+	page_cache_t* page_cache;
+	trace_cache_t* trace_cache;
+} disassembler_t;
+
+#ifndef STANDALONE_DECODER
+disassembler_t* init_disassembler(uint64_t filter[4][2], int disassembler_word_width, redqueen_t *redqueen_state, page_cache_t* page_cache);
+#else
+disassembler_t* init_disassembler(uint64_t filter[4][2], int disassembler_word_width, page_cache_t* page_cache);
+#endif
+
+
+typedef enum disas_result_s { 
+	disas_success, 
+	disas_tnt_empty, 
+	disas_tip_pending,
+	disas_out_of_bounds,
+	disas_infinite_loop,
+	disas_page_fault,
+} disas_result_t;
+
+void reset_disassembler(disassembler_t* self);
+int get_capstone_mode(int word_width_in_bits);
+void disassembler_flush(disassembler_t* self);
+ __attribute__((hot)) disas_result_t trace_disassembler(disassembler_t* self, uint64_t entry_point, uint64_t limit, tnt_cache_t* tnt_cache_state, uint64_t* failed_page, disassembler_mode_t mode);
+void destroy_disassembler(disassembler_t* self);
+
+#endif
diff --new-file -ur qemu/pt/fast_vm_reload.c QEMU-PT/pt/fast_vm_reload.c
--- qemu/pt/fast_vm_reload.c	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/fast_vm_reload.c	2021-08-26 11:20:22.675823051 +0200
@@ -0,0 +1,1386 @@
+/*
+
+Copyright (C) 2017 Sergej Schumilo
+
+This file is part of QEMU-PT (HyperTrash / kAFL).
+
+QEMU-PT is free software: you can redistribute it and/or modify
+it under the terms of the GNU General Public License as published by
+the Free Software Foundation, either version 2 of the License, or
+(at your option) any later version.
+
+QEMU-PT is distributed in the hope that it will be useful,
+but WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with QEMU-PT.  If not, see <http://www.gnu.org/licenses/>.
+
+*/
+
+#include "qemu/osdep.h"
+#include "sysemu/sysemu.h"
+#include "cpu.h"
+#include "qemu/main-loop.h"
+
+#include "exec/ram_addr.h"
+#include "qemu/rcu_queue.h"
+#include "migration/migration.h"
+#include "migration/register.h"
+#include "migration/savevm.h"
+#include "migration/qemu-file.h"
+#include "migration/qjson.h"
+#include "migration/global_state.h"
+
+
+#include <linux/kvm.h>
+#include <sys/ioctl.h>
+#include <sys/mman.h>
+#include <sys/stat.h>
+#include <sys/types.h>
+#include <immintrin.h>
+#include <stdint.h>
+
+#include "sysemu/kvm_int.h"
+#include "sysemu/cpus.h"
+#include "sysemu/reset.h"
+
+#include "pt/fast_vm_reload.h"
+#include "pt/debug.h"
+#include "pt/state.h"
+
+#include "sysemu/block-backend.h"
+#include "block/qapi.h"
+#include "sysemu/runstate.h"
+#include "migration/vmstate.h"
+
+#define FDL_MMAP_ADDRESS 0x2ffff0000000
+
+//#define RESET_VRAM
+//#define DEBUG_FDL
+//#define BENCHMARK
+//#define VERBOSE_DIRTY_PAGES
+//#define VERBOSE_QEMU_USER_SET_DIRTY
+#define STATE_BUFFER    0x8000000  /* up to 128MB */
+
+#define USER_FDL_SLOTS 0x400000 /* fix this later */
+
+#define BENCHMARK_ITERATIONS 1024*100
+
+#define KVM_VMX_FDL_SETUP_FD                _IO(KVMIO,  0xe5)
+#define KVM_VMX_FDL_SET                     _IOW(KVMIO, 0xe6, __u64)
+#define KVM_VMX_FDL_FLUSH                   _IO(KVMIO,  0xe7)
+#define KVM_VMX_FDL_GET_INDEX               _IOR(KVMIO, 0xe8, __u64)
+
+int vmx_fdl_fd = 0;
+uint64_t* fdl_data = NULL;
+
+extern int qemu_savevm_state(QEMUFile *f, Error **errp);
+
+/* new savevm routine */
+typedef struct SaveStateEntry {
+    QTAILQ_ENTRY(SaveStateEntry) entry;
+    char idstr[256];
+    int instance_id;
+    int alias_id;
+    int version_id;
+    int load_version_id;
+    int section_id;
+    int load_section_id;
+    SaveVMHandlers *ops;
+    const VMStateDescription *vmsd;
+    void *opaque;
+    void *compat;
+    int is_ram;
+} SaveStateEntry;
+
+
+typedef struct SaveState {
+    QTAILQ_HEAD(, SaveStateEntry) handlers;
+    int global_section_id;
+    bool skip_configuration;
+    uint32_t len;
+    const char *name;
+    uint32_t target_page_bits;
+} SaveState;
+
+extern SaveState savevm_state;
+
+extern void vmstate_save(QEMUFile *f, SaveStateEntry *se, QJSON *vmdesc);
+extern bool should_send_vmdesc(void);
+
+extern bool skip_section_footers;
+
+
+extern void save_section_footer(QEMUFile *f, SaveStateEntry *se);
+extern void save_section_header(QEMUFile *f, SaveStateEntry *se, uint8_t section_type);
+
+/* skip block ram */
+static void fast_qemu_savevm_state_complete_precopy(QEMUFile *f, bool iterable_only)
+{
+    QJSON *vmdesc;
+    int vmdesc_len;
+    SaveStateEntry *se;
+    int ret;
+    bool in_postcopy = migration_in_postcopy();
+
+    cpu_synchronize_all_states();
+
+    QTAILQ_FOREACH(se, &savevm_state.handlers, entry) {
+        if(strcmp(se->idstr, "ram") && strcmp(se->idstr, "block")){
+            if (!se->ops ||
+                (in_postcopy && se->ops->save_live_complete_postcopy) ||
+                (in_postcopy && !iterable_only) ||
+                !se->ops->save_live_complete_precopy) {
+                continue;
+            }
+
+            if (se->ops && se->ops->is_active) {
+                if (!se->ops->is_active(se->opaque)) {
+                    continue;
+                }
+            }
+
+            save_section_header(f, se, QEMU_VM_SECTION_END);
+
+            ret = se->ops->save_live_complete_precopy(f, se->opaque);
+            save_section_footer(f, se);
+            if (ret < 0) {
+                qemu_file_set_error(f, ret);
+                return;
+            }
+        }
+    }
+
+    if (iterable_only) {
+        return;
+    }
+
+    vmdesc = qjson_new();
+    json_prop_int(vmdesc, "page_size", TARGET_PAGE_SIZE);
+    json_start_array(vmdesc, "devices");
+    QTAILQ_FOREACH(se, &savevm_state.handlers, entry) {
+        if(strcmp(se->idstr, "ram") && strcmp(se->idstr, "block")){
+            if ((!se->ops || !se->ops->save_state) && !se->vmsd) {
+                continue;
+            }
+            if (se->vmsd && !vmstate_save_needed(se->vmsd, se->opaque)) {
+                continue;
+            }
+
+            json_start_object(vmdesc, NULL);
+            json_prop_str(vmdesc, "name", se->idstr);
+            json_prop_int(vmdesc, "instance_id", se->instance_id);
+
+            save_section_header(f, se, QEMU_VM_SECTION_FULL);
+            vmstate_save(f, se, vmdesc);
+            save_section_footer(f, se);
+
+            json_end_object(vmdesc);
+        }
+    }
+
+    if (!in_postcopy) {
+        /* Postcopy stream will still be going */
+        qemu_put_byte(f, QEMU_VM_EOF);
+    }
+
+    json_end_array(vmdesc);
+    qjson_finish(vmdesc);
+    vmdesc_len = strlen(qjson_get_str(vmdesc));
+
+    if (should_send_vmdesc()) {
+        qemu_put_byte(f, QEMU_VM_VMDESCRIPTION);
+        qemu_put_be32(f, vmdesc_len);
+        qemu_put_buffer(f, (uint8_t *)qjson_get_str(vmdesc), vmdesc_len);
+    }
+    qjson_destroy(vmdesc);
+
+    qemu_fflush(f);
+}
+
+
+static int fast_qemu_savevm_state_iterate(QEMUFile *f, bool postcopy) {
+    SaveStateEntry *se;
+    int ret = 1;
+
+    QTAILQ_FOREACH(se, &savevm_state.handlers, entry) {
+        if(strcmp(se->idstr, "ram") && strcmp(se->idstr, "block")){
+            if (!se->ops || !se->ops->save_live_iterate) {
+                continue;
+            }
+            if (se->ops && se->ops->is_active) {
+                if (!se->ops->is_active(se->opaque)) {
+                    continue;
+                }
+            }
+            /*
+             * In the postcopy phase, any device that doesn't know how to
+             * do postcopy should have saved it's state in the _complete
+             * call that's already run, it might get confused if we call
+             * iterate afterwards.
+             */
+            if (postcopy && !se->ops->save_live_complete_postcopy) {
+                continue;
+            }
+            if (qemu_file_rate_limit(f)) {
+                return 0;
+            }
+
+            save_section_header(f, se, QEMU_VM_SECTION_PART);
+
+            ret = se->ops->save_live_iterate(f, se->opaque);
+            save_section_footer(f, se);
+
+            if (ret < 0) {
+                qemu_file_set_error(f, ret);
+            }
+            if (ret <= 0) {
+                /* Do not proceed to the next vmstate before this one reported
+                   completion of the current stage. This serializes the migration
+                   and reduces the probability that a faster changing state is
+                   synchronized over and over again. */
+                break;
+            }
+        }
+    }
+    return ret;
+}
+
+static void fast_qemu_savevm_state_setup(QEMUFile *f){
+    SaveStateEntry *se;
+    int ret;
+
+    QTAILQ_FOREACH(se, &savevm_state.handlers, entry) {
+        if(strcmp(se->idstr, "ram") && strcmp(se->idstr, "block")){
+            if (!se->ops || !se->ops->save_setup) {
+                continue;
+            }
+            if (se->ops && se->ops->is_active) {
+                if (!se->ops->is_active(se->opaque)) {
+                    continue;
+                }
+            }
+            save_section_header(f, se, QEMU_VM_SECTION_START);
+
+            ret = se->ops->save_setup(f, se->opaque);
+            save_section_footer(f, se);
+            if (ret < 0) {
+                qemu_file_set_error(f, ret);
+                break;
+            }
+        }
+    }
+}
+
+static int fast_qemu_savevm_state(QEMUFile *f, Error **errp) {
+    qemu_savevm_state_header(f);
+    fast_qemu_savevm_state_setup(f);
+
+    while (qemu_file_get_error(f) == 0) {
+        if (fast_qemu_savevm_state_iterate(f, false) > 0) {
+            fast_qemu_savevm_state_complete_precopy(f, false);
+            break;
+        }
+    }
+
+    return 0;
+}
+
+
+/* QEMUFile RAM Emulation */
+static ssize_t fast_savevm_writev_buffer(void *opaque, struct iovec *iov, int iovcnt, int64_t pos){
+    ssize_t retval = 0;
+    for(uint32_t i = 0; i < iovcnt; i++){
+        memcpy((void*)(((struct fast_savevm_opaque_t*)(opaque))->buf + ((struct fast_savevm_opaque_t*)(opaque))->pos), iov[i].iov_base, iov[i].iov_len);
+        ((struct fast_savevm_opaque_t*)(opaque))->pos += iov[i].iov_len;
+        retval += iov[i].iov_len;
+    }   
+    return retval;
+}
+
+static int fast_savevm_fclose(void *opaque){
+    return 0;
+}
+
+static int fast_savevm_fclose_save_to_file(void *opaque){     
+    FILE* f = ((struct fast_savevm_opaque_t*)(opaque))->f;
+    fwrite(((struct fast_savevm_opaque_t*)(opaque))->buf, ((struct fast_savevm_opaque_t*)(opaque))->pos, 1, f);
+    fclose(f);
+
+    return 0;
+}
+
+static int fast_loadvm_fclose(void *opaque){
+    return 0;
+}
+
+static ssize_t fast_loadvm_get_buffer(void *opaque, uint8_t *buf, int64_t pos, size_t size){
+    memcpy(buf, (void*)(((struct fast_savevm_opaque_t*)(opaque))->buf + pos), size);
+    return size;
+}
+
+static const QEMUFileOps fast_savevm_ops = {
+    .writev_buffer  = (QEMUFileWritevBufferFunc*)fast_savevm_writev_buffer,
+    .close          = (QEMUFileCloseFunc*)fast_savevm_fclose
+};
+
+static const QEMUFileOps fast_savevm_ops2 = {
+    .writev_buffer  = (QEMUFileWritevBufferFunc*)fast_savevm_writev_buffer,
+    .close          = (QEMUFileCloseFunc*)fast_savevm_fclose_save_to_file
+};
+
+static const QEMUFileOps fast_loadvm_ops = {
+    .get_buffer     = (QEMUFileGetBufferFunc*)fast_loadvm_get_buffer,
+    .close          = (QEMUFileCloseFunc*)fast_loadvm_fclose
+};
+
+/* shadow memory */
+static void free_shadow_memory(fast_reload_t* self){
+    assert(self->guest_ram_size);
+    munmap(self->ptr, self->guest_ram_size);
+
+    free(self->shadow_memory);
+    free(self->ram_block_array);
+}
+
+static uint64_t alloc_shadow_memory(fast_reload_t* self){
+    RAMBlock *block;
+    uint64_t guest_ram_size = 0;
+    rcu_read_lock();
+    uint64_t offset = 0;
+    uint32_t i = 0;
+    void* ptr = NULL;
+
+    uint64_t total_memory_count = 0;
+    uint64_t start_region = 0;
+
+    QLIST_FOREACH_RCU(block, &ram_list.blocks, next) {
+        guest_ram_size += block->used_length;
+        QEMU_PT_PRINTF(RELOAD_PREFIX, "Block: %s (%lx)", block->idstr, block->used_length);
+
+        if(strstr(block->idstr, "pc.ram") != NULL) {
+            self->ram_region_index = self->shadow_memory_regions;
+            fprintf(stderr, "FOUND ---> RAM\n");
+        }
+
+        if(strstr(block->idstr, "vga.vram") != NULL) {
+            self->vram_region_index = self->shadow_memory_regions;
+            fprintf(stderr, "FOUND ---> VGA\n");
+        }
+
+        total_memory_count += block->used_length;
+
+        if(!block->offset){
+            start_region = (uint64_t)block->host;
+        }
+
+        self->shadow_memory_regions++;
+    }
+
+    QEMU_PT_PRINTF(RELOAD_PREFIX, "Start: %lx - %lx", start_region, start_region+total_memory_count);
+
+    ptr = mmap(NULL, guest_ram_size, PROT_READ | PROT_WRITE , MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);
+    madvise(ptr, guest_ram_size, MADV_MERGEABLE);
+
+    self->shadow_memory = malloc(sizeof(void*)*self->shadow_memory_regions);
+    self->ram_block_array = malloc(sizeof(RAMBlock*)*self->shadow_memory_regions);
+
+    offset = 0;
+    i = 0;
+    QLIST_FOREACH_RCU(block, &ram_list.blocks, next) {
+        QEMU_PT_PRINTF(RELOAD_PREFIX, "%lx %lx %lx\t%s\t%p", block->offset, block->used_length, block->max_length, block->idstr, block->host);
+        self->ram_block_array[i] = block;
+
+        memcpy(ptr+offset, block->host, block->used_length);
+        self->shadow_memory[i++] = ptr+offset;
+        offset += block->used_length;
+    }   
+
+    assert(start_region);
+
+    QEMU_PT_PRINTF(RELOAD_PREFIX, "Allocating Memory (%p) Size: %lx", ptr, guest_ram_size);
+
+    for(i = 0; i < self->shadow_memory_regions; i++){
+        QEMU_PT_PRINTF(RELOAD_PREFIX, "%p", self->shadow_memory[i]);
+    }
+
+    self->ptr = ptr;
+
+    int shadow_memory_regions_tmp = 0;
+    QLIST_FOREACH_RCU(block, &ram_list.blocks, next) {
+        if(!block->mr->readonly){
+
+            self->mr_ranges[self->mr_ranges_num].host_ptr = (void*)block->host;
+            self->mr_ranges[self->mr_ranges_num].shadow_ptr = (void*)self->shadow_memory[shadow_memory_regions_tmp]; 
+            
+            self->mr_ranges[self->mr_ranges_num].mr_start = block->mr->addr;
+            self->mr_ranges[self->mr_ranges_num].mr_end = block->mr->addr+(uint64_t)block->mr->size;
+            self->mr_ranges[self->mr_ranges_num].offset = block->offset;
+   
+            self->mr_ranges_num++;
+        }
+        shadow_memory_regions_tmp++;
+    }
+
+    rcu_read_unlock();
+    return guest_ram_size;
+}
+
+static uint64_t alloc_shadow_memory_from_dump(fast_reload_t* self,  const char* meta_file, const char* dump_file){
+    RAMBlock *block;
+    uint64_t guest_ram_size = 0;
+    rcu_read_lock();
+    uint64_t offset = 0;
+    uint32_t i = 0;
+    void* ptr = NULL;
+
+    FILE *f; 
+
+    fast_reload_dump_head_t head; 
+    fast_reload_dump_entry_t entry;
+
+    f = fopen (meta_file, "r");
+    assert(f != NULL);
+
+    assert(fread(&head, sizeof(fast_reload_dump_head_t), 1, f) == 1);
+
+    fclose(f);
+
+    self->shadow_memory_regions = head.shadow_memory_regions;
+    self->ram_region_index = head.ram_region_index;
+    self->vram_region_index = head.ram_region_index;
+
+    uint64_t total_memory_count = 0;
+    uint64_t start_region = 0;
+
+    uint64_t shadow_memory_regions_tmp = 0;
+
+    QLIST_FOREACH_RCU(block, &ram_list.blocks, next) {
+        guest_ram_size += block->used_length;
+        QEMU_PT_PRINTF(RELOAD_PREFIX, "Block: %s (%lx)", block->idstr, block->used_length);
+
+        if(!memcmp(block->idstr, "pc.ram", 6)){
+            self->ram_region_index = shadow_memory_regions_tmp;
+        }
+
+        if(!memcmp(block->idstr, "vga.vram", 8)){
+            self->vram_region_index = shadow_memory_regions_tmp;
+        }
+
+        total_memory_count += block->used_length;
+
+        if(!block->offset){
+            start_region = (uint64_t)block->host;
+        }
+
+        shadow_memory_regions_tmp++;
+    }
+
+    assert(self->shadow_memory_regions == shadow_memory_regions_tmp);
+
+    f = fopen (dump_file, "r");
+    assert(f != NULL);
+
+    fseek(f, 0L, SEEK_END);
+    assert(guest_ram_size == ftell(f));
+    fseek(f, 0L, SEEK_SET);
+
+    fclose(f);
+
+    QEMU_PT_PRINTF(RELOAD_PREFIX, "Start: %lx - %lx", start_region, start_region+total_memory_count);
+
+    int fd = open(dump_file, O_RDONLY);
+
+    ptr = mmap(0, guest_ram_size, PROT_READ, MAP_SHARED, fd, 0);
+    assert(ptr);
+    madvise(ptr, guest_ram_size, MADV_MERGEABLE);
+
+    self->shadow_memory = malloc(sizeof(void*)*self->shadow_memory_regions);
+    self->ram_block_array = malloc(sizeof(RAMBlock*)*self->shadow_memory_regions);
+
+    f = fopen (meta_file, "r");
+    fseek(f, sizeof(fast_reload_dump_head_t), SEEK_SET);
+
+
+    offset = 0;
+    QLIST_FOREACH_RCU(block, &ram_list.blocks, next) {
+        QEMU_PT_PRINTF(RELOAD_PREFIX, "%lx %lx %lx\t%s\t%p", block->offset, block->used_length, block->max_length, block->idstr, block->host);
+        self->ram_block_array[i] = block;
+        /* copy image to ram */
+        memcpy(block->host, ptr+offset,  block->used_length);
+        self->shadow_memory[i++] = ptr+offset;
+
+        assert(fread(&entry, sizeof(fast_reload_dump_entry_t), 1, f) == 1);
+
+        assert(!strcmp(entry.idstr, block->idstr));
+
+        printf("%lx vs %lx\n", entry.shadow_memory_offset, offset);
+        assert(entry.shadow_memory_offset == offset);
+
+        offset += block->used_length;
+    }   
+
+    fclose(f);
+
+    assert(start_region);
+
+    QEMU_PT_PRINTF(RELOAD_PREFIX, "Allocating Memory (%p) Size: %lx", ptr, guest_ram_size);
+
+    for(i = 0; i < self->shadow_memory_regions; i++){
+        QEMU_PT_PRINTF(RELOAD_PREFIX, "%p", self->shadow_memory[i]);
+    }
+
+    self->ptr = ptr;
+
+    shadow_memory_regions_tmp = 0;
+    QLIST_FOREACH_RCU(block, &ram_list.blocks, next) {
+        if(!block->mr->readonly){
+            self->mr_ranges[self->mr_ranges_num].host_ptr = (void*)block->host;
+            self->mr_ranges[self->mr_ranges_num].shadow_ptr = (void*)self->shadow_memory[shadow_memory_regions_tmp];
+            
+            self->mr_ranges[self->mr_ranges_num].mr_start = block->mr->addr;
+            self->mr_ranges[self->mr_ranges_num].mr_end = block->mr->addr+(uint64_t)block->mr->size;
+            self->mr_ranges[self->mr_ranges_num].offset = block->offset;
+
+            self->mr_ranges_num++;
+        }
+        shadow_memory_regions_tmp++;
+    }
+
+    rcu_read_unlock();
+    return guest_ram_size;
+}
+
+static void reset_dirty_tracking(fast_reload_t* self){
+    RAMBlock *block;
+    uint32_t i = 0;
+    QLIST_FOREACH_RCU(block, &ram_list.blocks, next) {
+        for (uint64_t addr = 0; addr < block->used_length; addr += TARGET_PAGE_SIZE) {
+            cpu_physical_memory_test_and_clear_dirty( block->offset + addr, TARGET_PAGE_SIZE, DIRTY_MEMORY_MIGRATION);
+        }
+        i++;
+    }
+}
+
+static void fdl_dirty_tracking_init(fast_reload_t* self, uint64_t ram_size){
+    uint8_t ret; 
+    CPUState* cpu = qemu_get_cpu(0);
+    kvm_cpu_synchronize_state(cpu);
+
+    assert(kvm_state);
+    if(!vmx_fdl_fd){
+        vmx_fdl_fd = kvm_vm_ioctl(kvm_state, KVM_VMX_FDL_SETUP_FD, (unsigned long)0);
+        
+        ret = ioctl(vmx_fdl_fd, KVM_VMX_FDL_SET, ram_size);
+
+        if (ret){
+            QEMU_PT_PRINTF(RELOAD_PREFIX, "Oops?! VMX_FDL not loaded? Aborting...");
+        }
+        
+        fdl_data = mmap((void*)FDL_MMAP_ADDRESS, (ram_size/0x1000)*8, PROT_READ | PROT_WRITE, MAP_SHARED | MAP_FIXED, vmx_fdl_fd, 0);
+        assert(fdl_data != (void*)0xFFFFFFFFFFFFFFFF);
+    }
+
+    ioctl(vmx_fdl_fd, KVM_VMX_FDL_FLUSH, (unsigned long)0);
+}
+
+static void fdl_write_info_file(fast_reload_t* self, const char* file, const char* misc, bool pre_image){
+
+    FILE* f = fopen(file, "w");
+
+    if(pre_image){
+        const char* msg = "THIS IS A QEMU-PT PRE IMAGE SNAPSHOT FILE!\n\n";
+        fwrite(msg, strlen(msg), 1, f);
+    }
+    else{
+        const char* msg = "THIS IS A QEMU-PT SNAPSHOT FILE!\n\n";
+        fwrite(msg, strlen(msg), 1, f);
+    }
+
+    fwrite(misc, strlen(misc), 1, f);
+    fclose(f);
+}
+
+static void fast_save_state_from_file(fast_reload_t* self, const char* qemu_state_file, bool switch_mode){
+    struct fast_savevm_opaque_t fast_savevm_opaque;
+    FILE* f;
+
+    uint8_t ret = global_state_store();
+    if (ret) {
+        return;
+    }
+
+    /* Testing Stuff */
+    struct stat buffer;   
+    assert(stat (qemu_state_file, &buffer) == 0);
+
+    void* state_buf2 = malloc(STATE_BUFFER);  
+
+    f = fopen(qemu_state_file, "r");
+    assert(fread(state_buf2, buffer.st_size, 1, f) == 1);
+    fclose(f);
+
+    fast_savevm_opaque.buf = state_buf2;
+    fast_savevm_opaque.f = NULL;//fopen("/tmp/qemu_state", "w");
+    fast_savevm_opaque.pos = 0;
+    QEMUFile* file_dump = qemu_fopen_ops(&fast_savevm_opaque, &fast_loadvm_ops);
+
+    if(!switch_mode){
+        qemu_devices_reset();
+        qemu_loadvm_state(file_dump);
+    }
+    else {
+        self->qemu_state = state_reallocation_new(file_dump);
+    }
+
+    free(state_buf2);
+}
+
+static void fast_save_state_to_file(fast_reload_t* self, const char* qemu_state_file){
+    Error *local_err = NULL;
+    struct fast_savevm_opaque_t fast_savevm_opaque;
+
+    void* state_buf = malloc(STATE_BUFFER);  
+
+    fast_savevm_opaque.buf = state_buf;
+    fast_savevm_opaque.f = fopen(qemu_state_file, "w");;
+    fast_savevm_opaque.pos = 0;
+    
+    uint8_t ret = global_state_store();
+    assert(!ret);
+
+    QEMUFile* f = qemu_fopen_ops(&fast_savevm_opaque, &fast_savevm_ops2);
+    ret = fast_qemu_savevm_state(f, &local_err);
+    QEMU_PT_PRINTF(RELOAD_PREFIX,"vm_state_size %ld %d", qemu_ftell(f), ret);
+    qemu_fclose(f);
+
+    fast_savevm_opaque.buf = state_buf;
+    fast_savevm_opaque.f = NULL;
+    fast_savevm_opaque.pos = 0;
+    QEMUFile* file_dump = qemu_fopen_ops(&fast_savevm_opaque, &fast_loadvm_ops);
+
+    qemu_mutex_lock_iothread();
+    self->qemu_state = state_reallocation_new(file_dump);
+    qemu_mutex_unlock_iothread();
+
+    free(file_dump);
+	free(state_buf);
+}
+
+static void fast_save_state_memory(fast_reload_t* self){
+    Error *local_err = NULL;
+    struct fast_savevm_opaque_t fast_savevm_opaque;
+
+    void* state_buf = malloc(STATE_BUFFER);  
+
+    fast_savevm_opaque.buf = state_buf;
+    fast_savevm_opaque.f = NULL; 
+    fast_savevm_opaque.pos = 0;
+    
+    uint8_t ret = global_state_store();
+    assert(!ret);
+
+    QEMUFile* f = qemu_fopen_ops(&fast_savevm_opaque, &fast_savevm_ops);
+    ret = fast_qemu_savevm_state(f, &local_err);
+    QEMU_PT_PRINTF(RELOAD_PREFIX,"vm_state_size %ld %d", qemu_ftell(f), ret);
+    qemu_fclose(f);
+
+    fast_savevm_opaque.buf = state_buf;
+    fast_savevm_opaque.f = NULL;
+    fast_savevm_opaque.pos = 0;
+    QEMUFile* file_dump = qemu_fopen_ops(&fast_savevm_opaque, &fast_loadvm_ops);
+
+    qemu_mutex_lock_iothread();
+    self->qemu_state = state_reallocation_new(file_dump);
+    qemu_mutex_unlock_iothread();
+
+    free(state_buf);
+}
+
+#ifdef DEBUG_FDL
+static void find_dirty_pages(fast_reload_t* self){
+    RAMBlock *block;
+    uint32_t i = 0;
+    QLIST_FOREACH_RCU(block, &ram_list.blocks, next) {
+
+
+    assert((void*)self->ram_block_array[self->ram_region_index]->host == block->host);
+
+    for(uint64_t j = 0; j < self->shadow_memory_regions; j++){
+        block = self->ram_block_array[j];
+        // optimize me (only 3k/s zzzZZzz...)
+        //if(memcmp(block->idstr, "/rom@etc/acpi/tables", strlen("/rom@etc/acpi/tables"))){
+            for (uint64_t addr = 0; addr < block->used_length; addr += TARGET_PAGE_SIZE) {
+                
+                if(memcmp(block->host+addr, self->shadow_memory[j]+addr, TARGET_PAGE_SIZE)){
+                        fprintf(stderr, "DIRTY: %ld %lx (%lx) %x %x \t%d (%s)\n", j, addr, block->host+addr, block->host, block->offset,  (cpu_physical_memory_test_and_clear_dirty( block->offset + addr, TARGET_PAGE_SIZE, DIRTY_MEMORY_MIGRATION)), block->idstr);
+                        fprintf(stderr, "COPY:  %lx %lx %lx %d\n", block->host+addr, self->shadow_memory[j]+addr, TARGET_PAGE_SIZE, j);
+                        memcpy(block->host+addr, self->shadow_memory[j]+addr, TARGET_PAGE_SIZE);
+                }
+            }
+        }
+        i++;
+        break;
+    }
+}
+#endif
+
+static void find_dirty_pages_fdl(fast_reload_t* self){
+    rcu_read_lock();
+    uint64_t index = ioctl(vmx_fdl_fd, KVM_VMX_FDL_GET_INDEX, NULL);
+    uint64_t addr = 0;
+    uint64_t offset_addr = 0;
+    for(uint64_t i = 0; i < index; i++){
+        addr = (fdl_data[i]);
+        for(uint8_t j = 0; j < self->mr_ranges_num; j++){
+            if(addr >= self->mr_ranges[j].mr_start && addr < self->mr_ranges[j].mr_end && likely(cpu_physical_memory_test_and_clear_dirty(self->mr_ranges[j].offset+(addr-self->mr_ranges[j].mr_start), TARGET_PAGE_SIZE, DIRTY_MEMORY_MIGRATION))){
+                offset_addr = addr-self->mr_ranges[j].mr_start;
+                memcpy((void*)(self->mr_ranges[j].host_ptr+offset_addr), (void*)(self->mr_ranges[j].shadow_ptr+offset_addr), TARGET_PAGE_SIZE);
+            }
+        }
+    }
+    rcu_read_unlock();
+
+}
+
+#ifdef RESET_VRAM
+static void reset_vram(fast_reload_t* self){
+    if(self->vram_region_index == 0xFFFFFFFF){
+        return;
+    }
+
+    uint64_t addr_a = (uint64_t)self->ram_block_array[self->vram_region_index]->host;
+    uint64_t addr_b = (uint64_t)self->shadow_memory[self->vram_region_index];
+    uint64_t addr_c = (uint64_t)self->ram_block_array[self->vram_region_index]->offset;
+
+    for (uint64_t addr = 0; addr < self->ram_block_array[self->vram_region_index]->used_length; addr += TARGET_PAGE_SIZE) {
+        if(cpu_physical_memory_test_and_clear_dirty(addr_c + addr, TARGET_PAGE_SIZE, DIRTY_MEMORY_MIGRATION ) ){ // || addr <= 0x1000){
+#ifdef VERBOSE_DIRTY_PAGES
+           fprintf(stderr, "VRAM DIRTY (%lx)\n", addr);
+#endif
+           memcpy((void*)(addr_a+addr), (void*)(addr_b+addr), TARGET_PAGE_SIZE);
+        }
+    }
+}
+#endif
+
+static void unset_black_list_pages(fast_reload_t* self){
+
+    uint64_t base_offset = (uint64_t)self->ram_block_array[self->ram_region_index]->offset;
+
+    for(uint32_t i = 0; i < self->black_list_pages_num; i++){
+        cpu_physical_memory_test_and_clear_dirty(base_offset+self->black_list_pages[i], TARGET_PAGE_SIZE, DIRTY_MEMORY_MIGRATION);
+    }
+}
+
+static void fast_reload_qemu_user_fdl_enable(fast_reload_t* self){
+    if(self){
+        self->qemu_user_fdl.enabled = true;
+    }
+}
+
+static void fast_reload_qemu_user_fdl_reset(fast_reload_t* self){
+    if(self){
+        self->qemu_user_fdl.pos = 0;
+    }
+}
+
+static void fast_reload_qemu_user_fdl_restore(fast_reload_t* self){
+    uint64_t addr_a = (uint64_t)self->ram_block_array[self->ram_region_index]->host;
+    uint64_t addr_b = (uint64_t)self->shadow_memory[self->ram_region_index];
+    uint64_t addr_c = (uint64_t)self->ram_block_array[self->ram_region_index]->offset;
+
+    for(uint64_t i = 0; i < self->qemu_user_fdl.pos; i++){
+        uint64_t addr = self->qemu_user_fdl.data[i];
+        cpu_physical_memory_test_and_clear_dirty(addr_c + addr, TARGET_PAGE_SIZE, DIRTY_MEMORY_MIGRATION );
+        memcpy((void*)(addr_a+addr), (void*)(addr_b+addr), TARGET_PAGE_SIZE);
+#ifdef VERBOSE_DIRTY_PAGES
+        fprintf(stderr, "QEMU_USER_FDL DIRTY (%lx)\n", addr);
+#endif
+
+    }
+    fast_reload_qemu_user_fdl_reset(self);
+}
+
+void fast_reload_qemu_user_fdl_set_dirty(fast_reload_t* self, MemoryRegion *mr, uint64_t addr, uint64_t length){
+    if(self && self->qemu_user_fdl.enabled && length >= 0x1000){
+        for(uint64_t offset = 0; offset < length; offset+=0x1000){
+            if (!cpu_physical_memory_test_dirty((addr+offset) & 0xFFFFFFFFFFFFF000, 0x1000, DIRTY_MEMORY_MIGRATION)){
+                self->qemu_user_fdl.data[self->qemu_user_fdl.pos++] = (addr+offset) & 0xFFFFFFFFFFFFF000;
+                assert(self->qemu_user_fdl.pos < USER_FDL_SLOTS); /* fix this later */
+#ifdef VERBOSE_QEMU_USER_SET_DIRTY
+                fprintf(stderr, "%s: %lx %lx\n", __func__, (mr->addr+addr+offset) & 0xFFFFFFFFFFFFF000, 0x1000);
+#endif
+            }
+        }
+    }
+}
+
+fast_reload_t* fast_reload_new(void){
+	fast_reload_t* self = malloc(sizeof(fast_reload_t));
+    memset(self, 0x0, sizeof(fast_reload_t));
+
+	self->black_list_pages_num = 0;
+	self->black_list_pages_size = 0;
+	self->black_list_pages = malloc(sizeof(uint64_t) * REALLOC_SIZE);
+    self->guest_ram_size = 0;
+
+
+    self->qemu_user_fdl.enabled = false;
+    self->qemu_user_fdl.pos = 0;
+    self->qemu_user_fdl.data = malloc(sizeof(uint64_t) * USER_FDL_SLOTS);
+    self->qemu_user_fdl.size = 4096;
+
+	self->qemu_state = NULL;
+
+	self->vmx_fdl_fd = 0;
+	self->fdl_data = NULL;
+
+	self->shadow_memory_regions = 0;
+	self->ram_region_index = 0;
+    self->vram_region_index = 0xFFFFFFFF;
+	self->shadow_memory = NULL;
+	self->ram_block_array = NULL;
+
+    self->cow_cache_array = NULL;
+    self->cow_cache_array_size = 0;
+
+	return self;
+}
+
+void fast_reload_destroy(fast_reload_t* self){
+
+    munmap(self->ptr, self->guest_ram_size);
+
+    free(self->black_list_pages);
+
+    free(self);
+}
+
+static void fast_reload_prepare_block_cow_from_file(fast_reload_t* self, const char* folder, bool switch_mode){
+    BlockBackend *blk;
+    fast_reload_cow_entry_t entry;
+
+    char* tmp1;
+    char* tmp2;
+
+    assert(asprintf(&tmp1, "%s/fs_cache.meta", folder) != -1);
+    assert(asprintf(&tmp2, "%s/fs_drv", folder) != -1);
+
+
+    self->cow_cache_array_size = 0;
+
+    FILE* f = fopen (tmp1, "r");
+    assert(f != NULL);
+
+    for (blk = blk_next(NULL); blk; blk = blk_next(blk)) {
+        if(blk && blk->cow_cache){
+            printf("%p %s\n", blk->cow_cache, blk->cow_cache->filename);
+            self->cow_cache_array_size++;
+        }
+    }
+
+    uint32_t temp_cow_cache_array_size; 
+
+    assert(fread(&temp_cow_cache_array_size, sizeof(uint32_t), 1, f) == 1);
+
+    printf("%d vs %x\n", temp_cow_cache_array_size, self->cow_cache_array_size);
+    assert(self->cow_cache_array_size == temp_cow_cache_array_size);
+
+    self->cow_cache_array = (cow_cache_t**)malloc(sizeof(cow_cache_t*)*self->cow_cache_array_size);
+
+    uint32_t i = 0;
+    uint32_t id = 0;
+    for (blk = blk_next(NULL); blk; blk = blk_next(blk)) {
+        if(blk && blk->cow_cache){
+            self->cow_cache_array[i++] = blk->cow_cache;
+            assert(fread(&entry, sizeof(fast_reload_cow_entry_t), 1, f) == 1);
+
+            assert(!strcmp(entry.idstr, blk->cow_cache->filename));
+            assert(entry.id == id);
+        }
+        id++;
+    }
+
+
+    fclose(f);
+
+    for(i = 0; i < self->cow_cache_array_size; i++){
+        read_primary_buffer(self->cow_cache_array[i], tmp2, switch_mode);
+    }
+
+    free(tmp1);
+    free(tmp2);
+}
+
+static void fast_reload_prepare_block_cow_to_file(fast_reload_t* self, const char* folder){
+    BlockBackend *blk;
+    fast_reload_cow_entry_t entry;
+
+    char* tmp1;
+    char* tmp2;
+
+    assert(asprintf(&tmp1, "%s/fs_cache.meta", folder) != -1);
+    assert(asprintf(&tmp2, "%s/fs_drv", folder) != -1);
+
+    self->cow_cache_array_size = 0;
+
+
+    FILE* f = fopen (tmp1, "w");
+    assert(f != NULL);
+
+    for (blk = blk_next(NULL); blk; blk = blk_next(blk)) {
+        if(blk && blk->cow_cache){
+            printf("%p %s\n", blk->cow_cache, blk->cow_cache->filename);
+            self->cow_cache_array_size++;
+        }
+    }
+
+    fwrite(&(self->cow_cache_array_size), sizeof(uint32_t), 1, f);
+
+    self->cow_cache_array = (cow_cache_t**)malloc(sizeof(cow_cache_t*)*self->cow_cache_array_size);
+
+    uint32_t i = 0;
+    uint32_t id = 0;
+    for (blk = blk_next(NULL); blk; blk = blk_next(blk)) {
+        if(blk && blk->cow_cache){
+            self->cow_cache_array[i++] = blk->cow_cache;
+
+            entry.id = id;
+            strncpy((char*)&entry.idstr, (const char*)blk->cow_cache->filename, 256);
+            fwrite(&entry, sizeof(fast_reload_cow_entry_t), 1, f);
+        }
+        id++;
+    }
+
+
+    fclose(f);
+
+    for(i = 0; i < self->cow_cache_array_size; i++){
+        switch_to_fuzz_mode(self->cow_cache_array[i]);
+        dump_primary_buffer(self->cow_cache_array[i], tmp2);
+    }
+
+    free(tmp1);
+    free(tmp2);
+}
+
+static void fast_reload_prepare_block_cow(fast_reload_t* self){
+
+    BlockBackend *blk;
+    for (blk = blk_next(NULL); blk; blk = blk_next(blk)) {
+        if(blk && blk->cow_cache){
+            printf("%p %s\n", blk->cow_cache, blk->cow_cache->filename);
+            self->cow_cache_array_size++;
+        }
+    }
+
+    self->cow_cache_array = (cow_cache_t**)malloc(sizeof(cow_cache_t*)*self->cow_cache_array_size);
+
+    uint32_t i = 0;
+    for (blk = blk_next(NULL); blk; blk = blk_next(blk)) {
+        if(blk && blk->cow_cache){
+            self->cow_cache_array[i++] = blk->cow_cache;
+        }
+    }
+
+
+    for(i = 0; i < self->cow_cache_array_size; i++){
+        switch_to_fuzz_mode(self->cow_cache_array[i]);
+    }
+}
+
+static void dump_to_file(fast_reload_t* self, uint64_t ram_size, const char* meta_file, const char* dump_file){
+    printf("black_list_pages_num: %lx\n", self->black_list_pages_num);
+    printf("black_list_pages_size: %lx\n", self->black_list_pages_size);
+    printf("black_list_pages ...\n");
+    for (uint64_t i = 0; i < self->black_list_pages_num; i++ ){
+        printf("self->black_list_pages[%ld] = %lx\n", i, self->black_list_pages[i]);
+    }
+
+    printf("shadow_memory_regions: %d\n", self->shadow_memory_regions);
+    printf("ram_region_index: %d\n", self->ram_region_index);
+
+    for (uint32_t i = 0; i < self->shadow_memory_regions; i++){
+        printf("self->shadow_memory[%d] = %lx %s\n", i, ((uint64_t)self->shadow_memory[i])-((uint64_t)self->ptr), self->ram_block_array[i]->idstr);
+    }
+
+    printf("ram_size: %lx\n", ram_size);
+
+    fast_reload_dump_head_t head; 
+    fast_reload_dump_entry_t entry;
+
+    FILE *f; 
+
+    printf("%s\n", meta_file);
+    f = fopen (meta_file, "w");
+    assert(f != NULL);
+
+    head.shadow_memory_regions = self->shadow_memory_regions;
+    head.ram_region_index = self->ram_region_index; 
+
+    fwrite(&head, sizeof(fast_reload_dump_head_t), 1, f);
+
+    for (uint64_t i = 0; i < self->shadow_memory_regions; i++){
+        memset(&entry, 0x0, sizeof(fast_reload_dump_entry_t));
+        entry.shadow_memory_offset = ((uint64_t)self->shadow_memory[i])-((uint64_t)self->ptr);
+        strncpy((char*)&entry.idstr, (const char*)self->ram_block_array[i]->idstr, 256);
+        fwrite(&entry, sizeof(fast_reload_dump_entry_t), 1, f);
+    }
+
+    fclose(f);
+
+    printf("%s\n", dump_file);
+
+    f = fopen (dump_file, "w");
+    assert(f != NULL);
+
+    fwrite(self->ptr, ram_size, 1, f);
+
+    fclose(f);
+
+    printf("FILE WRITTEN: %s\n", dump_file);
+
+}
+
+static bool folder_exits(const char* path){
+    struct stat sb;
+    return (stat(path, &sb) == 0 && S_ISDIR(sb.st_mode));
+}
+
+inline static void unlock_snapshot(const char* folder){
+    char* lock_file;
+
+    assert(asprintf(&lock_file, "%s/ready.lock", folder) != -1);
+
+    int fd = open(lock_file, O_WRONLY | O_CREAT, S_IRWXU);
+    close(fd);
+
+    free(lock_file);
+}
+
+void fast_reload_create_to_file(fast_reload_t* self, const char* folder, bool lock_iothread){
+    printf("----------- %s ----------\n", __func__);
+    assert(self != NULL);
+    QEMU_PT_PRINTF(RELOAD_PREFIX,"=> CREATING FAST RELOAD SNAPSHOT FROM CURRENT STATE (dumping to: %s)", folder);
+
+    rcu_read_lock();
+        
+    bdrv_drain_all();
+    bdrv_flush_all();
+
+    char* tmp1;
+    char* tmp2;
+    char* tmp3;
+    char* tmp4;
+
+    assert(asprintf(&tmp1, "%s/fast_snapshot.qemu_state", folder) != -1);
+    assert(asprintf(&tmp2, "%s/fast_snapshot.mem_meta", folder) != -1);
+    assert(asprintf(&tmp3, "%s/fast_snapshot.mem_dump", folder) != -1);
+    assert(asprintf(&tmp4, "%s/INFO.txt", folder) != -1);
+
+
+    if(!folder_exits(folder)){
+        QEMU_PT_PRINTF(RELOAD_PREFIX,"Folder %s does not exist...failed!", folder);
+        assert(0);
+    }
+
+    uint64_t ram_size = alloc_shadow_memory(self);
+    
+    self->guest_ram_size = ram_size;
+
+    dump_to_file(self, ram_size, tmp2, tmp3);
+
+    free_shadow_memory(self);
+
+    ram_size = alloc_shadow_memory_from_dump(self, tmp2, tmp3);
+
+    self->guest_ram_size = ram_size;
+
+    fdl_dirty_tracking_init(self, ram_size);
+    fast_reload_qemu_user_fdl_enable(self);
+
+    fast_save_state_to_file(self, tmp1);
+    fast_reload_prepare_block_cow_to_file(self, folder);
+
+    fdl_write_info_file(self, tmp4, "-", GET_GLOBAL_STATE()->fast_reload_pre_image);
+
+    qemu_mutex_lock_iothread();
+    reset_dirty_tracking(self);
+    memory_global_dirty_log_start();
+    qemu_mutex_unlock_iothread();
+
+    dump_global_state(folder);
+
+    free(tmp4);
+    free(tmp3);
+    free(tmp2);
+    free(tmp1);
+
+    rcu_read_unlock();
+    // smp cpu kick in
+    cpu_synchronize_all_post_init();
+
+
+    vm_start();
+
+    qemu_cpu_kick(qemu_get_cpu(0));
+    
+    qemu_mutex_lock_iothread();
+    fast_reload_restore(self);
+    qemu_mutex_unlock_iothread();
+
+    unlock_snapshot(folder);
+}
+
+inline static void wait_for_snapshot(const char* folder){
+    char* lock_file;
+
+    assert(asprintf(&lock_file, "%s/ready.lock", folder) != -1);
+
+    while( access(lock_file, F_OK ) == -1 ) {
+        sleep(1);
+
+    }
+    free(lock_file);
+}
+
+
+static void fast_reload_create_from_file_snapshot(fast_reload_t* self, const char* folder, bool lock_iothread, bool switch_mode){
+    printf("----------- %s ----------\n", __func__);
+    assert(self != NULL);
+    QEMU_PT_PRINTF(RELOAD_PREFIX,"=> CREATING FAST RELOAD SNAPSHOT FROM DUMP (located in: %s)", folder);
+
+    rcu_read_lock();
+
+    vm_stop(RUN_STATE_SAVE_VM);
+    wait_for_snapshot(folder);
+
+    cpu_synchronize_all_pre_loadvm();
+
+    bdrv_drain_all();
+    bdrv_flush_all();
+
+    char* tmp1;
+    char* tmp2;
+    char* tmp3;
+
+    assert(asprintf(&tmp1, "%s/fast_snapshot.qemu_state", folder) != -1);
+    assert(asprintf(&tmp2, "%s/fast_snapshot.mem_meta", folder) != -1);
+    assert(asprintf(&tmp3, "%s/fast_snapshot.mem_dump", folder) != -1);
+
+    if(!folder_exits(folder)){
+        QEMU_PT_PRINTF(RELOAD_PREFIX,"Folder %s does not exist...failed!", folder);
+        assert(0);
+    }
+
+    uint64_t ram_size = alloc_shadow_memory_from_dump(self, tmp2, tmp3);
+
+    self->guest_ram_size = ram_size;
+
+    if(switch_mode){
+        fdl_dirty_tracking_init(self, ram_size);
+        fast_reload_qemu_user_fdl_enable(self);
+    }
+
+    fast_save_state_from_file(self, tmp1, switch_mode);
+
+    fast_reload_prepare_block_cow_from_file(self, folder, switch_mode);
+
+    if(switch_mode){
+        if (lock_iothread){
+            qemu_mutex_lock_iothread();
+        }
+    }
+
+    if(switch_mode){
+        reset_dirty_tracking(self);
+        memory_global_dirty_log_start();
+    }
+
+    if (lock_iothread){
+        qemu_mutex_unlock_iothread();
+    }
+
+    if(switch_mode){
+        load_global_state(folder);
+    }
+
+    free(tmp3);
+    free(tmp2);
+    free(tmp1);
+
+    rcu_read_unlock();
+    // smp cpu kick in
+    cpu_synchronize_all_post_init();
+
+    vm_start();
+    sleep(1); /* temporary race condition fix */
+    return;
+}
+
+void fast_reload_create_from_file(fast_reload_t* self, const char* folder, bool lock_iothread){
+    fast_reload_create_from_file_snapshot(self, folder, lock_iothread, true);
+}
+
+void fast_reload_create_from_file_pre_image(fast_reload_t* self, const char* folder, bool lock_iothread){
+    fast_reload_create_from_file_snapshot(self, folder, lock_iothread, false);
+}
+
+void fast_reload_create_in_memory(fast_reload_t* self, bool lock_iothread){
+
+    assert(self != NULL);
+    fprintf(stderr, "===>%s\n", __func__);
+    QEMU_PT_PRINTF(RELOAD_PREFIX,"=> CREATING FAST RELOAD SNAPSHOT FROM CURRENT VM STATE");
+
+    rcu_read_lock();
+
+    bdrv_drain_all();
+    bdrv_flush_all();
+
+    cpu_synchronize_all_pre_loadvm();
+
+
+    uint64_t ram_size = alloc_shadow_memory(self);
+    self->guest_ram_size = ram_size;
+
+    fdl_dirty_tracking_init(self, ram_size);
+    fast_reload_qemu_user_fdl_enable(self);
+
+    fast_save_state_memory(self);
+    fast_reload_prepare_block_cow(self);
+
+    if (lock_iothread){
+        qemu_mutex_lock_iothread();
+    }
+
+    reset_dirty_tracking(self);
+    memory_global_dirty_log_start();
+
+    if (lock_iothread){
+        qemu_mutex_unlock_iothread();
+    }
+
+    rcu_read_unlock();
+    cpu_synchronize_all_post_init();
+
+    vm_start();
+    
+    if (lock_iothread){
+        qemu_mutex_lock_iothread();
+    }
+    fast_reload_restore(self);
+    if (lock_iothread){
+        qemu_mutex_unlock_iothread();
+    }
+}
+
+void fast_reload_restore(fast_reload_t* self){    
+
+    static struct kvm_clock_data data = {.clock = 0, .flags = 0};
+
+	assert(self != NULL);
+
+    vm_stop(RUN_STATE_RESTORE_VM);
+    
+    rcu_read_lock();
+
+    cpu_synchronize_all_states();
+
+    bdrv_drain_all_begin();
+
+    memory_global_dirty_log_sync();
+
+    smp_wmb();
+
+    unset_black_list_pages(self);
+
+    find_dirty_pages_fdl(self);
+    fast_reload_qemu_user_fdl_restore(self);
+#ifdef RESET_VRAM
+    reset_vram(self);
+#endif
+#ifdef DEBUG_FDL
+    find_dirty_pages(self);
+#endif
+    
+    for(uint32_t i = 0; i < self->cow_cache_array_size; i++){
+        cow_cache_reset(self->cow_cache_array[i]);
+    }
+
+    fdl_fast_reload(self->qemu_state);
+
+
+    GET_GLOBAL_STATE()->cow_cache_full = false;
+    qemu_get_cpu(0)->halted = false;
+
+    kvm_vm_ioctl(kvm_state, KVM_SET_CLOCK, &data);
+
+    kvm_arch_put_registers(qemu_get_cpu(0), KVM_PUT_FULL_STATE_FAST);
+    qemu_get_cpu(0)->vcpu_dirty = false;
+
+    bdrv_drain_all_end();
+    rcu_read_unlock();
+
+    vm_start();
+    qemu_clock_enable(QEMU_CLOCK_VIRTUAL, true);
+    cpu_resume(qemu_get_cpu(0));    
+
+}
+
+bool read_snapshot_memory(fast_reload_t* self, uint64_t address, void* ptr, size_t size){
+    if(address == 0xFFFFFFFFFFFFFFFF){
+        return false;
+    }
+    uint64_t snapshot_memory_offset = (uint64_t)self->shadow_memory[self->ram_region_index];
+
+    uint64_t max_size = (uint64_t)self->shadow_memory[self->ram_region_index+1] - (uint64_t)self->shadow_memory[self->ram_region_index];
+
+    if(address+size > max_size){
+        memset(ptr, 0x0, size);
+        fprintf(stderr, "FAILED\t=> READING MEMORY FROM SNAPSHOT (at %lx) MAX = %lx\n", address, max_size);
+
+        return false;
+    }
+      
+    memcpy(ptr, (void*)(snapshot_memory_offset+address), size);
+    return true;
+}
+
+void* fast_reload_get_physmem_shadow_ptr(fast_reload_t* self, uint64_t physaddr){
+	assert(self != NULL);
+    assert(!(physaddr&0xFFF));  /* physaddr must be 4kb align ! */
+    if (self->shadow_memory_regions){
+        for(uint64_t j = 0; j < self->shadow_memory_regions; j++){
+            if(physaddr >= self->ram_block_array[j]->offset && physaddr < (self->ram_block_array[j]->offset+self->ram_block_array[j]->used_length)){
+                return self->shadow_memory[j]+(physaddr-self->ram_block_array[j]->offset);
+            } 
+        }
+    }
+    return NULL;
+}
+
+void fast_reload_blacklist_page(fast_reload_t* self, uint64_t physaddr){
+    //printf("--> %lx\n", physaddr);
+	assert(self != NULL);
+
+	if(self->black_list_pages_num <= self->black_list_pages_size){
+    	self->black_list_pages_size += REALLOC_SIZE;
+    	self->black_list_pages = realloc(self->black_list_pages, sizeof(uint64_t) * self->black_list_pages_size);
+    }
+
+    self->black_list_pages[self->black_list_pages_num] = physaddr;
+    self->black_list_pages_num++;
+
+}
+
+bool fast_reload_snapshot_exists(fast_reload_t* self){
+	if(!self || !self->qemu_state){
+		return false;
+	}
+	return true;
+}
+
+      
diff --new-file -ur qemu/pt/fast_vm_reload.h QEMU-PT/pt/fast_vm_reload.h
--- qemu/pt/fast_vm_reload.h	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/fast_vm_reload.h	2021-08-24 21:54:55.938586449 +0200
@@ -0,0 +1,115 @@
+/*
+
+Copyright (C) 2017 Sergej Schumilo
+
+This file is part of QEMU-PT (HyperTrash / kAFL).
+
+QEMU-PT is free software: you can redistribute it and/or modify
+it under the terms of the GNU General Public License as published by
+the Free Software Foundation, either version 2 of the License, or
+(at your option) any later version.
+
+QEMU-PT is distributed in the hope that it will be useful,
+but WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with QEMU-PT.  If not, see <http://www.gnu.org/licenses/>.
+
+*/
+
+#ifndef FAST_VM_RELOAD_H
+#define FAST_VM_RELOAD_H
+
+#include "qemu/osdep.h"
+#include "monitor/monitor.h"
+#include "qemu-common.h"
+#include "pt/state_reallocation.h"
+#include "pt/block_cow.h"
+
+
+typedef struct fast_reload_dump_head_s{
+    uint32_t shadow_memory_regions; 
+	uint32_t ram_region_index;
+} fast_reload_dump_head_t; 
+
+typedef struct fast_reload_dump_entry_s{
+	uint64_t shadow_memory_offset; 
+    char idstr[256];
+} fast_reload_dump_entry_t; 
+
+typedef struct fast_reload_cow_entry_s{
+	uint32_t id; 
+    char idstr[256];
+} fast_reload_cow_entry_t; 
+
+
+typedef struct qemu_user_fdl_s{
+	bool enabled;
+	uint64_t pos;
+	uint64_t* data; 
+	uint64_t size;
+} qemu_user_fdl_t; 
+
+typedef struct mr_range_s{
+	uint64_t mr_start;
+	uint64_t mr_end;
+	void* host_ptr;
+	void* shadow_ptr;
+	uint64_t offset; 
+} mr_range_t;
+
+typedef struct fast_reload_s{
+	state_reallocation_t* qemu_state;
+
+	int vmx_fdl_fd;
+	uint64_t* fdl_data;
+
+	qemu_user_fdl_t qemu_user_fdl;
+
+	uint64_t black_list_pages_num;
+	uint64_t black_list_pages_size;
+	uint64_t* black_list_pages;
+
+	void** shadow_memory;
+	RAMBlock** ram_block_array; 
+	uint32_t shadow_memory_regions; 
+	uint32_t ram_region_index;
+	uint32_t vram_region_index;
+
+	mr_range_t mr_ranges[10];
+	uint8_t mr_ranges_num; 
+
+	void* ptr;
+
+	cow_cache_t **cow_cache_array;
+	uint32_t cow_cache_array_size;
+
+	uint64_t guest_ram_size;
+
+
+} fast_reload_t;
+
+
+fast_reload_t* fast_reload_new(void);
+
+
+void fast_reload_create_to_file(fast_reload_t* self, const char* folder, bool lock_iothread);
+void fast_reload_create_from_file(fast_reload_t* self, const char* folder, bool lock_iothread);
+void fast_reload_create_from_file_pre_image(fast_reload_t* self, const char* folder, bool lock_iothread);
+void fast_reload_create_in_memory(fast_reload_t* self, bool lock_iothread);
+
+
+void fast_reload_restore(fast_reload_t* self);
+void fast_reload_blacklist_page(fast_reload_t* self, uint64_t physaddr);
+void* fast_reload_get_physmem_shadow_ptr(fast_reload_t* self, uint64_t physaddr);
+bool fast_reload_snapshot_exists(fast_reload_t* self);
+
+bool read_snapshot_memory(fast_reload_t* self, uint64_t address, void* ptr, size_t size);
+
+void fast_reload_destroy(fast_reload_t* self);
+
+void fast_reload_qemu_user_fdl_set_dirty(fast_reload_t* self, MemoryRegion *mr, uint64_t addr, uint64_t length);
+
+#endif
\ No newline at end of file
diff --new-file -ur qemu/pt/file_helper.c QEMU-PT/pt/file_helper.c
--- qemu/pt/file_helper.c	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/file_helper.c	2021-08-24 21:54:55.938586449 +0200
@@ -0,0 +1,141 @@
+#include <assert.h>
+#include <string.h>
+
+#include <sys/stat.h> 
+#include <fcntl.h>
+#include <unistd.h>
+#include <errno.h>
+
+#include "redqueen.h"
+#include "debug.h"
+#include "file_helper.h"
+
+
+///////////////////////////////////////////////////////////////////////////////////
+// Private Helper Functions Declarations
+///////////////////////////////////////////////////////////////////////////////////
+
+size_t _count_lines_in_file(FILE* fp);
+
+void _parse_addresses_in_file(FILE* fp, size_t num_addrs, uint64_t* addrs);
+
+///////////////////////////////////////////////////////////////////////////////////
+// Public Functions
+///////////////////////////////////////////////////////////////////////////////////
+
+void write_debug_result(char* buf){
+  int unused __attribute__((unused));
+	int fd = open("/tmp/qemu_debug.txt", O_WRONLY | O_CREAT | O_APPEND, S_IRWXU);
+  assert(fd > 0);
+	unused = write(fd, buf, strlen(buf));
+  close(fd);
+}
+
+void parse_address_file(char* path, size_t* num_addrs, uint64_t** addrs){
+  FILE* fp = fopen(path,"r");
+  if(!fp){
+    *num_addrs = 0;
+    *addrs = NULL;
+    return;
+  }
+
+  *num_addrs = _count_lines_in_file(fp);
+  if(*num_addrs == 0){
+    *addrs = NULL;
+    goto exit_function;
+  }
+
+  assert(*num_addrs < 0xffff);
+  *addrs = malloc(sizeof(uint64_t)*(*num_addrs));
+  _parse_addresses_in_file(fp, *num_addrs, *addrs);
+
+  exit_function:
+  fclose(fp);
+}
+
+
+int re_fd = 0;
+int se_fd = 0;
+int trace_fd = 0;
+
+void write_re_result(char* buf){
+  int unused __attribute__((unused));
+	if (!re_fd)
+	  re_fd = open(redqueen_workdir.redqueen_results, O_WRONLY | O_CREAT | O_APPEND, S_IRWXU);
+	unused = write(re_fd, buf, strlen(buf));
+}
+
+void write_trace_result(redqueen_trace_t* trace_state){
+	//int fd;
+  int unused __attribute__((unused));
+	if (!trace_fd)
+		trace_fd = open(redqueen_workdir.pt_trace_results, O_WRONLY | O_CREAT | O_APPEND, S_IRWXU);
+  redqueen_trace_write_file(trace_state, trace_fd);
+  //unused = write(trace_fd, buf, strlen(buf));
+	//close(fd);
+}
+
+void fsync_all_traces(void){
+  if (!trace_fd){
+    fsync(trace_fd);
+  }
+  if (!se_fd){
+    fsync(se_fd);
+  }
+  if (!re_fd){
+    fsync(re_fd);
+  }
+}
+
+void write_se_result(char* buf){
+	//int fd;
+  int unused __attribute__((unused));
+	if (!se_fd)
+		se_fd = open(redqueen_workdir.symbolic_results, O_WRONLY | O_CREAT | O_APPEND, S_IRWXU);
+	unused = write(se_fd, buf, strlen(buf));
+	//close(fd);
+}
+
+void delete_trace_files(void){
+  int unused __attribute__((unused));
+	if (!trace_fd)
+		trace_fd = open(redqueen_workdir.pt_trace_results, O_WRONLY | O_CREAT | O_APPEND, S_IRWXU);
+	unused = ftruncate(trace_fd, 0);
+}
+
+void delete_redqueen_files(void){
+  int unused __attribute__((unused));
+	if (!re_fd)
+		re_fd = open(redqueen_workdir.redqueen_results, O_WRONLY | O_CREAT | O_APPEND, S_IRWXU);
+	if (!se_fd)
+		se_fd = open(redqueen_workdir.symbolic_results, O_WRONLY | O_CREAT | O_APPEND, S_IRWXU);
+	unused = ftruncate(re_fd, 0);
+	unused = ftruncate(se_fd, 0);
+}
+
+///////////////////////////////////////////////////////////////////////////////////
+// Private Helper Functions Definitions
+///////////////////////////////////////////////////////////////////////////////////
+
+size_t _count_lines_in_file(FILE* fp){
+  size_t val = 0;
+  size_t count = 0;
+  while(1){
+    int scanres = fscanf(fp, "%lx", &val);
+    if(scanres == 0){
+      printf("WARNING, invalid line in address file");
+      assert(scanres != 0);
+    }
+    if(scanres == -1){break;}
+    count+=1;
+  }
+  rewind(fp);
+  return count;
+}
+
+void _parse_addresses_in_file(FILE* fp, size_t num_addrs, uint64_t* addrs){
+  for(size_t i = 0; i < num_addrs; i++){
+    assert(fscanf(fp, "%lx", &addrs[i]) == 1);
+  }
+}
+
diff --new-file -ur qemu/pt/file_helper.h QEMU-PT/pt/file_helper.h
--- qemu/pt/file_helper.h	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/file_helper.h	2021-08-24 21:54:55.938586449 +0200
@@ -0,0 +1,25 @@
+#include <stdio.h>
+#include <stdint.h>
+#include <stdlib.h>
+#include "redqueen_trace.h"
+
+//doesn't take ownership of path, num_addrs or addrs
+void parse_address_file(char* path, size_t* num_addrs, uint64_t** addrs);
+
+//doesn't take ownership of buf
+void write_re_result(char* buf);
+
+//doesn't take ownership of buf
+void write_se_result(char* buf);
+
+//doesn't take ownership of buf
+void write_trace_result(redqueen_trace_t* trace_state);
+
+//doesn' take ownership of buf
+void write_debug_result(char* buf);
+
+void delete_redqueen_files(void);
+
+void delete_trace_files(void);
+
+void fsync_all_traces(void);
diff --new-file -ur qemu/pt/helpers.c QEMU-PT/pt/helpers.c
--- qemu/pt/helpers.c	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/helpers.c	2021-08-24 21:54:55.938586449 +0200
@@ -0,0 +1,20 @@
+#include <stdio.h>
+#include <stdint.h>
+#include "pt/helpers.h"
+#include "qemu/osdep.h"
+#include <linux/kvm.h>
+#include <sys/ioctl.h>
+#include <sys/mman.h>
+#include "qemu-common.h"
+#include "exec/memory.h"
+#include "qemu/main-loop.h"
+#include "sysemu/kvm_int.h"
+#include "sysemu/kvm.h"
+
+uint64_t get_rip(CPUState *cpu){
+	kvm_arch_get_registers(cpu);
+	X86CPU *x86_cpu = X86_CPU(cpu);
+	CPUX86State *env = &x86_cpu->env;
+	kvm_cpu_synchronize_state(cpu);
+	return env->eip;
+}
\ No newline at end of file
diff --new-file -ur qemu/pt/helpers.h QEMU-PT/pt/helpers.h
--- qemu/pt/helpers.h	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/helpers.h	2021-08-24 21:54:55.942586446 +0200
@@ -0,0 +1,5 @@
+#pragma once 
+
+#include "qemu/osdep.h"
+
+uint64_t get_rip(CPUState *cpu);
\ No newline at end of file
diff --new-file -ur qemu/pt/hypercall.c QEMU-PT/pt/hypercall.c
--- qemu/pt/hypercall.c	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/hypercall.c	2021-08-24 21:54:55.938586449 +0200
@@ -0,0 +1,637 @@
+/*
+
+Copyright (C) 2017 Sergej Schumilo
+
+This file is part of QEMU-PT (kAFL).
+
+QEMU-PT is free software: you can redistribute it and/or modify
+it under the terms of the GNU General Public License as published by
+the Free Software Foundation, either version 2 of the License, or
+(at your option) any later version.
+
+QEMU-PT is distributed in the hope that it will be useful,
+but WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with QEMU-PT.  If not, see <http://www.gnu.org/licenses/>.
+
+*/
+
+#include "qemu/osdep.h"
+#include <linux/kvm.h>
+#include <sys/ioctl.h>
+#include <sys/mman.h>
+#include "qemu-common.h"
+#include "exec/memory.h"
+#include "qemu/main-loop.h"
+#include "sysemu/kvm_int.h"
+#include "sysemu/kvm.h"
+#include "pt.h"
+#include "pt/hypercall.h"
+#include "pt/memory_access.h"
+#include "pt/interface.h"
+#include "pt/debug.h"
+#include "pt/synchronization.h"
+#include "pt/fast_vm_reload.h"
+#include "pt/kvm_nested.h"
+#include "pt/state.h"
+#include "pt/trace_cache.h"
+#include "sysemu/runstate.h"
+#include "pt/helpers.h"
+#include "pt/decoder.h"
+
+
+#ifdef CONFIG_REDQUEEN
+#include "pt/redqueen.h"
+#endif
+
+bool reload_mode_temp = false;
+bool notifiers_enabled = false;
+
+bool hypercall_enabled = false;
+void* program_buffer = NULL;
+char info_buffer[INFO_SIZE];
+char hprintf_buffer[HPRINTF_SIZE];
+
+static bool init_state = true;
+
+bool pt_hypercalls_enabled(void){
+	return hypercall_enabled;
+}
+
+void pt_setup_enable_hypercalls(void){
+	hypercall_enabled = true;
+}
+
+void pt_setup_ip_filters(uint8_t filter_id, uint64_t start, uint64_t end){
+	fprintf(stderr, "--> %s\n", __func__);
+	if (filter_id < INTEL_PT_MAX_RANGES){
+
+		GET_GLOBAL_STATE()->pt_ip_filter_configured[filter_id] = true;
+		GET_GLOBAL_STATE()->pt_ip_filter_a[filter_id] = start;
+		GET_GLOBAL_STATE()->pt_ip_filter_b[filter_id] = end;
+
+	}
+}
+
+void hypercall_commit_filter(void){
+}
+
+bool setup_snapshot_once = false;
+
+
+void pt_setup_program(void* ptr){
+	program_buffer = ptr;
+}
+
+//extern bool sigprof_enabled;
+
+void create_fast_snapshot(CPUState *cpu, bool nested){
+
+	if(!nested){
+		/* decrease RIP value by vmcall instruction size */
+		X86CPU *x86_cpu = X86_CPU(cpu);
+		CPUX86State *env = &x86_cpu->env;
+		kvm_arch_get_registers(cpu);
+		kvm_cpu_synchronize_state(cpu);
+		env->eip -= 3; /* vmcall size */
+		kvm_arch_put_registers(cpu, KVM_PUT_FULL_STATE);
+	}
+	else{
+		kvm_arch_get_registers(cpu);
+		X86CPU *x86_cpu = X86_CPU(cpu);
+	  CPUX86State *env = &x86_cpu->env;
+	  env->eip -= 3; 
+		set_nested_rip(cpu, env->eip);
+		kvm_arch_put_registers(cpu, KVM_PUT_FULL_STATE);
+	}
+
+	/* create now fastreload snapshot */
+	if (GET_GLOBAL_STATE()->fast_reload_enabled){
+		printf("===> GET_GLOBAL_STATE()->fast_reload_enabled: TRUE\n");
+		if (GET_GLOBAL_STATE()->fast_reload_mode){
+			printf("===> GET_GLOBAL_STATE()->fast_reload_mode: TRUE\n");
+			qemu_mutex_lock_iothread();
+			fast_reload_restore(get_fast_reload_snapshot());
+			qemu_mutex_unlock_iothread();
+			//fast_reload_create_from_file(get_fast_reload_snapshot(), GET_GLOBAL_STATE()->fast_reload_path, true);
+		}
+		else{
+			printf("===> GET_GLOBAL_STATE()->fast_reload_mode: FALSE\n");
+			fast_reload_create_to_file(get_fast_reload_snapshot(), GET_GLOBAL_STATE()->fast_reload_path, true);
+		}
+	}
+	else{
+		printf("===> GET_GLOBAL_STATE()->fast_reload_enabled: FALSE\n");
+		fast_reload_create_in_memory(get_fast_reload_snapshot(), true);
+	}
+}
+
+bool handle_hypercall_kafl_next_payload(struct kvm_run *run, CPUState *cpu){
+	if(hypercall_enabled){
+		if (init_state){
+			set_state_auxiliary_result_buffer(GET_GLOBAL_STATE()->auxilary_buffer, 2);
+			synchronization_lock();
+
+		} else {
+			if(!setup_snapshot_once){ 
+				fuzz_bitmap_reset();
+				create_fast_snapshot(cpu, false);
+				
+				setup_snapshot_once = true;
+
+				for(int i = 0; i < INTEL_PT_MAX_RANGES; i++){
+					if(GET_GLOBAL_STATE()->pt_ip_filter_configured[i]){
+						pt_enable_ip_filtering(cpu, i, true, false);
+					}
+				}
+				qemu_mutex_lock_iothread();
+				fast_reload_restore(get_fast_reload_snapshot());
+				qemu_mutex_unlock_iothread();
+				GET_GLOBAL_STATE()->in_fuzzing_mode = true;
+				set_state_auxiliary_result_buffer(GET_GLOBAL_STATE()->auxilary_buffer, 3);
+			}
+			else{
+				synchronization_lock();
+				reset_timeout_detector(&GET_GLOBAL_STATE()->timeout_detector);
+				GET_GLOBAL_STATE()->in_fuzzing_mode = true;
+
+				return true;
+			}
+		}
+	}
+	return false;
+}
+
+bool acquire_print_once_bool = true;
+bool release_print_once_bool = true;
+
+static void acquire_print_once(CPUState *cpu){
+	if(acquire_print_once_bool){
+		acquire_print_once_bool = false;
+		kvm_arch_get_registers(cpu);
+		fprintf(stderr,  "handle_hypercall_kafl_acquire at:%lx\n", get_rip(cpu));
+	}
+}
+
+void handle_hypercall_kafl_acquire(struct kvm_run *run, CPUState *cpu){
+	if(hypercall_enabled){
+		if (!init_state){
+			acquire_print_once(cpu);
+			synchronization_enter_fuzzing_loop(cpu);
+		}
+	}
+}
+
+void handle_hypercall_get_payload(struct kvm_run *run, CPUState *cpu){
+	if(hypercall_enabled && !setup_snapshot_once){
+			QEMU_PT_PRINTF(CORE_PREFIX, "Payload Address:\t%lx", (uint64_t)run->hypercall.args[0]);
+			kvm_arch_get_registers(cpu);	
+			CPUX86State *env = &(X86_CPU(cpu))->env;
+			GET_GLOBAL_STATE()->parent_cr3 = env->cr[3] & 0xFFFFFFFFFFFFF000ULL;
+			remap_payload_buffer(run->hypercall.args[0], cpu);
+			set_payload_buffer(run->hypercall.args[0]);
+	}
+}
+
+static void set_return_value(CPUState *cpu, uint64_t return_value){
+	kvm_arch_get_registers(cpu);	
+	CPUX86State *env = &(X86_CPU(cpu))->env;
+	env->regs[R_EAX] =  return_value;
+	kvm_arch_put_registers(cpu, KVM_PUT_RUNTIME_STATE);	
+}
+
+void handle_hypercall_kafl_req_stream_data(struct kvm_run *run, CPUState *cpu){
+	static uint8_t req_stream_buffer[0x1000];
+
+	kvm_arch_get_registers(cpu);	
+	/* address must be page aligned */
+	if(((uint64_t)run->hypercall.args[0]&0xFFF) != 0){
+		fprintf(stderr, "%s: ERROR -> address is not page aligned!\n", __func__);
+		set_return_value(cpu, 0xFFFFFFFFFFFFFFFFUL);
+	}
+	else{
+		read_virtual_memory((uint64_t)run->hypercall.args[0], (uint8_t*)req_stream_buffer, 0x100, cpu);
+		uint64_t bytes = sharedir_request_file(GET_GLOBAL_STATE()->sharedir, (char*)req_stream_buffer, req_stream_buffer);
+		if(bytes != 0xFFFFFFFFFFFFFFFFUL){
+			write_virtual_memory((uint64_t)run->hypercall.args[0], (uint8_t*)req_stream_buffer, bytes, cpu);
+		}
+		set_return_value(cpu, bytes);
+	}
+}
+
+void handle_hypercall_kafl_range_submit(struct kvm_run *run, CPUState *cpu){
+	uint64_t buffer[3];
+	read_virtual_memory((uint64_t)run->hypercall.args[0], (uint8_t*)&buffer, sizeof(buffer), cpu);
+
+	if(buffer[2] >= 2){
+		QEMU_PT_PRINTF(CORE_PREFIX, "%s: illegal range=%ld\n", __func__, buffer[2]);
+		return;
+	}
+
+	if(GET_GLOBAL_STATE()->pt_ip_filter_configured[buffer[2]]){
+			QEMU_PT_PRINTF(CORE_PREFIX, "Ignoring agent-provided address ranges (abort reason: 1)");
+		return;
+	}
+
+	if (buffer[0] != 0 && buffer[1] != 0 ){
+		GET_GLOBAL_STATE()->pt_ip_filter_a[buffer[2]] = buffer[0];
+		GET_GLOBAL_STATE()->pt_ip_filter_b[buffer[2]] = buffer[1];
+		GET_GLOBAL_STATE()->pt_ip_filter_configured[buffer[2]] = true;
+		QEMU_PT_PRINTF(CORE_PREFIX, "Configuring agent-provided address ranges:");
+		QEMU_PT_PRINTF(CORE_PREFIX, "\tIP0: %lx-%lx [ENABLED]", GET_GLOBAL_STATE()->pt_ip_filter_a[buffer[2]], GET_GLOBAL_STATE()->pt_ip_filter_b[buffer[2]]);
+	}
+	else{
+		QEMU_PT_PRINTF(CORE_PREFIX, "Ignoring agent-provided address ranges (abort reason: 2)");	
+	}
+
+}
+
+void handle_hypercall_get_program(struct kvm_run *run, CPUState *cpu){
+	kvm_arch_get_registers(cpu);
+	X86CPU *x86_cpu = X86_CPU(cpu);
+		CPUX86State *env = &x86_cpu->env;
+
+	if(hypercall_enabled){
+		if(program_buffer){
+
+			if (env->cr[4] & CR4_PAE_MASK) {
+        if (env->hflags & HF_LMA_MASK) {
+				}
+				else{
+					fprintf(stderr, "IN 32Bit PAE MODE\n");
+					abort();
+				}
+			}
+			else{
+				fprintf(stderr, "IN 32Bit MODE\n");
+				abort();
+			}
+			
+			write_virtual_memory((uint64_t)run->hypercall.args[0], program_buffer, PROGRAM_SIZE, cpu);
+		}
+	}
+}
+
+
+static void release_print_once(CPUState *cpu){
+	if(release_print_once_bool){
+		release_print_once_bool = false;
+		kvm_arch_get_registers(cpu);
+		fprintf(stderr,  "handle_hypercall_kafl_release at:%lx\n", get_rip(cpu));
+	}
+}
+
+void handle_hypercall_kafl_release(struct kvm_run *run, CPUState *cpu){
+	if(hypercall_enabled){
+		if (init_state){
+			init_state = false;	
+		} else {
+			synchronization_disable_pt(cpu);
+			release_print_once(cpu);
+		}
+	}
+}
+
+struct kvm_set_guest_debug_data {
+    struct kvm_guest_debug dbg;
+    int err;
+};
+
+void handle_hypercall_kafl_mtf(struct kvm_run *run, CPUState *cpu){
+	kvm_arch_get_registers_fast(cpu);
+
+	kvm_vcpu_ioctl(cpu, KVM_VMX_PT_DISABLE_MTF);
+
+	kvm_remove_all_breakpoints(cpu);
+	kvm_insert_breakpoint(cpu, GET_GLOBAL_STATE()->dump_page_addr, 1, 1);
+	kvm_update_guest_debug(cpu, 0);
+
+	kvm_vcpu_ioctl(cpu, KVM_VMX_PT_SET_PAGE_DUMP_CR3, GET_GLOBAL_STATE()->pt_c3_filter);
+	kvm_vcpu_ioctl(cpu, KVM_VMX_PT_ENABLE_PAGE_DUMP_CR3);
+}
+
+void handle_hypercall_kafl_page_dump_bp(struct kvm_run *run, CPUState *cpu, uint64_t page){
+	kvm_arch_get_registers_fast(cpu);
+
+	kvm_vcpu_ioctl(cpu, KVM_VMX_PT_DISABLE_MTF);
+
+	bool success = false;
+	page_cache_fetch(GET_GLOBAL_STATE()->page_cache, page, &success, false);
+	if(success){
+
+		kvm_remove_all_breakpoints(cpu);
+		kvm_vcpu_ioctl(cpu, KVM_VMX_PT_DISABLE_PAGE_DUMP_CR3);
+
+		pt_decoder_reset(cpu->pt_decoder_state);
+	}
+	else{
+		fprintf(stderr, "%s: FAIL: %d\n", __func__, success);
+
+		kvm_remove_all_breakpoints(cpu);
+
+		kvm_vcpu_ioctl(cpu, KVM_VMX_PT_DISABLE_PAGE_DUMP_CR3);
+		kvm_vcpu_ioctl(cpu, KVM_VMX_PT_ENABLE_MTF);
+	}
+
+}
+
+static inline void set_page_dump_bp(CPUState *cpu, uint64_t cr3, uint64_t addr){
+		
+	kvm_remove_all_breakpoints(cpu);
+	kvm_insert_breakpoint(cpu, addr, 1, 1);
+	kvm_update_guest_debug(cpu, 0);
+
+	kvm_vcpu_ioctl(cpu, KVM_VMX_PT_SET_PAGE_DUMP_CR3, cr3);
+	kvm_vcpu_ioctl(cpu, KVM_VMX_PT_ENABLE_PAGE_DUMP_CR3);
+}
+
+void handle_hypercall_kafl_cr3(struct kvm_run *run, CPUState *cpu){
+	if(hypercall_enabled){
+		pt_set_cr3(cpu, run->hypercall.args[0] & 0xFFFFFFFFFFFFF000ULL, false);
+		if(GET_GLOBAL_STATE()->dump_page){
+			set_page_dump_bp(cpu, run->hypercall.args[0] & 0xFFFFFFFFFFFFF000ULL, GET_GLOBAL_STATE()->dump_page_addr);
+		}
+	}
+}
+
+void handle_hypercall_kafl_submit_panic(struct kvm_run *run, CPUState *cpu){
+	if(hypercall_enabled){
+		QEMU_PT_PRINTF(CORE_PREFIX, "Panic address:\t%lx", (uint64_t)run->hypercall.args[0]);
+		if(notifiers_enabled){
+			write_virtual_memory((uint64_t)run->hypercall.args[0], (uint8_t*)PANIC_PAYLOAD, PAYLOAD_BUFFER_SIZE, cpu);
+		}
+	}
+}
+
+void handle_hypercall_kafl_submit_kasan(struct kvm_run *run, CPUState *cpu){
+	if(hypercall_enabled){
+		QEMU_PT_PRINTF(CORE_PREFIX, "kASAN address:\t%lx", (uint64_t)run->hypercall.args[0]);
+		if(notifiers_enabled){
+			write_virtual_memory((uint64_t)run->hypercall.args[0], (uint8_t*)KASAN_PAYLOAD, PAYLOAD_BUFFER_SIZE, cpu);
+		}
+	}
+}
+
+//#define PANIC_DEBUG
+
+void handle_hypercall_kafl_panic(struct kvm_run *run, CPUState *cpu){
+	static char reason[1024];
+	if(hypercall_enabled){
+#ifdef PANIC_DEBUG
+		if(run->hypercall.args[0]){
+			//QEMU_PT_PRINTF(CORE_PREFIX, "Panic in user mode!");
+		} else{
+			fprintf(stderr, "Panic in kernel mode!\n");
+			QEMU_PT_PRINTF(CORE_PREFIX, "Panic in kernel mode!");
+			//assert(0);
+		}
+#endif
+		if(fast_reload_snapshot_exists(get_fast_reload_snapshot())){
+
+			if(run->hypercall.args[0] & 0x8000000000000000ULL){
+
+				reason[0] = '\x00';
+
+				uint64_t address = run->hypercall.args[0] & 0x7FFFFFFFFFFFULL;
+				uint64_t signal = (run->hypercall.args[0] & 0x7800000000000ULL) >> 47;
+
+				snprintf(reason, 1024, "PANIC IN USER MODE (SIG: %d\tat 0x%lx)", (uint8_t)signal, address);
+				set_crash_reason_auxiliary_buffer(GET_GLOBAL_STATE()->auxilary_buffer, reason, strlen(reason));
+			}
+			else{
+				switch(run->hypercall.args[0]){
+					case 0:
+						set_crash_reason_auxiliary_buffer(GET_GLOBAL_STATE()->auxilary_buffer, (char*)"PANIC IN KERNEL MODE!", strlen("PANIC IN KERNEL MODE!"));
+						break;
+					case 1:
+						set_crash_reason_auxiliary_buffer(GET_GLOBAL_STATE()->auxilary_buffer, (char*)"PANIC IN USER MODE!", strlen("PANIC IN USER MODE!"));
+						break;
+					default:
+						set_crash_reason_auxiliary_buffer(GET_GLOBAL_STATE()->auxilary_buffer, (char*)"???", strlen("???"));
+						break;
+				}
+
+			}
+			synchronization_lock_crash_found();
+		} else{
+			QEMU_PT_PRINTF(CORE_PREFIX, "Panic detected during initialization of stage 1 or stage 2 loader");
+			QEMU_PT_PRINTF_DEBUG("Protocol - SEND: KAFL_PROTO_CRASH");
+		}
+	}
+}
+
+void handle_hypercall_kafl_panic_extended(struct kvm_run *run, CPUState *cpu){
+		if(fast_reload_snapshot_exists(get_fast_reload_snapshot())){
+			read_virtual_memory((uint64_t)run->hypercall.args[0], (uint8_t*)hprintf_buffer, HPRINTF_SIZE, cpu);
+			set_crash_reason_auxiliary_buffer(GET_GLOBAL_STATE()->auxilary_buffer, hprintf_buffer, strlen(hprintf_buffer));
+			synchronization_lock_crash_found();
+		} else{
+			QEMU_PT_PRINTF(CORE_PREFIX, "Panic detected during initialization of stage 1 or stage 2 loader");
+			QEMU_PT_PRINTF_DEBUG("Protocol - SEND: KAFL_PROTO_CRASH");
+		}
+}
+
+
+void handle_hypercall_kafl_kasan(struct kvm_run *run, CPUState *cpu){
+	if(hypercall_enabled){
+#ifdef PANIC_DEBUG
+		if(run->hypercall.args[0]){
+			QEMU_PT_PRINTF(CORE_PREFIX, "ASan notification in user mode!");
+		} else{
+			QEMU_PT_PRINTF(CORE_PREFIX, "ASan notification in kernel mode!");
+		}
+#endif
+		if(fast_reload_snapshot_exists(get_fast_reload_snapshot())){
+			synchronization_lock_asan_found();
+		} else{
+			QEMU_PT_PRINTF(CORE_PREFIX, "KASAN detected during initialization of stage 1 or stage 2 loader");
+			QEMU_PT_PRINTF_DEBUG("Protocol - SEND: KAFL_PROTO_KASAN");
+		}
+	}
+}
+
+void handle_hypercall_kafl_lock(struct kvm_run *run, CPUState *cpu){
+
+	if(!GET_GLOBAL_STATE()->fast_reload_pre_image){
+		QEMU_PT_PRINTF(CORE_PREFIX, "Skipping pre image creation (hint: set pre=on) ...");
+		return; 
+	}
+
+	QEMU_PT_PRINTF(CORE_PREFIX, "Creating pre image snapshot <%s> ...", GET_GLOBAL_STATE()->fast_reload_pre_path);
+
+	kvm_arch_get_registers(cpu);
+	fast_reload_create_to_file(get_fast_reload_snapshot(), GET_GLOBAL_STATE()->fast_reload_pre_path, true);
+
+	qemu_mutex_lock_iothread();
+	fast_reload_restore(get_fast_reload_snapshot());
+
+	qemu_mutex_unlock_iothread();
+
+	QEMU_PT_PRINTF(CORE_PREFIX, "Done! Bye!");
+
+	qemu_system_shutdown_request(SHUTDOWN_CAUSE_GUEST_SHUTDOWN);
+
+
+	return;
+	if(GET_GLOBAL_STATE()->fast_reload_enabled != true){
+		return;
+	}
+
+	fast_reload_create_to_file(get_fast_reload_snapshot(), GET_GLOBAL_STATE()->fast_reload_path, true);
+
+
+	qemu_mutex_lock_iothread();
+	fast_reload_restore(get_fast_reload_snapshot());
+
+	qemu_mutex_unlock_iothread();
+
+	return;
+}
+
+void handle_hypercall_kafl_info(struct kvm_run *run, CPUState *cpu){
+	if(setup_snapshot_once)
+		return;
+		
+	printf("%s\n", __func__);
+
+	read_virtual_memory((uint64_t)run->hypercall.args[0], (uint8_t*)info_buffer, INFO_SIZE, cpu);
+	FILE* info_file_fd = fopen(INFO_FILE, "w");
+	fprintf(info_file_fd, "%s\n", info_buffer);
+	fclose(info_file_fd);
+	if(hypercall_enabled){
+		//hypercall_snd_char(KAFL_PROTO_INFO);
+		QEMU_PT_PRINTF_DEBUG("Protocol - SEND: KAFL_PROTO_INFO");
+		abort();
+
+	}
+	qemu_system_shutdown_request(SHUTDOWN_CAUSE_GUEST_SHUTDOWN);
+}
+
+void enable_notifies(void){
+	notifiers_enabled = true;
+}
+
+void handle_hypercall_kafl_printf(struct kvm_run *run, CPUState *cpu){
+	read_virtual_memory((uint64_t)run->hypercall.args[0], (uint8_t*)hprintf_buffer, HPRINTF_SIZE, cpu);
+	set_hprintf_auxiliary_buffer(GET_GLOBAL_STATE()->auxilary_buffer, hprintf_buffer, strnlen(hprintf_buffer, HPRINTF_SIZE)+1);
+	synchronization_lock();
+}
+
+void handle_hypercall_kafl_printk(struct kvm_run *run, CPUState *cpu){
+	abort();
+}
+
+void handle_hypercall_kafl_printk_addr(struct kvm_run *run, CPUState *cpu){
+	if(!notifiers_enabled){
+		printf("%s\n", __func__);
+		printf("%lx\n", (uint64_t)run->hypercall.args[0]);
+		write_virtual_memory((uint64_t)run->hypercall.args[0], (uint8_t*)PRINTK_PAYLOAD, PRINTK_PAYLOAD_SIZE, cpu);
+	}		
+}
+
+void handle_hypercall_kafl_user_range_advise(struct kvm_run *run, CPUState *cpu){
+	kAFL_ranges* buf = malloc(sizeof(kAFL_ranges));
+
+	for(int i = 0; i < INTEL_PT_MAX_RANGES; i++){
+		buf->ip[i] = GET_GLOBAL_STATE()->pt_ip_filter_a[i]; 
+		buf->size[i] = (GET_GLOBAL_STATE()->pt_ip_filter_b[i]-GET_GLOBAL_STATE()->pt_ip_filter_a[i]);
+		buf->enabled[i] = (uint8_t)GET_GLOBAL_STATE()->pt_ip_filter_configured[i];
+	}
+
+	write_virtual_memory((uint64_t)run->hypercall.args[0], (uint8_t *)buf, sizeof(kAFL_ranges), cpu);
+	free(buf);
+}
+
+void handle_hypercall_kafl_user_submit_mode(struct kvm_run *run, CPUState *cpu){
+	switch((uint64_t)run->hypercall.args[0]){
+		case KAFL_MODE_64:
+			QEMU_PT_PRINTF(CORE_PREFIX, "target runs in KAFL_MODE_64 ...");
+			GET_GLOBAL_STATE()->disassembler_word_width = 64;
+			break;
+		case KAFL_MODE_32:
+			QEMU_PT_PRINTF(CORE_PREFIX, "target runs in KAFL_MODE_32 ...");
+			GET_GLOBAL_STATE()->disassembler_word_width = 32;
+			break;
+		case KAFL_MODE_16:
+			QEMU_PT_PRINTF(CORE_PREFIX, "target runs in KAFL_MODE_16 ...");
+			GET_GLOBAL_STATE()->disassembler_word_width = 16;
+			abort(); /* not implemented in this version (due to hypertrash hacks) */
+			break;
+		default:
+			QEMU_PT_PRINTF(CORE_PREFIX, "target runs in unkown mode...");
+			GET_GLOBAL_STATE()->disassembler_word_width = 0;
+			abort(); /* not implemented in this version (due to hypertrash hacks) */
+			break;
+	}
+}
+
+#ifdef CONFIG_REDQUEEN
+bool handle_hypercall_kafl_hook(struct kvm_run *run, CPUState *cpu){
+	X86CPU *cpux86 = X86_CPU(cpu);
+    CPUX86State *env = &cpux86->env;
+
+	for(uint8_t i = 0; i < INTEL_PT_MAX_RANGES; i++){
+		if (GET_GLOBAL_STATE()->redqueen_state && (env->eip >= GET_GLOBAL_STATE()->pt_ip_filter_a[i]) && (env->eip <= GET_GLOBAL_STATE()->pt_ip_filter_b[i])){
+			handle_hook(GET_GLOBAL_STATE()->redqueen_state);
+			return true;
+		}else if (cpu->singlestep_enabled && (GET_GLOBAL_STATE()->redqueen_state)->singlestep_enabled){
+			handle_hook(GET_GLOBAL_STATE()->redqueen_state);
+			return true;
+    }
+	}
+	return false;
+}
+
+void handle_hypercall_kafl_user_abort(struct kvm_run *run, CPUState *cpu){
+	if(hypercall_enabled){
+		QEMU_PT_PRINTF_DEBUG("Protocol - SEND: KAFL_PROTO_PT_ABORT");
+	}
+	fprintf(stderr, "USER ABORT!\n");
+	qemu_system_shutdown_request(SHUTDOWN_CAUSE_GUEST_SHUTDOWN);
+}
+
+void pt_enable_rqi(CPUState *cpu){
+	reload_mode_temp = true;
+	GET_GLOBAL_STATE()->redqueen_enable_pending = true;
+}
+
+void pt_disable_rqi(CPUState *cpu){
+	reload_mode_temp = false;
+	GET_GLOBAL_STATE()->redqueen_disable_pending = true;
+	GET_GLOBAL_STATE()->redqueen_instrumentation_mode = REDQUEEN_NO_INSTRUMENTATION;
+}
+
+void pt_set_enable_patches_pending(CPUState *cpu){
+	GET_GLOBAL_STATE()->patches_enable_pending = true;
+}
+
+void pt_set_redqueen_instrumentation_mode(CPUState *cpu, int redqueen_mode){
+	GET_GLOBAL_STATE()->redqueen_instrumentation_mode = redqueen_mode;
+}
+
+void pt_set_redqueen_update_blacklist(CPUState *cpu, bool newval){
+  assert(!newval || !GET_GLOBAL_STATE()->redqueen_update_blacklist);
+	GET_GLOBAL_STATE()->redqueen_update_blacklist = newval;
+}
+
+void pt_set_disable_patches_pending(CPUState *cpu){
+	GET_GLOBAL_STATE()->patches_disable_pending = true;
+}
+
+void pt_enable_rqi_trace(CPUState *cpu){
+	if (GET_GLOBAL_STATE()->redqueen_state){
+		redqueen_set_trace_mode(GET_GLOBAL_STATE()->redqueen_state);
+	}
+}
+
+void pt_disable_rqi_trace(CPUState *cpu){
+	if (GET_GLOBAL_STATE()->redqueen_state){
+		redqueen_unset_trace_mode(GET_GLOBAL_STATE()->redqueen_state);
+		return;
+	}
+}
+
+#endif
diff --new-file -ur qemu/pt/hypercall.h QEMU-PT/pt/hypercall.h
--- qemu/pt/hypercall.h	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/hypercall.h	2021-08-24 21:54:55.938586449 +0200
@@ -0,0 +1,154 @@
+/*
+
+Copyright (C) 2017 Sergej Schumilo
+
+This file is part of QEMU-PT (kAFL).
+
+QEMU-PT is free software: you can redistribute it and/or modify
+it under the terms of the GNU General Public License as published by
+the Free Software Foundation, either version 2 of the License, or
+(at your option) any later version.
+
+QEMU-PT is distributed in the hope that it will be useful,
+but WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with QEMU-PT.  If not, see <http://www.gnu.org/licenses/>.
+
+*/
+
+#ifndef HYPERCALL_H
+#define HYPERCALL_H
+
+#define PAYLOAD_BUFFER_SIZE		26
+#define PRINTK_PAYLOAD_SIZE		4
+
+#define KAFL_MODE_64	0
+#define KAFL_MODE_32	1
+#define KAFL_MODE_16	2
+
+typedef struct{
+	uint64_t ip[4];
+	uint64_t size[4];
+	uint8_t enabled[4];
+} kAFL_ranges; 
+
+bool check_bitmap_byte(uint32_t value);
+
+//#define PANIC_DEBUG
+
+/*
+ * Panic Notifier Payload (x86-64)
+ * fa                      cli
+ * 48 c7 c0 1f 00 00 00    mov    rax,0x1f
+ * 48 c7 c3 08 00 00 00    mov    rbx,0x8
+ * 48 c7 c1 00 00 00 00    mov    rcx,0x0
+ * 0f 01 c1                vmcall
+ * f4                      hlt
+ */
+#define PANIC_PAYLOAD "\xFA\x48\xC7\xC0\x1F\x00\x00\x00\x48\xC7\xC3\x08\x00\x00\x00\x48\xC7\xC1\x00\x00\x00\x00\x0F\x01\xC1\xF4"
+
+/*
+ * KASAN Notifier Payload (x86-64)
+ * fa                      cli
+ * 48 c7 c0 1f 00 00 00    mov    rax,0x1f
+ * 48 c7 c3 08 00 00 00    mov    rbx,0x9
+ * 48 c7 c1 00 00 00 00    mov    rcx,0x0
+ * 0f 01 c1                vmcall
+ * f4                      hlt
+ */
+#define KASAN_PAYLOAD "\xFA\x48\xC7\xC0\x1F\x00\x00\x00\x48\xC7\xC3\x09\x00\x00\x00\x48\xC7\xC1\x00\x00\x00\x00\x0F\x01\xC1\xF4"
+
+/*
+ * printk Notifier Payload (x86-64)
+ * 0f 01 c1                vmcall
+ * c3                      retn
+ */
+#define PRINTK_PAYLOAD "\x0F\x01\xC1\xC3"
+
+void pt_setup_program(void* ptr);
+void pt_setup_snd_handler(void (*tmp)(char, void*), void* tmp_s);
+void pt_setup_ip_filters(uint8_t filter_id, uint64_t start, uint64_t end);
+void pt_setup_enable_hypercalls(void);
+
+void pt_disable_wrapper(CPUState *cpu);
+
+void hypercall_submit_address(uint64_t address);
+bool hypercall_check_tuple(uint64_t current_addr, uint64_t prev_addr);
+//void hypercall_check_in_range(uint64_t* addr);
+
+
+bool hypercall_check_transition(uint64_t value);
+void hypercall_submit_transition(uint32_t value);
+
+void hypercall_enable_filter(void);
+void hypercall_disable_filter(void);
+void hypercall_commit_filter(void);
+
+bool pt_hypercalls_enabled(void);
+
+void hypercall_unlock(void);
+void hypercall_reload(void);
+
+void handle_hypercall_kafl_acquire(struct kvm_run *run, CPUState *cpu);
+void handle_hypercall_get_payload(struct kvm_run *run, CPUState *cpu);
+void handle_hypercall_get_program(struct kvm_run *run, CPUState *cpu);
+void handle_hypercall_kafl_release(struct kvm_run *run, CPUState *cpu);
+void handle_hypercall_kafl_cr3(struct kvm_run *run, CPUState *cpu);
+void handle_hypercall_kafl_submit_panic(struct kvm_run *run, CPUState *cpu);
+void handle_hypercall_kafl_submit_kasan(struct kvm_run *run, CPUState *cpu);
+void handle_hypercall_kafl_panic(struct kvm_run *run, CPUState *cpu);
+void handle_hypercall_kafl_panic_extended(struct kvm_run *run, CPUState *cpu);
+void handle_hypercall_kafl_kasan(struct kvm_run *run, CPUState *cpu);
+void handle_hypercall_kafl_lock(struct kvm_run *run, CPUState *cpu);
+void handle_hypercall_kafl_info(struct kvm_run *run, CPUState *cpu);
+void handle_hypercall_kafl_printf(struct kvm_run *run, CPUState *cpu);
+void handle_hypercall_kafl_printk_addr(struct kvm_run *run, CPUState *cpu);
+void handle_hypercall_kafl_printk(struct kvm_run *run, CPUState *cpu);
+void handle_hypercall_kafl_user_range_advise(struct kvm_run *run, CPUState *cpu);
+void handle_hypercall_kafl_user_submit_mode(struct kvm_run *run, CPUState *cpu);
+void handle_hypercall_kafl_user_abort(struct kvm_run *run, CPUState *cpu);
+
+/* HyperTrash! */
+/*
+void handle_hypercall_kafl_nested_hprintf(struct kvm_run *run, CPUState *cpu);
+void handle_hypercall_kafl_nested_prepare(struct kvm_run *run, CPUState *cpu);
+void handle_hypercall_kafl_nested_config(struct kvm_run *run, CPUState *cpu);
+void handle_hypercall_kafl_nested_release(struct kvm_run *run, CPUState *cpu);
+void handle_hypercall_kafl_nested_acquire(struct kvm_run *run, CPUState *cpu);
+*/
+
+void handle_hypercall_kafl_page_dump_bp(struct kvm_run *run, CPUState *cpu, uint64_t page);
+
+
+void hprintf(char* msg);
+void enable_notifies(void);
+
+bool handle_hypercall_kafl_next_payload(struct kvm_run *run, CPUState *cpu);
+void hypercall_reset_hprintf_counter(void);
+
+#ifdef CONFIG_REDQUEEN
+
+
+bool handle_hypercall_kafl_hook(struct kvm_run *run, CPUState *cpu);
+void handle_hypercall_kafl_mtf(struct kvm_run *run, CPUState *cpu);
+void pt_enable_rqo(CPUState *cpu);
+void pt_disable_rqo(CPUState *cpu);
+void pt_enable_rqi(CPUState *cpu);
+void pt_disable_rqi(CPUState *cpu);
+void pt_enable_rqi_trace(CPUState *cpu);
+void pt_disable_rqi_trace(CPUState *cpu);
+void pt_set_redqueen_instrumentation_mode(CPUState *cpu, int redqueen_instruction_mode);
+void pt_set_redqueen_update_blacklist(CPUState *cpu, bool newval);
+void pt_set_enable_patches_pending(CPUState *cpu);
+void pt_set_disable_patches_pending(CPUState *cpu);
+
+void create_fast_snapshot(CPUState *cpu, bool nested);
+
+void handle_hypercall_kafl_range_submit(struct kvm_run *run, CPUState *cpu);
+void handle_hypercall_kafl_req_stream_data(struct kvm_run *run, CPUState *cpu);
+
+#endif
+#endif
diff --new-file -ur qemu/pt/interface.c QEMU-PT/pt/interface.c
--- qemu/pt/interface.c	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/interface.c	2021-08-24 21:54:55.938586449 +0200
@@ -0,0 +1,453 @@
+/*
+
+Copyright (C) 2017 Sergej Schumilo
+
+This file is part of QEMU-PT (kAFL).
+
+QEMU-PT is free software: you can redistribute it and/or modify
+it under the terms of the GNU General Public License as published by
+the Free Software Foundation, either version 2 of the License, or
+(at your option) any later version.
+
+QEMU-PT is distributed in the hope that it will be useful,
+but WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with QEMU-PT.  If not, see <http://www.gnu.org/licenses/>.
+
+*/
+
+#include "qemu/osdep.h"
+#include "qapi/error.h"
+#include "qemu/cutils.h"
+#include "hw/qdev-properties.h"
+#include "hw/hw.h"
+#include "hw/i386/pc.h"
+#include "hw/pci/pci.h"
+#include "hw/pci/msi.h"
+#include "hw/pci/msix.h"
+#include "sysemu/kvm.h"
+#include "migration/migration.h"
+#include "qemu/error-report.h"
+#include "qemu/event_notifier.h"
+#include "qom/object_interfaces.h"
+#include "chardev/char-fe.h"
+#include "sysemu/hostmem.h"
+#include "sysemu/qtest.h"
+#include "qapi/visitor.h"
+#include "exec/ram_addr.h"
+#include <sys/mman.h>
+#include <sys/stat.h>
+#include "pt.h"
+#include "pt/hypercall.h"
+#include "pt/interface.h"
+#include "pt/debug.h"
+#include "pt/synchronization.h"
+#include "pt/state_reallocation.h"
+#include "pt/memory_access.h"
+#include <sys/ioctl.h>
+#include "pt/state.h"
+#include "pt/trace_cache.h"
+#include "pt/sharedir.h"
+
+#include <time.h>
+
+#ifdef CONFIG_REDQUEEN
+#include "redqueen.h"
+#endif
+
+#define CONVERT_UINT64(x) (uint64_t)(strtoull(x, NULL, 16))
+
+#define TYPE_KAFLMEM "kafl"
+#define KAFLMEM(obj) \
+		OBJECT_CHECK(kafl_mem_state, (obj), TYPE_KAFLMEM)
+
+uint32_t kafl_bitmap_size = DEFAULT_KAFL_BITMAP_SIZE;
+
+static void pci_kafl_guest_realize(DeviceState *dev, Error **errp);
+
+typedef struct kafl_mem_state {
+	DeviceState parent_obj;
+
+	Chardev *kafl_chr_drv_state;
+	CharBackend chr;
+
+	char* sharedir;
+
+	char* workdir; 
+	uint32_t worker_id;
+	
+	char* redqueen_workdir;
+	char* data_bar_fd_0;
+	char* data_bar_fd_1;
+	char* data_bar_fd_2;
+	char* bitmap_file;
+
+	char* filter_bitmap[4];
+	char* ip_filter[4][2];
+
+	uint64_t bitmap_size;
+
+	bool debug_mode; 	/* support for hprintf */
+	bool notifier;
+	bool dump_pt_trace;
+
+#ifdef CONFIG_REDQUEEN
+	bool redqueen;
+#endif
+	
+} kafl_mem_state;
+
+static void kafl_guest_event(void *opaque, QEMUChrEvent event){
+}
+
+static void send_char(char val, void* tmp_s){
+	kafl_mem_state *s = tmp_s;
+
+	assert(val == KAFL_PING);
+	__sync_synchronize();
+
+	qemu_chr_fe_write(&s->chr, (const uint8_t *) &val, 1);
+}
+
+static int kafl_guest_can_receive(void * opaque){
+	return sizeof(int64_t);
+}
+
+static kafl_mem_state* state = NULL;
+
+static void init_send_char(kafl_mem_state* s){
+	state = s;
+}
+
+bool interface_send_char(char val){
+
+	if(state){
+		send_char(val, state);
+		return true;
+	}
+	return false;
+}
+
+static void kafl_guest_receive(void *opaque, const uint8_t * buf, int size){
+	int i;				
+	for(i = 0; i < size; i++){
+		switch(buf[i]){
+			case KAFL_PING:
+				synchronization_unlock();
+				break;
+			case '\n':
+				break;
+			case 'E':
+				exit(0);
+			default:
+				break;
+				assert(false);
+		}
+	}
+}
+
+static int kafl_guest_create_memory_bar(kafl_mem_state *s, int region_num, uint64_t bar_size, const char* file, Error **errp){
+	void * ptr;
+	int fd;
+	struct stat st;
+	
+	fd = open(file, O_CREAT|O_RDWR, S_IRWXU|S_IRWXG|S_IRWXO);
+	assert(ftruncate(fd, bar_size) == 0);
+	stat(file, &st);
+	QEMU_PT_PRINTF(INTERFACE_PREFIX, "new shm file: (max size: %lx) %lx", bar_size, st.st_size);
+	
+	assert(bar_size == st.st_size);
+	ptr = mmap(0, bar_size, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0);
+
+	if (ptr == MAP_FAILED) {
+		error_setg_errno(errp, errno, "Failed to mmap memory");
+		return -1;
+	}
+
+	switch(region_num){
+		case 1:	pt_setup_program((void*)ptr);
+				break;
+		case 2:	set_payload_buffer_mapping_fd(fd, bar_size);
+				break;
+	}
+
+	init_send_char(s);
+
+	return 0;
+}
+
+static void kafl_guest_setup_bitmap(kafl_mem_state *s, char* filename, uint32_t bitmap_size){
+	void * ptr;
+	int fd;
+	struct stat st;
+	
+	fd = open(filename, O_CREAT|O_RDWR, S_IRWXU|S_IRWXG|S_IRWXO);
+	assert(ftruncate(fd, bitmap_size) == 0);
+	stat(filename, &st);
+	assert(bitmap_size == st.st_size);
+	ptr = mmap(0, bitmap_size, PROT_READ|PROT_WRITE, MAP_SHARED, fd, 0);
+	fuzz_bitmap_set_ptr((void*)ptr);
+}
+
+static bool folder_exits(const char* path){
+	struct stat sb;
+	return (stat(path, &sb) == 0 && S_ISDIR(sb.st_mode));
+}
+
+static bool file_exits(const char* path){
+	struct stat sb;   
+	return (stat (path, &sb) == 0);
+}
+
+static bool verify_workdir_state(kafl_mem_state *s, Error **errp){
+
+	char* workdir = s->workdir;
+	uint32_t id = s->worker_id;
+	char* tmp;
+
+	if (!folder_exits(workdir)){
+		QEMU_PT_PRINTF(INTERFACE_PREFIX, "%s does not exist...", workdir);
+		return false;
+	}
+
+	assert(asprintf(&tmp, "%s/interface_%d", workdir, id) != -1);
+	if (!file_exits(tmp)){
+		QEMU_PT_PRINTF(INTERFACE_PREFIX, "%s does not exist...", tmp);
+		free(tmp);
+		return false;
+	}
+	free(tmp);
+
+	assert(asprintf(&tmp, "%s/payload_%d", workdir, id) != -1);
+	if (!file_exits(tmp)){
+		QEMU_PT_PRINTF(INTERFACE_PREFIX, "%s does not exist...", tmp);
+		free(tmp);
+		return false;
+	}
+	else {
+		kafl_guest_create_memory_bar(s, 2, PAYLOAD_SIZE, tmp, errp);
+	}
+	free(tmp);
+
+	assert(asprintf(&tmp, "%s/bitmap_%d", workdir, id) != -1);
+	if (!file_exits(tmp)){
+		QEMU_PT_PRINTF(INTERFACE_PREFIX, "%s does not exist...", tmp);
+		free(tmp);
+		return false;
+	} else {
+		kafl_guest_setup_bitmap(s, tmp, fuzz_bitmap_get_size());
+	}
+	free(tmp);
+
+	assert(asprintf(&tmp, "%s/program", workdir) != -1);
+	if (!file_exits(tmp)){
+		QEMU_PT_PRINTF(INTERFACE_PREFIX, "%s does not exist...", tmp);
+		free(tmp);
+		return false;
+	} 
+	else {
+		kafl_guest_create_memory_bar(s, 1, PROGRAM_SIZE, tmp, errp);
+	}
+	free(tmp);
+
+
+	assert(asprintf(&tmp, "%s/page_cache.lock", workdir) != -1);
+	if (!file_exits(tmp)){
+		QEMU_PT_PRINTF(INTERFACE_PREFIX, "%s does not exist...", tmp);
+		free(tmp);
+		return false;
+	}
+	free(tmp);
+
+	assert(asprintf(&tmp, "%s/page_cache.addr", workdir) != -1);
+	if (!file_exits(tmp)){
+		QEMU_PT_PRINTF(INTERFACE_PREFIX, "%s does not exist...", tmp);
+		free(tmp);
+		return false;
+	}
+	free(tmp);
+
+	assert(asprintf(&tmp, "%s/page_cache.dump", workdir) != -1);
+	if (!file_exits(tmp)){
+		QEMU_PT_PRINTF(INTERFACE_PREFIX, "%s does not exist...", tmp);
+		free(tmp);
+		return false;
+	}
+	free(tmp);
+
+	assert(asprintf(&tmp, "%s/page_cache", workdir) != -1);
+	init_page_cache(tmp);
+
+	assert(asprintf(&tmp, "%s/redqueen_workdir_%d/", workdir, id) != -1);
+	if (!folder_exits(tmp)){
+		QEMU_PT_PRINTF(INTERFACE_PREFIX, "%s does not exist...", tmp);
+		free(tmp);
+		return false;
+	}
+	else {
+		setup_redqueen_workdir(tmp);
+	}
+	free(tmp);
+
+	init_redqueen_state();
+	init_redqueen_patch_state();
+
+  if(s->dump_pt_trace){
+	  assert(asprintf(&tmp, "%s/pt_trace_dump_%d", workdir, id) != -1);
+    pt_open_pt_trace_file(tmp);
+    free(tmp);
+  }
+
+
+	assert(asprintf(&tmp, "%s/aux_buffer_%d", workdir, id) != -1);
+	init_aux_buffer(tmp);
+	free(tmp);
+
+
+	return true;
+}
+
+#define KVM_VMX_PT_GET_ADDRN				_IO(KVMIO,	0xe9)
+
+static void check_range(uint8_t i){
+	int ret = 0;
+	int kvm = open("/dev/kmv-pt", O_RDWR | O_CLOEXEC);
+	ret = ioctl(kvm, KVM_VMX_PT_GET_ADDRN, NULL);
+
+	if(ret == -1){
+		QEMU_PT_PRINTF(INTERFACE_PREFIX, "ERROR: Multi range tracing is not supported! Please upgrade your kernel to 4.20-rc4!\n");
+		abort();
+	}
+
+	if(ret < (i+1)){
+		QEMU_PT_PRINTF(INTERFACE_PREFIX, "ERROR: CPU supports only %d IP filters!\n", ret);
+		abort();
+	}
+	close(kvm);
+}
+
+static bool verify_sharedir_state(kafl_mem_state *s, Error **errp){
+
+	char* sharedir = s->sharedir;
+
+	if (!folder_exits(sharedir)){
+		QEMU_PT_PRINTF(INTERFACE_PREFIX, "%s does not exist...", sharedir);
+		return false;
+	}
+	return true;
+}
+
+
+static void pci_kafl_guest_realize(DeviceState *dev, Error **errp){
+	uint64_t tmp0, tmp1;
+	kafl_mem_state *s = KAFLMEM(dev);
+
+	if(s->bitmap_size <= 0){
+		s->bitmap_size = DEFAULT_KAFL_BITMAP_SIZE;
+	}
+	fuzz_bitmap_set_size((uint32_t)s->bitmap_size);
+
+	if(s->worker_id == 0xFFFF){
+		QEMU_PT_PRINTF(INTERFACE_PREFIX, "Invalid worker id...\n");
+		abort();
+	}
+
+	if (!s->workdir || !verify_workdir_state(s, errp)){
+		QEMU_PT_PRINTF(INTERFACE_PREFIX, "Invalid work dir...\n");
+		abort();
+	}
+
+	if (!s->sharedir || !verify_sharedir_state(s, errp)){
+		QEMU_PT_PRINTF(INTERFACE_PREFIX, "Invalid sharedir...\n");
+		//abort();
+	}
+	else{
+		sharedir_set_dir(GET_GLOBAL_STATE()->sharedir, s->sharedir);
+	}
+
+	if(&s->chr)
+		qemu_chr_fe_set_handlers(&s->chr, kafl_guest_can_receive, kafl_guest_receive, kafl_guest_event, NULL, s, NULL, true);
+
+	for(uint8_t i = 0; i < INTEL_PT_MAX_RANGES; i++){
+		if(s->ip_filter[i][0] && s->ip_filter[i][1]){
+			if(i >= 1){
+				check_range(i);
+			}
+			tmp0 = CONVERT_UINT64(s->ip_filter[i][0]);
+			tmp1 = CONVERT_UINT64(s->ip_filter[i][1]);
+			if (tmp0 < tmp1){
+				pt_setup_ip_filters(i, tmp0, tmp1);
+			}
+		}
+	}
+
+	if(s->debug_mode){
+		GET_GLOBAL_STATE()->enable_hprintf = true;
+	}
+
+	if(s->notifier){
+		enable_notifies();
+	}
+
+	pt_setup_enable_hypercalls();
+	init_crash_handler();
+}
+
+static Property kafl_guest_properties[] = {
+	DEFINE_PROP_CHR("chardev", kafl_mem_state, chr),
+
+	DEFINE_PROP_STRING("sharedir", kafl_mem_state, sharedir),
+
+
+	DEFINE_PROP_STRING("workdir", kafl_mem_state, workdir),
+	DEFINE_PROP_UINT32("worker_id", kafl_mem_state, worker_id, 0xFFFF),
+
+	/* 
+	 * Since DEFINE_PROP_UINT64 is somewhat broken (signed/unsigned madness),
+	 * let's use DEFINE_PROP_STRING and post-process all values by strtol...
+	 */
+	DEFINE_PROP_STRING("ip0_a", kafl_mem_state, ip_filter[0][0]),
+	DEFINE_PROP_STRING("ip0_b", kafl_mem_state, ip_filter[0][1]),
+	DEFINE_PROP_STRING("ip1_a", kafl_mem_state, ip_filter[1][0]),
+	DEFINE_PROP_STRING("ip1_b", kafl_mem_state, ip_filter[1][1]),
+	DEFINE_PROP_STRING("ip2_a", kafl_mem_state, ip_filter[2][0]),
+	DEFINE_PROP_STRING("ip2_b", kafl_mem_state, ip_filter[2][1]),
+	DEFINE_PROP_STRING("ip3_a", kafl_mem_state, ip_filter[3][0]),
+	DEFINE_PROP_STRING("ip3_b", kafl_mem_state, ip_filter[3][1]),
+
+	DEFINE_PROP_UINT64("bitmap_size", kafl_mem_state, bitmap_size, DEFAULT_KAFL_BITMAP_SIZE),
+	DEFINE_PROP_BOOL("debug_mode", kafl_mem_state, debug_mode, false),
+	DEFINE_PROP_BOOL("crash_notifier", kafl_mem_state, notifier, true),
+	DEFINE_PROP_BOOL("dump_pt_trace", kafl_mem_state, dump_pt_trace, false),
+
+
+	DEFINE_PROP_END_OF_LIST(),
+};
+
+static void kafl_guest_class_init(ObjectClass *klass, void *data){
+	DeviceClass *dc = DEVICE_CLASS(klass);
+	dc->realize = pci_kafl_guest_realize;
+	dc->props = kafl_guest_properties;
+	set_bit(DEVICE_CATEGORY_MISC, dc->categories);
+	dc->desc = "KAFL Inter-VM shared memory";
+}
+
+static void kafl_guest_init(Object *obj){
+}
+
+static const TypeInfo kafl_guest_info = {
+	.name          = TYPE_KAFLMEM,
+	.parent        = TYPE_DEVICE,
+	.instance_size = sizeof(kafl_mem_state),
+	.instance_init = kafl_guest_init,
+	.class_init    = kafl_guest_class_init,
+};
+
+static void kafl_guest_register_types(void){
+	type_register_static(&kafl_guest_info);
+}
+
+type_init(kafl_guest_register_types)
diff --new-file -ur qemu/pt/interface.h QEMU-PT/pt/interface.h
--- qemu/pt/interface.h	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/interface.h	2021-08-24 21:54:55.938586449 +0200
@@ -0,0 +1,42 @@
+/*
+
+Copyright (C) 2017 Sergej Schumilo
+
+This file is part of QEMU-PT (kAFL).
+
+QEMU-PT is free software: you can redistribute it and/or modify
+it under the terms of the GNU General Public License as published by
+the Free Software Foundation, either version 2 of the License, or
+(at your option) any later version.
+
+QEMU-PT is distributed in the hope that it will be useful,
+but WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with QEMU-PT.  If not, see <http://www.gnu.org/licenses/>.
+
+*/
+
+#ifndef INTERFACE_H
+#define INTERFACE_H
+
+#define DEFAULT_KAFL_BITMAP_SIZE	0x10000
+#define DEFAULT_EDGE_FILTER_SIZE	0x1000000
+
+#define PROGRAM_SIZE				(128 << 20) /* 128MB Application Data */
+#define PAYLOAD_SIZE				(128 << 10)	/* 128KB Payload Data */
+#define INFO_SIZE					(128 << 10)	/* 128KB Info Data */
+#define HPRINTF_SIZE				0x1000 		/* 4KB hprintf Data */
+
+#define INFO_FILE					"/tmp/kAFL_info.txt"
+#define HPRINTF_FILE				"/tmp/kAFL_printf.txt"
+
+#define HPRINTF_LIMIT				512
+
+#define KAFL_PING           'x'
+
+bool interface_send_char(char val);
+
+#endif
diff --new-file -ur qemu/pt/khash.h QEMU-PT/pt/khash.h
--- qemu/pt/khash.h	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/khash.h	2021-08-24 21:54:55.938586449 +0200
@@ -0,0 +1,677 @@
+/* The MIT License
+
+   Copyright (c) 2008, 2009, 2011 by Attractive Chaos <attractor@live.co.uk>
+
+   Permission is hereby granted, free of charge, to any person obtaining
+   a copy of this software and associated documentation files (the
+   "Software"), to deal in the Software without restriction, including
+   without limitation the rights to use, copy, modify, merge, publish,
+   distribute, sublicense, and/or sell copies of the Software, and to
+   permit persons to whom the Software is furnished to do so, subject to
+   the following conditions:
+
+   The above copyright notice and this permission notice shall be
+   included in all copies or substantial portions of the Software.
+
+   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+   EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+   MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+   NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+   BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+   ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+   CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+   SOFTWARE.
+*/
+
+/*
+  An example:
+
+#include "khash.h"
+KHASH_MAP_INIT_INT(32, char)
+int main() {
+	int ret, is_missing;
+	khiter_t k;
+	khash_t(32) *h = kh_init(32);
+	k = kh_put(32, h, 5, &ret);
+	kh_value(h, k) = 10;
+	k = kh_get(32, h, 10);
+	is_missing = (k == kh_end(h));
+	k = kh_get(32, h, 5);
+	kh_del(32, h, k);
+	for (k = kh_begin(h); k != kh_end(h); ++k)
+		if (kh_exist(h, k)) kh_value(h, k) = 1;
+	kh_destroy(32, h);
+	return 0;
+}
+*/
+
+/*
+  2013-05-02 (0.2.8):
+
+	* Use quadratic probing. When the capacity is power of 2, stepping function
+	  i*(i+1)/2 guarantees to traverse each bucket. It is better than double
+	  hashing on cache performance and is more robust than linear probing.
+
+	  In theory, double hashing should be more robust than quadratic probing.
+	  However, my implementation is probably not for large hash tables, because
+	  the second hash function is closely tied to the first hash function,
+	  which reduce the effectiveness of double hashing.
+
+	Reference: http://research.cs.vt.edu/AVresearch/hashing/quadratic.php
+
+  2011-12-29 (0.2.7):
+
+    * Minor code clean up; no actual effect.
+
+  2011-09-16 (0.2.6):
+
+	* The capacity is a power of 2. This seems to dramatically improve the
+	  speed for simple keys. Thank Zilong Tan for the suggestion. Reference:
+
+	   - http://code.google.com/p/ulib/
+	   - http://nothings.org/computer/judy/
+
+	* Allow to optionally use linear probing which usually has better
+	  performance for random input. Double hashing is still the default as it
+	  is more robust to certain non-random input.
+
+	* Added Wang's integer hash function (not used by default). This hash
+	  function is more robust to certain non-random input.
+
+  2011-02-14 (0.2.5):
+
+    * Allow to declare global functions.
+
+  2009-09-26 (0.2.4):
+
+    * Improve portability
+
+  2008-09-19 (0.2.3):
+
+	* Corrected the example
+	* Improved interfaces
+
+  2008-09-11 (0.2.2):
+
+	* Improved speed a little in kh_put()
+
+  2008-09-10 (0.2.1):
+
+	* Added kh_clear()
+	* Fixed a compiling error
+
+  2008-09-02 (0.2.0):
+
+	* Changed to token concatenation which increases flexibility.
+
+  2008-08-31 (0.1.2):
+
+	* Fixed a bug in kh_get(), which has not been tested previously.
+
+  2008-08-31 (0.1.1):
+
+	* Added destructor
+*/
+
+
+#ifndef __AC_KHASH_H
+#define __AC_KHASH_H
+
+/*!
+  @header
+
+  Generic hash table library.
+ */
+
+#define AC_VERSION_KHASH_H "0.2.8"
+
+#include <stdlib.h>
+#include <string.h>
+#include <limits.h>
+#include <assert.h>
+
+/* compiler specific configuration */
+
+#if UINT_MAX == 0xffffffffu
+typedef unsigned int khint32_t;
+#elif ULONG_MAX == 0xffffffffu
+typedef unsigned long khint32_t;
+#endif
+
+#if ULONG_MAX == ULLONG_MAX
+typedef unsigned long khint64_t;
+#else
+typedef unsigned long long khint64_t;
+#endif
+
+#ifndef kh_inline
+#ifdef _MSC_VER
+#define kh_inline __inline
+#else
+#define kh_inline inline
+#endif
+#endif /* kh_inline */
+
+#ifndef klib_unused
+#if (defined __clang__ && __clang_major__ >= 3) || (defined __GNUC__ && __GNUC__ >= 3)
+#define klib_unused __attribute__ ((__unused__))
+#else
+#define klib_unused
+#endif
+#endif /* klib_unused */
+
+typedef khint64_t khint_t;
+typedef khint_t khiter_t;
+
+#define __ac_isempty(flag, i) ((flag[i>>4]>>((i&0xfU)<<1))&2)
+#define __ac_isdel(flag, i) ((flag[i>>4]>>((i&0xfU)<<1))&1)
+#define __ac_iseither(flag, i) ((flag[i>>4]>>((i&0xfU)<<1))&3)
+#define __ac_set_isdel_false(flag, i) (flag[i>>4]&=~(1ul<<((i&0xfU)<<1)))
+#define __ac_set_isempty_false(flag, i) (flag[i>>4]&=~(2ul<<((i&0xfU)<<1)))
+#define __ac_set_isboth_false(flag, i) (flag[i>>4]&=~(3ul<<((i&0xfU)<<1)))
+#define __ac_set_isdel_true(flag, i) (flag[i>>4]|=1ul<<((i&0xfU)<<1))
+#define __ac_fw(item, fp) (fwrite(&(item), 1, sizeof(item), fp))
+
+#define __ac_fsize(m) ((m) < 16? 1 : (m)>>4)
+
+#ifndef kroundup32
+#define kroundup32(x) (--(x), (x)|=(x)>>1, (x)|=(x)>>2, (x)|=(x)>>4, (x)|=(x)>>8, (x)|=(x)>>16, ++(x))
+#endif
+
+#ifndef kcalloc
+#define kcalloc(N,Z) calloc(N,Z)
+#endif
+#ifndef kmalloc
+#define kmalloc(Z) malloc(Z)
+#endif
+#ifndef krealloc
+#define krealloc(P,Z) realloc(P,Z)
+#endif
+#ifndef kfree
+#define kfree(P) free(P)
+#endif
+
+static const double __ac_HASH_UPPER = 0.77;
+
+#define __KHASH_TYPE(name, khkey_t, khval_t) \
+	typedef struct kh_##name##_s { \
+		khint_t n_buckets, size, n_occupied, upper_bound; \
+		khint32_t *flags; \
+		khkey_t *keys; \
+		khval_t *vals; \
+	} kh_##name##_t;
+
+#define __KHASH_PROTOTYPES(name, khkey_t, khval_t)	 					\
+	extern kh_##name##_t *kh_init_##name(void);							\
+	extern void kh_destroy_##name(kh_##name##_t *h);					\
+	extern void kh_clear_##name(kh_##name##_t *h);						\
+	extern khint_t kh_get_##name(const kh_##name##_t *h, khkey_t key); 	\
+	extern int kh_resize_##name(kh_##name##_t *h, khint_t new_n_buckets); \
+	extern khint_t kh_put_##name(kh_##name##_t *h, khkey_t key, int *ret); \
+	extern void kh_del_##name(kh_##name##_t *h, khint_t x);
+
+#define __KHASH_IMPL(name, SCOPE, khkey_t, khval_t, kh_is_map, __hash_func, __hash_equal) \
+	SCOPE kh_##name##_t *kh_init_##name(void) {							\
+		return (kh_##name##_t*)kcalloc(1, sizeof(kh_##name##_t));		\
+	}																	\
+	SCOPE void kh_destroy_##name(kh_##name##_t *h)						\
+	{																	\
+		if (h) {														\
+			kfree((void *)h->keys); kfree(h->flags);					\
+			kfree((void *)h->vals);										\
+			kfree(h);													\
+		}																\
+	}																	\
+	SCOPE void kh_clear_##name(kh_##name##_t *h)						\
+	{																	\
+		if (h && h->flags) {											\
+			memset(h->flags, 0xaa, __ac_fsize(h->n_buckets) * sizeof(khint32_t)); \
+			h->size = h->n_occupied = 0;								\
+		}																\
+	}																	\
+	SCOPE khint_t kh_get_##name(const kh_##name##_t *h, khkey_t key) 	\
+	{																	\
+		if (h->n_buckets) {												\
+			khint_t k, i, last, mask, step = 0; \
+			mask = h->n_buckets - 1;									\
+			k = __hash_func(key); i = k & mask;							\
+			last = i; \
+			while (!__ac_isempty(h->flags, i) && (__ac_isdel(h->flags, i) || !__hash_equal(h->keys[i], key))) { \
+				i = (i + (++step)) & mask; \
+				if (i == last) return h->n_buckets;						\
+			}															\
+			return __ac_iseither(h->flags, i)? h->n_buckets : i;		\
+		} else return 0;												\
+	}																	\
+	SCOPE int kh_resize_##name(kh_##name##_t *h, khint_t new_n_buckets) \
+	{ /* This function uses 0.25*n_buckets bytes of working space instead of [sizeof(key_t+val_t)+.25]*n_buckets. */ \
+		khint32_t *new_flags = 0;										\
+		khint_t j = 1;													\
+		{																\
+			kroundup32(new_n_buckets); 									\
+			if (new_n_buckets < 4) new_n_buckets = 4;					\
+			if (h->size >= (khint_t)(new_n_buckets * __ac_HASH_UPPER + 0.5)) j = 0;	/* requested size is too small */ \
+			else { /* hash table size to be changed (shrink or expand); rehash */ \
+				new_flags = (khint32_t*)kmalloc(__ac_fsize(new_n_buckets) * sizeof(khint32_t));	\
+				if (!new_flags) return -1;								\
+				memset(new_flags, 0xaa, __ac_fsize(new_n_buckets) * sizeof(khint32_t)); \
+				if (h->n_buckets < new_n_buckets) {	/* expand */		\
+					khkey_t *new_keys = (khkey_t*)krealloc((void *)h->keys, new_n_buckets * sizeof(khkey_t)); \
+					if (!new_keys) { kfree(new_flags); return -1; }		\
+					h->keys = new_keys;									\
+					if (kh_is_map) {									\
+						khval_t *new_vals = (khval_t*)krealloc((void *)h->vals, new_n_buckets * sizeof(khval_t)); \
+						if (!new_vals) { kfree(new_flags); return -1; }	\
+						h->vals = new_vals;								\
+					}													\
+				} /* otherwise shrink */								\
+			}															\
+		}																\
+		if (j) { /* rehashing is needed */								\
+			for (j = 0; j != h->n_buckets; ++j) {						\
+				if (__ac_iseither(h->flags, j) == 0) {					\
+					khkey_t key = h->keys[j];							\
+					khval_t val;										\
+					khint_t new_mask;									\
+					new_mask = new_n_buckets - 1; 						\
+					if (kh_is_map) val = h->vals[j];					\
+					__ac_set_isdel_true(h->flags, j);					\
+					while (1) { /* kick-out process; sort of like in Cuckoo hashing */ \
+						khint_t k, i, step = 0; \
+						k = __hash_func(key);							\
+						i = k & new_mask;								\
+						while (!__ac_isempty(new_flags, i)) i = (i + (++step)) & new_mask; \
+						__ac_set_isempty_false(new_flags, i);			\
+						if (i < h->n_buckets && __ac_iseither(h->flags, i) == 0) { /* kick out the existing element */ \
+							{ khkey_t tmp = h->keys[i]; h->keys[i] = key; key = tmp; } \
+							if (kh_is_map) { khval_t tmp = h->vals[i]; h->vals[i] = val; val = tmp; } \
+							__ac_set_isdel_true(h->flags, i); /* mark it as deleted in the old hash table */ \
+						} else { /* write the element and jump out of the loop */ \
+							h->keys[i] = key;							\
+							if (kh_is_map) h->vals[i] = val;			\
+							break;										\
+						}												\
+					}													\
+				}														\
+			}															\
+			if (h->n_buckets > new_n_buckets) { /* shrink the hash table */ \
+				h->keys = (khkey_t*)krealloc((void *)h->keys, new_n_buckets * sizeof(khkey_t)); \
+				if (kh_is_map) h->vals = (khval_t*)krealloc((void *)h->vals, new_n_buckets * sizeof(khval_t)); \
+			}															\
+			kfree(h->flags); /* free the working space */				\
+			h->flags = new_flags;										\
+			h->n_buckets = new_n_buckets;								\
+			h->n_occupied = h->size;									\
+			h->upper_bound = (khint_t)(h->n_buckets * __ac_HASH_UPPER + 0.5); \
+		}																\
+		return 0;														\
+	}																	\
+	SCOPE khint_t kh_put_##name(kh_##name##_t *h, khkey_t key, int *ret) \
+	{																	\
+		khint_t x;														\
+		if (h->n_occupied >= h->upper_bound) { /* update the hash table */ \
+			if (h->n_buckets > (h->size<<1)) {							\
+				if (kh_resize_##name(h, h->n_buckets - 1) < 0) { /* clear "deleted" elements */ \
+					*ret = -1; return h->n_buckets;						\
+				}														\
+			} else if (kh_resize_##name(h, h->n_buckets + 1) < 0) { /* expand the hash table */ \
+				*ret = -1; return h->n_buckets;							\
+			}															\
+		} /* TODO: to implement automatically shrinking; resize() already support shrinking */ \
+		{																\
+			khint_t k, i, site, last, mask = h->n_buckets - 1, step = 0; \
+			x = site = h->n_buckets; k = __hash_func(key); i = k & mask; \
+			if (__ac_isempty(h->flags, i)) x = i; /* for speed up */	\
+			else {														\
+				last = i; \
+				while (!__ac_isempty(h->flags, i) && (__ac_isdel(h->flags, i) || !__hash_equal(h->keys[i], key))) { \
+					if (__ac_isdel(h->flags, i)) site = i;				\
+					i = (i + (++step)) & mask; \
+					if (i == last) { x = site; break; }					\
+				}														\
+				if (x == h->n_buckets) {								\
+					if (__ac_isempty(h->flags, i) && site != h->n_buckets) x = site; \
+					else x = i;											\
+				}														\
+			}															\
+		}																\
+		if (__ac_isempty(h->flags, x)) { /* not present at all */		\
+			h->keys[x] = key;											\
+			__ac_set_isboth_false(h->flags, x);							\
+			++h->size; ++h->n_occupied;									\
+			*ret = 1;													\
+		} else if (__ac_isdel(h->flags, x)) { /* deleted */				\
+			h->keys[x] = key;											\
+			__ac_set_isboth_false(h->flags, x);							\
+			++h->size;													\
+			*ret = 2;													\
+		} else *ret = 0; /* Don't touch h->keys[x] if present and not deleted */ \
+		return x;														\
+	}																	\
+	SCOPE void kh_del_##name(kh_##name##_t *h, khint_t x)				\
+	{																	\
+		if (x != h->n_buckets && !__ac_iseither(h->flags, x)) {			\
+			__ac_set_isdel_true(h->flags, x);							\
+			--h->size;													\
+		}																\
+	}                                                                   \
+    SCOPE void kh_write_##name(kh_##name##_t *map, const char *path) {  \
+        FILE *fp = fopen(path, "wb");                                   \
+        if(fp == NULL) {                                                \
+            fprintf(stderr, "[%s] Could not open file %s.\n", __func__, path);\
+            assert(0);                                              \
+						/*exit(EXIT_FAILURE);*/                                     \
+        }                                                               \
+        __ac_fw(map->n_buckets, fp);                                    \
+        __ac_fw(map->n_occupied, fp);                                   \
+        __ac_fw(map->size, fp);                                         \
+        __ac_fw(map->upper_bound, fp);                                  \
+        fwrite(map->flags, __ac_fsize(map->n_buckets), sizeof(khint32_t), fp);\
+        fwrite(map->keys, map->n_buckets, sizeof(*map->keys), fp);      \
+        fwrite(map->vals, map->n_buckets, sizeof(*map->vals), fp);      \
+        fclose(fp);                                                     \
+    }                                                                   \
+    SCOPE kh_##name##_t *khash_load_##name(const char *path)            \
+    {                                                                   \
+        kh_##name##_t *ret = calloc(1, sizeof(kh_##name##_t));          \
+        FILE *fp = fopen(path, "rb");                                   \
+        assert(sizeof(ret->n_buckets) == fread(&ret->n_buckets, 1, sizeof(ret->n_buckets), fp));          \
+        assert(sizeof(ret->n_occupied) == fread(&ret->n_occupied, 1, sizeof(ret->n_occupied), fp));        \
+        assert(sizeof(ret->size) == fread(&ret->size, 1, sizeof(ret->size), fp));                    \
+        assert(sizeof(ret->upper_bound) == fread(&ret->upper_bound, 1, sizeof(ret->upper_bound), fp));      \
+        ret->flags = malloc(sizeof(*ret->flags) * __ac_fsize(ret->n_buckets));\
+        ret->keys =  malloc(sizeof(khkey_t) * ret->n_buckets);          \
+        ret->vals =  malloc(sizeof(khval_t) * ret->n_buckets);          \
+        assert(sizeof(*ret->flags) == fread(ret->flags, __ac_fsize(ret->n_buckets), sizeof(*ret->flags), fp));\
+        assert(ret->n_buckets * sizeof(*ret->keys) == fread(ret->keys, 1, ret->n_buckets * sizeof(*ret->keys), fp));   \
+        assert(ret->n_buckets * sizeof(*ret->vals) == fread(ret->vals, 1, ret->n_buckets * sizeof(*ret->vals), fp));   \
+        fclose(fp);                                                     \
+        return ret;                                                     \
+    }
+
+#define KHASH_DECLARE(name, khkey_t, khval_t)		 					\
+	__KHASH_TYPE(name, khkey_t, khval_t) 								\
+	__KHASH_PROTOTYPES(name, khkey_t, khval_t)
+
+#define KHASH_INIT2(name, SCOPE, khkey_t, khval_t, kh_is_map, __hash_func, __hash_equal) \
+	__KHASH_TYPE(name, khkey_t, khval_t) 								\
+	__KHASH_IMPL(name, SCOPE, khkey_t, khval_t, kh_is_map, __hash_func, __hash_equal)
+
+#define KHASH_INIT(name, khkey_t, khval_t, kh_is_map, __hash_func, __hash_equal) \
+	KHASH_INIT2(name, static kh_inline klib_unused, khkey_t, khval_t, kh_is_map, __hash_func, __hash_equal)
+
+/* --- BEGIN OF HASH FUNCTIONS --- */
+
+/*! @function
+  @abstract     Integer hash function
+  @param  key   The integer [khint32_t]
+  @return       The hash value [khint_t]
+ */
+#define kh_int_hash_func(key) (khint32_t)(key)
+/*! @function
+  @abstract     Integer comparison function
+ */
+#define kh_int_hash_equal(a, b) ((a) == (b))
+/*! @function
+  @abstract     64-bit integer hash function
+  @param  key   The integer [khint64_t]
+  @return       The hash value [khint_t]
+ */
+#define kh_int64_hash_func(key) (khint32_t)((key)>>33^(key)^(key)<<11)
+/*! @function
+  @abstract     64-bit integer comparison function
+ */
+#define kh_int64_hash_equal(a, b) ((a) == (b))
+/*! @function
+  @abstract     const char* hash function
+  @param  s     Pointer to a null terminated string
+  @return       The hash value
+ */
+static kh_inline khint_t __ac_X31_hash_string(const char *s)
+{
+	khint_t h = (khint_t)*s;
+	if (h) for (++s ; *s; ++s) h = (h << 5) - h + (khint_t)*s;
+	return h;
+}
+/*! @function
+  @abstract     Another interface to const char* hash function
+  @param  key   Pointer to a null terminated string [const char*]
+  @return       The hash value [khint_t]
+ */
+#define kh_str_hash_func(key) __ac_X31_hash_string(key)
+/*! @function
+  @abstract     Const char* comparison function
+ */
+#define kh_str_hash_equal(a, b) (strcmp(a, b) == 0)
+
+static kh_inline khint_t __ac_Wang_hash(khint_t key)
+{
+    key += ~(key << 15);
+    key ^=  (key >> 10);
+    key +=  (key << 3);
+    key ^=  (key >> 6);
+    key += ~(key << 11);
+    key ^=  (key >> 16);
+    return key;
+}
+#define kh_int_hash_func2(key) __ac_Wang_hash((khint_t)key)
+
+/* --- END OF HASH FUNCTIONS --- */
+
+/* Other convenient macros... */
+
+/*!
+  @abstract Type of the hash table.
+  @param  name  Name of the hash table [symbol]
+ */
+#define khash_t(name) kh_##name##_t
+
+/*! @function
+  @abstract     Initiate a hash table.
+  @param  name  Name of the hash table [symbol]
+  @return       Pointer to the hash table [khash_t(name)*]
+ */
+#define kh_init(name) kh_init_##name()
+
+/*! @function
+  @abstract     Destroy a hash table.
+  @param  name  Name of the hash table [symbol]
+  @param  h     Pointer to the hash table [khash_t(name)*]
+ */
+#define kh_destroy(name, h) kh_destroy_##name(h)
+
+/*! @function
+  @abstract     Reset a hash table without deallocating memory.
+  @param  name  Name of the hash table [symbol]
+  @param  h     Pointer to the hash table [khash_t(name)*]
+ */
+#define kh_clear(name, h) kh_clear_##name(h)
+
+/*! @function
+  @abstract     Resize a hash table.
+  @param  name  Name of the hash table [symbol]
+  @param  h     Pointer to the hash table [khash_t(name)*]
+  @param  s     New size [khint_t]
+ */
+#define kh_resize(name, h, s) kh_resize_##name(h, s)
+
+/*! @function
+  @abstract     Insert a key to the hash table.
+  @param  name  Name of the hash table [symbol]
+  @param  h     Pointer to the hash table [khash_t(name)*]
+  @param  k     Key [type of keys]
+  @param  r     Extra return code: -1 if the operation failed;
+                0 if the key is present in the hash table;
+                1 if the bucket is empty (never used); 2 if the element in
+				the bucket has been deleted [int*]
+  @return       Iterator to the inserted element [khint_t]
+ */
+#define kh_put(name, h, k, r) kh_put_##name(h, k, r)
+
+/*! @function
+  @abstract     Retrieve a key from the hash table.
+  @param  name  Name of the hash table [symbol]
+  @param  h     Pointer to the hash table [khash_t(name)*]
+  @param  k     Key [type of keys]
+  @return       Iterator to the found element, or kh_end(h) if the element is absent [khint_t]
+ */
+#define kh_get(name, h, k) kh_get_##name(h, k)
+
+/*! @function
+  @abstract     Remove a key from the hash table.
+  @param  name  Name of the hash table [symbol]
+  @param  h     Pointer to the hash table [khash_t(name)*]
+  @param  k     Iterator to the element to be deleted [khint_t]
+ */
+#define kh_del(name, h, k) kh_del_##name(h, k)
+
+/*! @function
+  @abstract     Write a hash map to disk.
+  @param  h     Pointer to the hash table [khash_t(name)*]
+  @param  path  Path to which to write. [const char *]
+ */
+#define kh_write(name, h, path) kh_write_##name(h, path)
+
+/*! @function
+  @abstract     Load a hash table from disk
+  @param  name  Name of the hash table [symbol]
+  @param  path  Path to file from which to load [const char *]
+ */
+
+#define kh_load(name, path) khash_load_##name(path)
+
+/*! @function
+  @abstract     Test whether a bucket contains data.
+  @param  h     Pointer to the hash table [khash_t(name)*]
+  @param  x     Iterator to the bucket [khint_t]
+  @return       1 if containing data; 0 otherwise [int]
+ */
+#define kh_exist(h, x) (!__ac_iseither((h)->flags, (x)))
+
+/*! @function
+  @abstract     Get key given an iterator
+  @param  h     Pointer to the hash table [khash_t(name)*]
+  @param  x     Iterator to the bucket [khint_t]
+  @return       Key [type of keys]
+ */
+#define kh_key(h, x) ((h)->keys[x])
+
+/*! @function
+  @abstract     Get value given an iterator
+  @param  h     Pointer to the hash table [khash_t(name)*]
+  @param  x     Iterator to the bucket [khint_t]
+  @return       Value [type of values]
+  @discussion   For hash sets, calling this results in segfault.
+ */
+#define kh_val(h, x) ((h)->vals[x])
+
+/*! @function
+  @abstract     Alias of kh_val()
+ */
+#define kh_value(h, x) ((h)->vals[x])
+
+/*! @function
+  @abstract     Get the start iterator
+  @param  h     Pointer to the hash table [khash_t(name)*]
+  @return       The start iterator [khint_t]
+ */
+#define kh_begin(h) (khint_t)(0)
+
+/*! @function
+  @abstract     Get the end iterator
+  @param  h     Pointer to the hash table [khash_t(name)*]
+  @return       The end iterator [khint_t]
+ */
+#define kh_end(h) ((h)->n_buckets)
+
+/*! @function
+  @abstract     Get the number of elements in the hash table
+  @param  h     Pointer to the hash table [khash_t(name)*]
+  @return       Number of elements in the hash table [khint_t]
+ */
+#define kh_size(h) ((h)->size)
+
+/*! @function
+  @abstract     Get the number of buckets in the hash table
+  @param  h     Pointer to the hash table [khash_t(name)*]
+  @return       Number of buckets in the hash table [khint_t]
+ */
+#define kh_n_buckets(h) ((h)->n_buckets)
+
+/*! @function
+  @abstract     Iterate over the entries in the hash table
+  @param  h     Pointer to the hash table [khash_t(name)*]
+  @param  kvar  Variable to which key will be assigned
+  @param  vvar  Variable to which value will be assigned
+  @param  code  Block of code to execute
+ */
+#define kh_foreach(h, kvar, vvar, code) { khint_t __i;		\
+	for (__i = kh_begin(h); __i != kh_end(h); ++__i) {		\
+		if (!kh_exist(h,__i)) continue;						\
+		(kvar) = kh_key(h,__i);								\
+		(vvar) = kh_val(h,__i);								\
+		code;												\
+	} }
+
+/*! @function
+  @abstract     Iterate over the values in the hash table
+  @param  h     Pointer to the hash table [khash_t(name)*]
+  @param  vvar  Variable to which value will be assigned
+  @param  code  Block of code to execute
+ */
+#define kh_foreach_value(h, vvar, code) { khint_t __i;		\
+	for (__i = kh_begin(h); __i != kh_end(h); ++__i) {		\
+		if (!kh_exist(h,__i)) continue;						\
+		(vvar) = kh_val(h,__i);								\
+		code;												\
+	} }
+
+/* More conenient interfaces */
+
+/*! @function
+  @abstract     Instantiate a hash set containing integer keys
+  @param  name  Name of the hash table [symbol]
+ */
+#define KHASH_SET_INIT_INT(name)										\
+	KHASH_INIT(name, khint32_t, char, 0, kh_int_hash_func, kh_int_hash_equal)
+
+/*! @function
+  @abstract     Instantiate a hash map containing integer keys
+  @param  name  Name of the hash table [symbol]
+  @param  khval_t  Type of values [type]
+ */
+#define KHASH_MAP_INIT_INT(name, khval_t)								\
+	KHASH_INIT(name, khint32_t, khval_t, 1, kh_int_hash_func, kh_int_hash_equal)
+
+/*! @function
+  @abstract     Instantiate a hash map containing 64-bit integer keys
+  @param  name  Name of the hash table [symbol]
+ */
+#define KHASH_SET_INIT_INT64(name)										\
+	KHASH_INIT(name, khint64_t, char, 0, kh_int64_hash_func, kh_int64_hash_equal)
+
+/*! @function
+  @abstract     Instantiate a hash map containing 64-bit integer keys
+  @param  name  Name of the hash table [symbol]
+  @param  khval_t  Type of values [type]
+ */
+#define KHASH_MAP_INIT_INT64(name, khval_t)								\
+	KHASH_INIT(name, khint64_t, khval_t, 1, kh_int64_hash_func, kh_int64_hash_equal)
+
+typedef const char *kh_cstr_t;
+/*! @function
+  @abstract     Instantiate a hash map containing const char* keys
+  @param  name  Name of the hash table [symbol]
+ */
+#define KHASH_SET_INIT_STR(name)										\
+	KHASH_INIT(name, kh_cstr_t, char, 0, kh_str_hash_func, kh_str_hash_equal)
+
+/*! @function
+  @abstract     Instantiate a hash map containing const char* keys
+  @param  name  Name of the hash table [symbol]
+  @param  khval_t  Type of values [type]
+ */
+#define KHASH_MAP_INIT_STR(name, khval_t)								\
+	KHASH_INIT(name, kh_cstr_t, khval_t, 1, kh_str_hash_func, kh_str_hash_equal)
+
+#endif /* __AC_KHASH_H */
\ No newline at end of file
diff --new-file -ur qemu/pt/kvm_nested.c QEMU-PT/pt/kvm_nested.c
--- qemu/pt/kvm_nested.c	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/kvm_nested.c	2021-08-26 11:19:23.550856253 +0200
@@ -0,0 +1,436 @@
+#include "pt/kvm_nested.h"
+#include "cpu.h"
+#include <linux/kvm.h>
+#include "pt/debug.h"
+#include "exec/ram_addr.h"
+#include "qemu/rcu_queue.h"
+#include "pt/state.h"
+#include "sysemu/kvm.h"
+#include "pt.h"
+
+#define PPAGE_SIZE 0x1000
+#define PENTRIES 0x200
+#define PLEVEL_4_SHIFT 12
+#define PLEVEL_3_SHIFT 21
+#define PLEVEL_2_SHIFT 30
+#define PLEVEL_1_SHIFT 39
+#define SIGN_EXTEND_TRESHOLD 0x100
+#define SIGN_EXTEND 0xFFFF000000000000ULL
+#define PAGETABLE_MASK 0xFFFFFFFFFF000ULL
+#define CHECK_BIT(var,pos) !!(((var) & (1ULL<<(pos))))
+
+
+struct vmcs_hdr {
+	uint32_t revision_id:31;
+	uint32_t shadow_vmcs:1;
+};
+
+struct __attribute__((__packed__)) vmcs12 {
+	/* According to the Intel spec, a VMCS region must start with the
+	 * following two fields. Then follow implementation-specific data.
+	 */
+	struct vmcs_hdr hdr;
+	uint32_t abort;
+
+	uint32_t launch_state; /* set to 0 by VMCLEAR, to 1 by VMLAUNCH */
+	uint32_t padding[7]; /* room for future expansion */
+
+	uint64_t io_bitmap_a;
+	uint64_t io_bitmap_b;
+	uint64_t msr_bitmap;
+	uint64_t vm_exit_msr_store_addr;
+	uint64_t vm_exit_msr_load_addr;
+	uint64_t vm_entry_msr_load_addr;
+	uint64_t tsc_offset;
+	uint64_t virtual_apic_page_addr;
+	uint64_t apic_access_addr;
+	uint64_t posted_intr_desc_addr;
+	uint64_t ept_pointer;
+	uint64_t eoi_exit_bitmap0;
+	uint64_t eoi_exit_bitmap1;
+	uint64_t eoi_exit_bitmap2;
+	uint64_t eoi_exit_bitmap3;
+	uint64_t xss_exit_bitmap;
+	uint64_t guest_physical_address;
+	uint64_t vmcs_link_pointer;
+	uint64_t guest_ia32_debugctl;
+	uint64_t guest_ia32_pat;
+	uint64_t guest_ia32_efer;
+	uint64_t guest_ia32_perf_global_ctrl;
+	uint64_t guest_pdptr0;
+	uint64_t guest_pdptr1;
+	uint64_t guest_pdptr2;
+	uint64_t guest_pdptr3;
+	uint64_t guest_bndcfgs;
+	uint64_t host_ia32_pat;
+	uint64_t host_ia32_efer;
+	uint64_t host_ia32_perf_global_ctrl;
+	uint64_t vmread_bitmap;
+	uint64_t vmwrite_bitmap;
+	uint64_t vm_function_control;
+	uint64_t eptp_list_address;
+	uint64_t pml_address;
+	uint64_t padding64[3]; /* room for future expansion */
+	/*
+	 * To allow migration of L1 (complete with its L2 guests) between
+	 * machines of different natural widths (32 or 64 bit), we cannot have
+	 * unsigned long fields with no explict size. We use uint64_t (aliased
+	 * uint64_t) instead. Luckily, x86 is little-endian.
+	 */
+	uint64_t cr0_guest_host_mask;
+	uint64_t cr4_guest_host_mask;
+	uint64_t cr0_read_shadow;
+	uint64_t cr4_read_shadow;
+	uint64_t cr3_target_value0;
+	uint64_t cr3_target_value1;
+	uint64_t cr3_target_value2;
+	uint64_t cr3_target_value3;
+	uint64_t exit_qualification;
+	uint64_t guest_linear_address;
+	uint64_t guest_cr0;
+	uint64_t guest_cr3;
+	uint64_t guest_cr4;
+	uint64_t guest_es_base;
+	uint64_t guest_cs_base;
+	uint64_t guest_ss_base;
+	uint64_t guest_ds_base;
+	uint64_t guest_fs_base;
+	uint64_t guest_gs_base;
+	uint64_t guest_ldtr_base;
+	uint64_t guest_tr_base;
+	uint64_t guest_gdtr_base;
+	uint64_t guest_idtr_base;
+	uint64_t guest_dr7;
+	uint64_t guest_rsp;
+	uint64_t guest_rip;
+	uint64_t guest_rflags;
+	uint64_t guest_pending_dbg_exceptions;
+	uint64_t guest_sysenter_esp;
+	uint64_t guest_sysenter_eip;
+	uint64_t host_cr0;
+	uint64_t host_cr3;
+	uint64_t host_cr4;
+	uint64_t host_fs_base;
+	uint64_t host_gs_base;
+	uint64_t host_tr_base;
+	uint64_t host_gdtr_base;
+	uint64_t host_idtr_base;
+	uint64_t host_ia32_sysenter_esp;
+	uint64_t host_ia32_sysenter_eip;
+	uint64_t host_rsp;
+	uint64_t host_rip;
+	uint64_t paddingl[8]; /* room for future expansion */
+	uint32_t pin_based_vm_exec_control;
+	uint32_t cpu_based_vm_exec_control;
+	uint32_t exception_bitmap;
+	uint32_t page_fault_error_code_mask;
+	uint32_t page_fault_error_code_match;
+	uint32_t cr3_target_count;
+	uint32_t vm_exit_controls;
+	uint32_t vm_exit_msr_store_count;
+	uint32_t vm_exit_msr_load_count;
+	uint32_t vm_entry_controls;
+	uint32_t vm_entry_msr_load_count;
+	uint32_t vm_entry_intr_info_field;
+	uint32_t vm_entry_exception_error_code;
+	uint32_t vm_entry_instruction_len;
+	uint32_t tpr_threshold;
+	uint32_t secondary_vm_exec_control;
+	uint32_t vm_instruction_error;
+	uint32_t vm_exit_reason;
+	uint32_t vm_exit_intr_info;
+	uint32_t vm_exit_intr_error_code;
+	uint32_t idt_vectoring_info_field;
+	uint32_t idt_vectoring_error_code;
+	uint32_t vm_exit_instruction_len;
+	uint32_t vmx_instruction_info;
+	uint32_t guest_es_limit;
+	uint32_t guest_cs_limit;
+	uint32_t guest_ss_limit;
+	uint32_t guest_ds_limit;
+	uint32_t guest_fs_limit;
+	uint32_t guest_gs_limit;
+	uint32_t guest_ldtr_limit;
+	uint32_t guest_tr_limit;
+	uint32_t guest_gdtr_limit;
+	uint32_t guest_idtr_limit;
+	uint32_t guest_es_ar_bytes;
+	uint32_t guest_cs_ar_bytes;
+	uint32_t guest_ss_ar_bytes;
+	uint32_t guest_ds_ar_bytes;
+	uint32_t guest_fs_ar_bytes;
+	uint32_t guest_gs_ar_bytes;
+	uint32_t guest_ldtr_ar_bytes;
+	uint32_t guest_tr_ar_bytes;
+	uint32_t guest_interruptibility_info;
+	uint32_t guest_activity_state;
+	uint32_t guest_sysenter_cs;
+	uint32_t host_ia32_sysenter_cs;
+	uint32_t vmx_preemption_timer_value;
+	uint32_t padding32[7]; /* room for future expansion */
+	uint16_t virtual_processor_id;
+	uint16_t posted_intr_nv;
+	uint16_t guest_es_selector;
+	uint16_t guest_cs_selector;
+	uint16_t guest_ss_selector;
+	uint16_t guest_ds_selector;
+	uint16_t guest_fs_selector;
+	uint16_t guest_gs_selector;
+	uint16_t guest_ldtr_selector;
+	uint16_t guest_tr_selector;
+	uint16_t guest_intr_status;
+	uint16_t host_es_selector;
+	uint16_t host_cs_selector;
+	uint16_t host_ss_selector;
+	uint16_t host_ds_selector;
+	uint16_t host_fs_selector;
+	uint16_t host_gs_selector;
+	uint16_t host_tr_selector;
+	uint16_t guest_pml_index;
+};
+
+
+static void write_address(uint64_t address, uint64_t size, uint64_t prot){
+	static uint64_t next_address = PAGETABLE_MASK;
+	static uint64_t last_address = 0x0; 
+	static uint64_t last_prot = 0;
+	if(address != next_address || prot != last_prot){
+		/* do not print guard pages or empty pages without any permissions */
+		if(last_address && (CHECK_BIT(last_prot, 1) || !CHECK_BIT(last_prot, 63))){
+			if(CHECK_BIT(last_prot, 1) && !CHECK_BIT(last_prot, 63)){
+				QEMU_PT_PRINTF(NESTED_VM_PREFIX, "%016lx - %016lx %c%c%c [WARNING]",
+					last_address, next_address,
+		            CHECK_BIT(last_prot, 1) ? 'W' : '-', 
+		            CHECK_BIT(last_prot, 2) ? 'U' : 'K', 
+		            !CHECK_BIT(last_prot, 63)? 'X' : '-');
+			}
+			else{
+				QEMU_PT_PRINTF(NESTED_VM_PREFIX, "%016lx - %016lx %c%c%c",
+					last_address, next_address,
+		            CHECK_BIT(last_prot, 1) ? 'W' : '-', 
+		            CHECK_BIT(last_prot, 2) ? 'U' : 'K', 
+		            !CHECK_BIT(last_prot, 63)? 'X' : '-');
+			}
+		}
+		last_address = address;
+	}
+	next_address = address+size;
+	last_prot = prot;
+	
+}
+
+void print_48_paging(uint64_t cr3){
+    uint64_t paging_entries_level_1[PENTRIES];
+    uint64_t paging_entries_level_2[PENTRIES];
+    uint64_t paging_entries_level_3[PENTRIES];
+    uint64_t paging_entries_level_4[PENTRIES];
+
+    uint64_t address_identifier_1, address_identifier_2, address_identifier_3, address_identifier_4;
+    uint32_t i1, i2, i3,i4;
+
+    cpu_physical_memory_rw((cr3&PAGETABLE_MASK), (uint8_t *) paging_entries_level_1, PPAGE_SIZE, false);
+    for(i1 = 0; i1 < 512; i1++){
+        if(paging_entries_level_1[i1]){
+            address_identifier_1 = ((uint64_t)i1) << PLEVEL_1_SHIFT;
+            if (i1 & SIGN_EXTEND_TRESHOLD){
+                address_identifier_1 |= SIGN_EXTEND;
+            }
+            if(CHECK_BIT(paging_entries_level_1[i1], 0)){ /* otherwise swapped out */ 
+                cpu_physical_memory_rw((paging_entries_level_1[i1]&PAGETABLE_MASK), (uint8_t *) paging_entries_level_2, PPAGE_SIZE, false);
+                for(i2 = 0; i2 < PENTRIES; i2++){
+                    if(paging_entries_level_2[i2]){
+                        address_identifier_2 = (((uint64_t)i2) << PLEVEL_2_SHIFT) + address_identifier_1;
+                        if (CHECK_BIT(paging_entries_level_2[i2], 0)){ /* otherwise swapped out */ 
+                            if((paging_entries_level_2[i2]&PAGETABLE_MASK) == (paging_entries_level_1[i1]&PAGETABLE_MASK)){
+                            		/* loop */
+                              	continue;
+                            }
+                            if (CHECK_BIT(paging_entries_level_2[i2], 7)){
+                                write_address(address_identifier_2, 0x40000000, (uint64_t)paging_entries_level_2[i2] & ((1ULL<<63) | (1ULL<<2) | (1ULL<<1)));
+                            }
+                            else{
+                                /* otherwise this PDPE references a 1GB page */
+                                cpu_physical_memory_rw((paging_entries_level_2[i2]&PAGETABLE_MASK), (uint8_t *) paging_entries_level_3, PPAGE_SIZE, false);
+																for(i3 = 0; i3 < PENTRIES; i3++){
+																		if(paging_entries_level_3[i3]){
+									    									address_identifier_3 = (((uint64_t)i3) << PLEVEL_3_SHIFT) + address_identifier_2;
+									    									if (CHECK_BIT(paging_entries_level_3[i3], 0)){ /* otherwise swapped out */ 
+									        									if (CHECK_BIT(paging_entries_level_3[i3], 7)){
+									        											write_address(address_identifier_3, 0x200000, (uint64_t)paging_entries_level_3[i3] & ((1ULL<<63) | (1ULL<<2) | (1ULL<<1)));
+									        									}
+									        									else{
+																								cpu_physical_memory_rw((paging_entries_level_3[i3]&PAGETABLE_MASK), (uint8_t *) paging_entries_level_4, PPAGE_SIZE, false);
+											    											for(i4 = 0; i4 < PENTRIES; i4++){
+											        											if(paging_entries_level_4[i4]){
+											            											address_identifier_4 = (((uint64_t)i4) << PLEVEL_4_SHIFT) + address_identifier_3;
+											            											if (CHECK_BIT(paging_entries_level_4[i4], 0)){
+											                											write_address(address_identifier_4, 0x1000, (uint64_t)paging_entries_level_4[i4] & ((1ULL<<63) | (1ULL<<2) | (1ULL<<1)));
+											            											}
+											        											}
+											   												}
+									        									}
+																				}
+																		}
+																}
+                            }
+                        }
+                    }
+                }
+            }
+        }
+    }
+		write_address(0, 0x1000, 0);
+}
+
+uint64_t get_nested_guest_rip(CPUState *cpu){
+
+	X86CPU *cpux86 = X86_CPU(cpu);
+	CPUX86State *env = &cpux86->env;
+
+	kvm_vcpu_ioctl(cpu, KVM_GET_NESTED_STATE, env->nested_state);
+	
+	struct vmcs12* saved_vmcs = (struct vmcs12*)&(env->nested_state->data);
+
+	return saved_vmcs->guest_rip;
+}
+
+uint64_t get_nested_host_rip(CPUState *cpu){
+
+	X86CPU *cpux86 = X86_CPU(cpu);
+	CPUX86State *env = &cpux86->env;
+
+	kvm_vcpu_ioctl(cpu, KVM_GET_NESTED_STATE, env->nested_state);
+	
+	struct vmcs12* saved_vmcs = (struct vmcs12*)&(env->nested_state->data);
+
+	return saved_vmcs->host_rip;
+}
+
+uint64_t get_nested_host_cr3(CPUState *cpu){
+
+	X86CPU *cpux86 = X86_CPU(cpu);
+	CPUX86State *env = &cpux86->env;
+
+	kvm_vcpu_ioctl(cpu, KVM_GET_NESTED_STATE, env->nested_state);
+	
+	struct vmcs12* saved_vmcs = (struct vmcs12*)&(env->nested_state->data);
+
+	return saved_vmcs->host_cr3;
+}
+
+void set_nested_rip(CPUState *cpu, uint64_t rip){
+
+	X86CPU *cpux86 = X86_CPU(cpu);
+	CPUX86State *env = &cpux86->env;
+
+	//kvm_vcpu_ioctl(cpu, KVM_GET_NESTED_STATE, env->nested_state);
+	
+	struct vmcs12* saved_vmcs = (struct vmcs12*)&(env->nested_state->data);
+
+	saved_vmcs->guest_rip = rip;
+
+	//return saved_vmcs->guest_rip;
+}
+
+void kvm_nested_get_info(CPUState *cpu){
+
+	X86CPU *cpux86 = X86_CPU(cpu);
+	CPUX86State *env = &cpux86->env;
+
+	kvm_vcpu_ioctl(cpu, KVM_GET_NESTED_STATE, env->nested_state);
+	
+	struct vmcs12* saved_vmcs = (struct vmcs12*)&(env->nested_state->data);
+	QEMU_PT_PRINTF(NESTED_VM_PREFIX, "VMCS host_cr3:\t%lx", saved_vmcs->host_cr3);
+	QEMU_PT_PRINTF(NESTED_VM_PREFIX, "VMCS host_cr4:\t%lx", saved_vmcs->host_cr4);
+	QEMU_PT_PRINTF(NESTED_VM_PREFIX, "VMCS host_ia32_efer:\t%lx", saved_vmcs->host_ia32_efer);
+	QEMU_PT_PRINTF(NESTED_VM_PREFIX, "VMCS host_cr0:\t%lx", saved_vmcs->host_cr0);
+
+	return;
+
+	//cpu->parent_cr3 = saved_vmcs->host_cr3+0x1000;
+	GET_GLOBAL_STATE()->parent_cr3 = saved_vmcs->host_cr3+0x1000;
+	fprintf(stderr, "saved_vmcs->guest_cr3: %lx %lx %lx\n", saved_vmcs->guest_cr3, saved_vmcs->host_cr3, env->cr[3]);
+	pt_set_cr3(cpu, saved_vmcs->host_cr3+0x1000, false); /* USERSPACE */
+	//pt_set_cr3(cpu, saved_vmcs->host_cr3+0x1000, false); /* KERNELSPACE (kpti fuzzing fix) (https://gruss.cc/files/kaiser.pdf)!!! */
+
+	/* let's modify page permissions of our CR3 referencing PTs */
+	//change_page_permissions(cpu->parent_cr3, cpu);
+
+    if (!(saved_vmcs->host_cr0 & CR0_PG_MASK)) {
+        printf("PG disabled\n");
+    }
+    else{
+    	if (saved_vmcs->host_cr4 & CR4_PAE_MASK) {
+	        if (saved_vmcs->host_ia32_efer & (1 << 10)) {
+	            if (saved_vmcs->host_cr0 & CR4_LA57_MASK) {
+	            	QEMU_PT_PRINTF(NESTED_VM_PREFIX, "mem_info_la57");
+	            	abort();
+	                //mem_info_la57(mon, env);
+	            } else {
+	            	QEMU_PT_PRINTF(NESTED_VM_PREFIX, " ==== L1 Page Tables ====");
+	            	print_48_paging(saved_vmcs->host_cr3);
+
+	            	if(saved_vmcs->ept_pointer){
+		            	QEMU_PT_PRINTF(NESTED_VM_PREFIX, " ==== L2 Page Tables ====");
+		            	print_48_paging(saved_vmcs->ept_pointer);
+		            }
+	                //mem_info_la48(mon, env);
+	            }
+	        } 
+	        else{
+	        	QEMU_PT_PRINTF(NESTED_VM_PREFIX, "mem_info_pae32");
+	        	abort();
+	            //mem_info_pae32(mon, env);
+	        }
+	    } 
+	    else {
+	    	QEMU_PT_PRINTF(NESTED_VM_PREFIX, "mem_info_32");
+	    	abort();
+	        //mem_info_32(mon, env);
+	    }
+    }
+}
+
+#define AREA_DESC_LEN                   256
+#define MAGIC_NUMBER                    0x41584548U
+
+typedef struct {
+        uint32_t base;
+        uint32_t size;
+				uint32_t virtual_base;
+        char desc[AREA_DESC_LEN];
+}area_t_export_t;
+
+typedef struct {
+        uint32_t magic;
+        uint8_t num_mmio_areas;
+        uint8_t num_io_areas;
+        uint8_t num_alloc_areas;
+        uint8_t padding;
+}config_t;
+
+void print_configuration(FILE *stream, void* configuration, size_t size){
+
+	fprintf(stream, "%s: size: %lx\n", __func__, size);
+	assert((size-sizeof(config_t))%sizeof(area_t_export_t) == 0);
+
+	assert(((config_t*)configuration)->magic == MAGIC_NUMBER);
+
+	fprintf(stream, "%s: num_mmio_areas: %x\n", __func__, ((config_t*)configuration)->num_mmio_areas);
+	fprintf(stream, "%s: num_io_areas: %x\n", __func__, ((config_t*)configuration)->num_io_areas);
+	fprintf(stream, "%s: num_alloc_areas: %x\n", __func__, ((config_t*)configuration)->num_alloc_areas);
+
+
+	for(int i = 0; i < ((config_t*)configuration)->num_mmio_areas; i++){
+	fprintf(stream, "\t-> MMIO: 0x%x (V: 0x%x) [0x%x]\t%s\n",       ((area_t_export_t*)(configuration+sizeof(config_t)))[i].base,
+																													((area_t_export_t*)(configuration+sizeof(config_t)))[i].virtual_base,
+																													((area_t_export_t*)(configuration+sizeof(config_t)))[i].size,
+																													((area_t_export_t*)(configuration+sizeof(config_t)))[i].desc );
+	}
+
+	for(int i = ((config_t*)configuration)->num_mmio_areas; i < (((config_t*)configuration)->num_mmio_areas+((config_t*)configuration)->num_io_areas); i++){
+	fprintf(stream, "\t->   IO: 0x%x [0x%x]\t%s\n",       ((area_t_export_t*)(configuration+sizeof(config_t)))[i].base,
+																													((area_t_export_t*)(configuration+sizeof(config_t)))[i].size,
+																													((area_t_export_t*)(configuration+sizeof(config_t)))[i].desc );
+	}
+}
\ No newline at end of file
diff --new-file -ur qemu/pt/kvm_nested.h QEMU-PT/pt/kvm_nested.h
--- qemu/pt/kvm_nested.h	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/kvm_nested.h	2021-08-24 21:54:55.942586446 +0200
@@ -0,0 +1,13 @@
+#pragma once
+#include "qemu/osdep.h"
+
+void print_48_paging(uint64_t cr3);
+void kvm_nested_get_info(CPUState *cpu);
+uint64_t get_nested_guest_rip(CPUState *cpu);
+uint64_t get_nested_host_rip(CPUState *cpu);
+
+
+uint64_t get_nested_host_cr3(CPUState *cpu);
+
+void set_nested_rip(CPUState *cpu, uint64_t rip);
+void print_configuration(FILE *stream, void* configuration, size_t size);
\ No newline at end of file
diff --new-file -ur qemu/pt/logger.c QEMU-PT/pt/logger.c
--- qemu/pt/logger.c	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/logger.c	2021-08-24 21:54:55.938586449 +0200
@@ -0,0 +1,113 @@
+/*
+
+Copyright (C) 2017 Sergej Schumilo
+
+This file is part of QEMU-PT (kAFL).
+
+QEMU-PT is free software: you can redistribute it and/or modify
+it under the terms of the GNU General Public License as published by
+the Free Software Foundation, either version 2 of the License, or
+(at your option) any later version.
+
+QEMU-PT is distributed in the hope that it will be useful,
+but WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with QEMU-PT.  If not, see <http://www.gnu.org/licenses/>.
+
+*/
+
+#include <stdio.h>
+#include <stdarg.h>
+#include <stdint.h>
+#include "pt/logger.h"
+
+#ifdef SAMPLE_RAW_SINGLE
+#define SAMPLE_RAW_SINGLE_TARGET "/dev/shm/kafl_pt"
+
+int sample_raw_single_id = 0;
+FILE* sample_raw_single_file = NULL;
+
+void init_sample_raw_single(uint32_t id){
+	sample_raw_single_id = id;
+	char name[256];
+	snprintf(name, 256, SAMPLE_RAW_SINGLE_TARGET);
+	if (sample_raw_single_file)
+		fclose(sample_raw_single_file);
+	sample_raw_single_file = fopen(name, "wb"); 
+}
+
+void sample_raw_single(void* buffer, int bytes){
+	if (sample_raw_single_file){
+		fwrite(buffer, sizeof(char), bytes, sample_raw_single_file);
+		fflush(sample_raw_single_file);
+	}
+}
+#endif
+
+#ifdef SAMPLE_RAW
+#define SAMPLE_RAW_TARGET "/tmp/traces/sample_raw_%d"
+
+int sample_raw_id = 0;
+FILE* sample_raw_file = NULL;
+
+void init_sample_raw(void){
+	char name[256];
+	snprintf(name, 256, SAMPLE_RAW_TARGET, sample_raw_id++);
+	if (sample_raw_file)
+		fclose(sample_raw_file);
+	sample_raw_file = fopen(name, "wb"); 
+}
+
+void sample_raw(void* buffer, int bytes){
+	if (sample_raw_file)
+		fwrite(buffer, sizeof(char), bytes, sample_raw_file);
+}
+#endif
+
+#ifdef SAMPLE_DECODED
+#define SAMPLE_DECODED_TARGET "/tmp/traces/sample_decoded_%d"
+
+int sample_decoded_id = 0;
+FILE* sample_decoded_file = NULL;
+
+void init_sample_decoded(void){
+	char name[256];
+	snprintf(name, 256, SAMPLE_DECODED_TARGET, sample_decoded_id++);
+	if (sample_decoded_file)
+		fclose(sample_decoded_file);
+	sample_decoded_file = fopen(name, "w"); 
+}
+
+void sample_decoded(uint64_t from,uint64_t to){
+	if (sample_decoded_file)
+		fprintf(sample_decoded_file, "%lx->%lx\n", from, to);
+}
+#endif
+
+#ifdef SAMPLE_DECODED_DETAILED
+#define SAMPLE_DETAILED_TARGET "/tmp/traces/sample_detailed_%d"
+
+int sample_detailed_id = 0;
+FILE* sample_detailed_file = NULL;
+
+void init_sample_decoded_detailed(void){
+	char name[256];
+	snprintf(name, 256, SAMPLE_DETAILED_TARGET, sample_detailed_id++);
+	if (sample_detailed_file)
+		fclose(sample_detailed_file);
+	sample_detailed_file = fopen(name, "w"); 
+}
+#endif
+
+void sample_decoded_detailed(const char *format, ...){
+	#ifdef SAMPLE_DECODED_DETAILED
+	va_list args;
+	va_start(args, format);
+	if (sample_detailed_file)
+		vfprintf(sample_detailed_file, format, args);
+	va_end(args);
+	#endif
+}
diff --new-file -ur qemu/pt/logger.h QEMU-PT/pt/logger.h
--- qemu/pt/logger.h	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/logger.h	2021-08-24 21:54:55.942586446 +0200
@@ -0,0 +1,69 @@
+/*
+
+Copyright (C) 2017 Sergej Schumilo
+
+This file is part of QEMU-PT (kAFL).
+
+QEMU-PT is free software: you can redistribute it and/or modify
+it under the terms of the GNU General Public License as published by
+the Free Software Foundation, either version 2 of the License, or
+(at your option) any later version.
+
+QEMU-PT is distributed in the hope that it will be useful,
+but WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with QEMU-PT.  If not, see <http://www.gnu.org/licenses/>.
+
+*/
+
+#ifndef LOGGER_H
+#define LOGGER_H
+
+	#define CREATE_VM_IMAGE
+	//#define SAMPLE_RAW
+	//#define SAMPLE_DECODED
+	//#define SAMPLE_DECODED_DETAILED
+	#define SAMPLE_RAW_SINGLE
+	
+	#ifdef CREATE_VM_IMAGE
+		#define DECODER_MEMORY_IMAGE "/tmp/data"
+	#endif
+
+	#ifdef SAMPLE_RAW_SINGLE
+		void init_sample_raw_single(uint32_t id);
+		void sample_raw_single(void* buffer, int bytes);
+	#endif
+	
+	#ifdef SAMPLE_RAW
+		void init_sample_raw(void);
+		void sample_raw(void* buffer, int bytes);
+	#endif
+
+	#ifdef SAMPLE_DECODED
+		void init_sample_decoded(void);
+		void sample_decoded(uint64_t from, uint64_t to);
+	#endif
+
+	#ifdef SAMPLE_DECODED_DETAILED
+		void init_sample_decoded_detailed(void);
+	#endif
+
+	void sample_decoded_detailed(const char *format, ...);
+
+#define UNUSED(x) (void)x;
+
+#ifdef SAMPLE_DECODED
+#define WRITE_SAMPLE_DECODED(addr) (sample_decoded(addr))
+#endif
+
+#ifdef SAMPLE_DECODED_DETAILED
+#define WRITE_SAMPLE_DECODED_DETAILED(format, ...) (sample_decoded_detailed(format, ##__VA_ARGS__))
+#else
+#define WRITE_SAMPLE_DECODED_DETAILED(format, ...)  (void)0
+#endif
+
+
+#endif
diff --new-file -ur qemu/pt/Makefile.objs QEMU-PT/pt/Makefile.objs
--- qemu/pt/Makefile.objs	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/Makefile.objs	2021-08-24 21:54:55.938586449 +0200
@@ -0,0 +1,22 @@
+obj-y += decoder.o disassembler.o tnt_cache.o hypercall.o logger.o memory_access.o interface.o fast_vm_reload.o state_reallocation.o synchronization.o block_cow.o
+obj-$(CONFIG_REDQUEEN) += redqueen.o patcher.o redqueen_patch.o file_helper.o redqueen_trace.o page_cache.o kvm_nested.o state.o debug.o auxiliary_buffer.o mmh3.o trace_cache.o nested_hypercalls.o sharedir.o helpers.o
+
+
+decoder.o-cflags 			:= -Ofast
+disassembler.o-cflags 		:= -Ofast -lcapstone
+tnt_cache.o-cflags 			:= -Ofast
+hypercall.o-cflags 			:= -Ofast
+logger.o-cflags			 	:= -Ofast
+memory_access.o-cflags 		:= -Ofast
+interface.o-cflags 			:= -Ofast
+fast_vm_reload.o-cflags 	:= -Ofast
+state_reallocation.o-cflags := -Ofast
+synchronization.o-cflags 	:= -Ofast
+redqueen.o-cflags 			:= -Ofast
+patcher.o-cflags 			:= -Ofast -lcapstone
+redqueen_patch.o-cflags 	:= -Ofast
+file_helper.o-cflags 		:= -Ofast
+redqueen_trace.o-cflags 	:= -Ofast
+page_cache.o-cflags 		:= -Ofast -lcapstone
+kvm_nested.o-cflags 		:= -Ofast
+block_cow.o-cflags 			:= -Ofast
\ No newline at end of file
diff --new-file -ur qemu/pt/memory_access.c QEMU-PT/pt/memory_access.c
--- qemu/pt/memory_access.c	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/memory_access.c	2021-08-24 21:54:55.938586449 +0200
@@ -0,0 +1,881 @@
+/*
+
+Copyright (C) 2017 Sergej Schumilo
+
+This file is part of QEMU-PT (kAFL).
+
+QEMU-PT is free software: you can redistribute it and/or modify
+it under the terms of the GNU General Public License as published by
+the Free Software Foundation, either version 2 of the License, or
+(at your option) any later version.
+
+QEMU-PT is distributed in the hope that it will be useful,
+but WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with QEMU-PT.  If not, see <http://www.gnu.org/licenses/>.
+
+*/
+#include <errno.h>
+#include "qemu/osdep.h"
+#include "sysemu/sysemu.h"
+#include "cpu.h"
+#include "exec/ram_addr.h"
+#include "qemu/rcu_queue.h"
+
+#include "memory_access.h"
+#include "hypercall.h"
+#include "debug.h"
+#include "pt/fast_vm_reload.h"
+#include "exec/gdbstub.h"
+#include "pt/state.h"
+#include "sysemu/kvm.h"
+#include "pt/disassembler.h"
+
+static uint64_t get_48_paging_phys_addr(uint64_t cr3, uint64_t addr);
+
+#define x86_64_PAGE_SIZE        0x1000
+#define x86_64_PAGE_MASK        ~(x86_64_PAGE_SIZE - 1)
+
+bool read_physical_memory(uint64_t address, uint8_t* data, uint32_t size, CPUState *cpu){
+    kvm_arch_get_registers(cpu);
+    cpu_physical_memory_read(address, data, size);
+    return true;
+}
+
+bool write_physical_memory(uint64_t address, uint8_t* data, uint32_t size, CPUState *cpu){
+    kvm_arch_get_registers(cpu);
+    cpu_physical_memory_write(address, data, size);
+    return true;
+}
+
+int shared_payload_buffer_fd = 0;
+uint32_t shared_payload_buffer_size = 0;
+
+static void refresh_kvm(CPUState *cpu){
+    if (!cpu->vcpu_dirty) {
+        kvm_arch_get_registers(cpu);
+    }
+}
+
+static void refresh_kvm_non_dirty(CPUState *cpu){
+    if (!cpu->vcpu_dirty) {
+        kvm_arch_get_registers_fast(cpu);
+    }
+}
+
+void set_payload_buffer_mapping_fd(int fd, uint32_t size){
+    shared_payload_buffer_fd = fd;
+    shared_payload_buffer_size = size;
+}
+
+
+uint8_t* buffer = NULL; 
+
+bool remap_payload_slot(uint64_t phys_addr, uint32_t slot, CPUState *cpu){
+    assert(shared_payload_buffer_fd && shared_payload_buffer_size);
+    RAMBlock *block;
+    refresh_kvm_non_dirty(cpu);
+
+    uint32_t i = slot;
+
+    QLIST_FOREACH_RCU(block, &ram_list.blocks, next) {
+        if(!memcmp(block->idstr, "pc.ram", 6)){
+            assert(munmap((void*)(((uint64_t)block->host) + phys_addr), x86_64_PAGE_SIZE) != -1);
+            assert(mmap((void*)(((uint64_t)block->host) + phys_addr), 0x1000, PROT_READ | PROT_WRITE, MAP_SHARED | MAP_FIXED, shared_payload_buffer_fd, (i*x86_64_PAGE_SIZE)) != (void*)-1);
+
+            if(i == 0){
+                buffer = (uint8_t*)(((uint64_t)block->host) + phys_addr);
+            }
+            fast_reload_blacklist_page(get_fast_reload_snapshot(), phys_addr);
+            break;
+        }
+    }
+    
+    return true;
+}
+
+
+
+bool remap_payload_slot_protected(uint64_t phys_addr, uint32_t slot, CPUState *cpu){
+
+    assert(shared_payload_buffer_fd && shared_payload_buffer_size);
+    RAMBlock *block;
+    refresh_kvm_non_dirty(cpu);
+
+    uint32_t i = slot;
+
+    QLIST_FOREACH_RCU(block, &ram_list.blocks, next) {
+        if(!memcmp(block->idstr, "pc.ram", 6)){
+            assert(munmap((void*)(((uint64_t)block->host) + phys_addr), x86_64_PAGE_SIZE) != -1);
+            assert(mmap((void*)(((uint64_t)block->host) + phys_addr), 0x1000, PROT_READ , MAP_SHARED | MAP_FIXED, shared_payload_buffer_fd, (i*x86_64_PAGE_SIZE))!= (void*) -1);
+
+            if(i == 0){
+                buffer = (uint8_t*)(((uint64_t)block->host) + phys_addr);
+            }
+            fast_reload_blacklist_page(get_fast_reload_snapshot(), phys_addr);
+            break;
+        }
+    }
+    
+    return true;
+}
+
+bool remap_payload_buffer(uint64_t virt_guest_addr, CPUState *cpu){
+    assert(shared_payload_buffer_fd && shared_payload_buffer_size);
+    RAMBlock *block;
+    refresh_kvm_non_dirty(cpu);
+
+
+    for(uint32_t i = 0; i < (shared_payload_buffer_size/x86_64_PAGE_SIZE); i++){
+        uint64_t phys_addr = get_48_paging_phys_addr(GET_GLOBAL_STATE()->parent_cr3, ((virt_guest_addr+(i*x86_64_PAGE_SIZE)) & x86_64_PAGE_MASK));
+
+        assert(phys_addr != 0xFFFFFFFFFFFFFFFFULL);
+
+        QLIST_FOREACH_RCU(block, &ram_list.blocks, next) {
+            if(!memcmp(block->idstr, "pc.ram", 6)){
+                if(munmap((void*)(((uint64_t)block->host) + phys_addr), x86_64_PAGE_SIZE) == -1){
+                    fprintf(stderr, "munmap failed!\n");
+                    //exit(1);
+                    assert(false);
+                }
+
+                if(mmap((void*)(((uint64_t)block->host) + phys_addr), 0x1000, PROT_READ | PROT_WRITE, MAP_SHARED | MAP_FIXED, shared_payload_buffer_fd, (i*x86_64_PAGE_SIZE)) == MAP_FAILED){
+                    fprintf(stderr, "mmap failed!\n");
+                    //exit(1);
+                    assert(false);
+                }
+
+                if(i == 0){
+                    buffer = (uint8_t*)(((uint64_t)block->host) + phys_addr);
+                }
+                fast_reload_blacklist_page(get_fast_reload_snapshot(), phys_addr);
+                break;
+            }
+        }
+    }
+    return true;
+}
+
+bool write_virtual_memory_cr3(uint64_t address, uint8_t* data, uint32_t size, CPUState *cpu, uint64_t cr3){
+    CPUX86State *env = &(X86_CPU(cpu))->env;
+    uint64_t old_cr3 = 0;
+    bool return_value = false;
+
+
+    uint64_t old_cr4 = 0;
+    uint64_t old_hflags = 0;
+
+    refresh_kvm(cpu);
+
+    old_cr3 = env->cr[3];
+    env->cr[3] = cr3;
+
+    old_cr4 = env->cr[4];
+    env->cr[4] = CR4_PAE_MASK | old_cr4;
+
+    old_hflags = env->hflags;
+    env->hflags = HF_LMA_MASK | old_hflags;
+    return_value = write_virtual_memory(address, data, size, cpu);
+    env->cr[3] = old_cr3;
+    env->cr[4] = old_cr4;
+    env->hflags = old_hflags;
+
+
+    return return_value;
+}
+
+bool write_virtual_shadow_memory_cr3(uint64_t address, uint8_t* data, uint32_t size, CPUState *cpu, uint64_t cr3){
+    fprintf(stderr, "%s\n", __func__);
+    CPUX86State *env = &(X86_CPU(cpu))->env;
+    uint64_t old_cr3 = 0;
+    bool return_value = false;
+    uint64_t old_cr4 = 0;
+    uint64_t old_hflags = 0;
+
+    refresh_kvm(cpu);
+     old_cr3 = env->cr[3];
+    env->cr[3] = cr3;
+
+    old_cr4 = env->cr[4];
+    env->cr[4] = CR4_PAE_MASK | old_cr4;
+
+    old_hflags = env->hflags;
+    env->hflags = HF_LMA_MASK | old_hflags;
+    return_value = write_virtual_shadow_memory(address, data, size, cpu);
+    env->cr[3] = old_cr3;
+    env->cr[4] = old_cr4;
+    env->hflags = old_hflags;
+
+    return return_value;
+}
+
+bool write_virtual_memory(uint64_t address, uint8_t* data, uint32_t size, CPUState *cpu)
+{
+    /* Todo: later &address_space_memory + phys_addr -> mmap SHARED */
+    int asidx;
+    MemTxAttrs attrs;
+    hwaddr phys_addr;
+    MemTxResult res;
+
+    uint64_t counter, l, i;
+
+    counter = size;
+    while(counter != 0){
+        l = x86_64_PAGE_SIZE;
+        if (l > counter)
+            l = counter;
+
+        refresh_kvm(cpu);
+        //cpu_synchronize_state(cpu);
+        asidx = cpu_asidx_from_attrs(cpu, MEMTXATTRS_UNSPECIFIED);
+        attrs = MEMTXATTRS_UNSPECIFIED;
+        phys_addr = cpu_get_phys_page_attrs_debug(cpu, (address & x86_64_PAGE_MASK), &attrs);
+
+        if (phys_addr == -1){
+            QEMU_PT_PRINTF(MEM_PREFIX, "phys_addr == -1:\t%lx", address);
+            return false;
+        }
+        
+        phys_addr += (address & ~x86_64_PAGE_MASK);   
+        res = address_space_rw(cpu_get_address_space(cpu, asidx), phys_addr, MEMTXATTRS_UNSPECIFIED, data, l, true);
+        if (res != MEMTX_OK){
+            QEMU_PT_PRINTF(MEM_PREFIX, "!MEMTX_OK:\t%lx", address);
+            return false;
+        }   
+
+        i++;
+        data += l;
+        address += l;
+        counter -= l;
+    }
+
+    return true;
+}
+
+
+void hexdump_virtual_memory(uint64_t address, uint32_t size, CPUState *cpu){
+    assert(size < 0x100000); // 1MB max 
+    uint64_t i = 0;
+    uint8_t tmp[17];
+    uint8_t* data = malloc(size);
+    bool success = read_virtual_memory(address, data, size, cpu);
+
+    if(success){
+        for (i = 0; i < size; i++){
+            if(!(i % 16)){
+                if (i != 0){
+                    printf ("  %s\n", tmp);
+                }
+                printf ("  %04lx ", i);
+            }
+            printf (" %02x", data[i]);
+
+            if ((data[i] < 0x20) || (data[i] > 0x7e))
+                tmp[i % 16] = '.';
+            else
+                tmp[i % 16] = data[i];
+            tmp[(i % 16) + 1] = '\0';
+        }
+
+        while ((i % 16) != 0) {
+            printf ("   ");
+            i++;
+        }
+        printf ("  %s\n", tmp);
+    }
+
+    free(data);
+}
+
+
+bool write_virtual_shadow_memory(uint64_t address, uint8_t* data, uint32_t size, CPUState *cpu)
+{
+    fprintf(stderr, "%s\n", __func__);
+    /* Todo: later &address_space_memory + phys_addr -> mmap SHARED */
+    int asidx;
+    MemTxAttrs attrs;
+    hwaddr phys_addr;
+    MemTxResult res;
+
+    uint64_t counter, l, i;
+
+    void* shadow_memory = NULL;
+
+    counter = size;
+    while(counter != 0){
+        l = x86_64_PAGE_SIZE;
+        if (l > counter)
+            l = counter;
+
+        refresh_kvm(cpu);
+        kvm_cpu_synchronize_state(cpu);
+        asidx = cpu_asidx_from_attrs(cpu, MEMTXATTRS_UNSPECIFIED);
+        attrs = MEMTXATTRS_UNSPECIFIED;
+        phys_addr = cpu_get_phys_page_attrs_debug(cpu, (address & x86_64_PAGE_MASK), &attrs);
+
+        if (phys_addr == -1){
+            QEMU_PT_PRINTF(MEM_PREFIX, "phys_addr == -1:\t%lx", address);
+            return false;
+        }
+        
+        res = address_space_rw(cpu_get_address_space(cpu, asidx), (phys_addr + (address & ~x86_64_PAGE_MASK)), MEMTXATTRS_UNSPECIFIED, data, l, true);
+        if (res != MEMTX_OK){
+            QEMU_PT_PRINTF(MEM_PREFIX, "!MEMTX_OK:\t%lx", address);
+            return false;
+        }   
+
+        shadow_memory = fast_reload_get_physmem_shadow_ptr(get_fast_reload_snapshot(), phys_addr);
+        if (shadow_memory){
+              memcpy(shadow_memory + (address & ~x86_64_PAGE_MASK), data, l);
+        }
+        else{
+            QEMU_PT_PRINTF(MEM_PREFIX, "get_physmem_shadow_ptr(%lx) == NULL", phys_addr);
+            assert(false);
+            return false;
+        }
+
+        phys_addr += (address & ~x86_64_PAGE_MASK);   
+
+
+        i++;
+        data += l;
+        address += l;
+        counter -= l;
+    }
+
+    return true;
+}
+
+static int redqueen_insert_sw_breakpoint(CPUState *cs, struct kvm_sw_breakpoint *bp)
+{
+    static const uint8_t int3 = 0xcc;
+
+    hwaddr phys_addr = (hwaddr) get_48_paging_phys_addr(GET_GLOBAL_STATE()->parent_cr3, bp->pc);
+    int asidx = cpu_asidx_from_attrs(cs, MEMTXATTRS_UNSPECIFIED);
+
+    if (address_space_rw(cpu_get_address_space(cs, asidx), phys_addr, MEMTXATTRS_UNSPECIFIED, (uint8_t *)&bp->saved_insn, 1, 0) ||
+        address_space_rw(cpu_get_address_space(cs, asidx), phys_addr, MEMTXATTRS_UNSPECIFIED, (uint8_t *)&int3, 1, 1)) {
+        //fprintf(stderr, "%s WRITE AT %lx %lx failed!\n", __func__, bp->pc, phys_addr);
+        return -EINVAL;
+    }
+
+    return 0;
+}
+
+static int redqueen_remove_sw_breakpoint(CPUState *cs, struct kvm_sw_breakpoint *bp)
+{
+    uint8_t int3;
+
+    hwaddr phys_addr = (hwaddr) get_48_paging_phys_addr(GET_GLOBAL_STATE()->parent_cr3, bp->pc);
+    int asidx = cpu_asidx_from_attrs(cs, MEMTXATTRS_UNSPECIFIED);
+
+    if (address_space_rw(cpu_get_address_space(cs, asidx), phys_addr, MEMTXATTRS_UNSPECIFIED, (uint8_t *)&int3, 1, 0) || int3 != 0xcc ||
+        address_space_rw(cpu_get_address_space(cs, asidx), phys_addr, MEMTXATTRS_UNSPECIFIED, (uint8_t *)&bp->saved_insn, 1, 1)) {
+        //fprintf(stderr, "%s failed\n", __func__);
+        return -EINVAL;
+    }
+
+    return 0;
+}
+
+static struct kvm_sw_breakpoint *redqueen_find_breakpoint(CPUState *cpu, target_ulong pc){
+    struct kvm_sw_breakpoint *bp;
+
+    QTAILQ_FOREACH(bp, &GET_GLOBAL_STATE()->redqueen_breakpoints, entry) {
+        if (bp->pc == pc) {
+            return bp;
+        }
+    }
+    return NULL;
+}
+
+static int redqueen_breakpoints_active(CPUState *cpu){
+    return !QTAILQ_EMPTY(&GET_GLOBAL_STATE()->redqueen_breakpoints);
+}
+
+struct kvm_set_guest_debug_data {
+    struct kvm_guest_debug dbg;
+    int err;
+};
+
+static int redqueen_update_guest_debug(CPUState *cpu) {
+    struct kvm_set_guest_debug_data data;
+
+    data.dbg.control = 0;
+
+    if (redqueen_breakpoints_active(cpu)) {
+        data.dbg.control |= KVM_GUESTDBG_ENABLE | KVM_GUESTDBG_USE_SW_BP;
+    }
+
+    return kvm_vcpu_ioctl(cpu, KVM_SET_GUEST_DEBUG, &data.dbg);
+
+    return 0;
+}
+
+static void redqueen_remove_all_breakpoints(CPUState *cpu) {
+    struct kvm_sw_breakpoint *bp, *next;
+
+    QTAILQ_FOREACH_SAFE(bp, &GET_GLOBAL_STATE()->redqueen_breakpoints, entry, next) {
+        redqueen_remove_sw_breakpoint(cpu, bp);
+        QTAILQ_REMOVE(&GET_GLOBAL_STATE()->redqueen_breakpoints, bp, entry);
+        g_free(bp);
+    }
+
+    redqueen_update_guest_debug(cpu);
+}
+
+static int redqueen_insert_breakpoint(CPUState *cpu, target_ulong addr, target_ulong len){
+    struct kvm_sw_breakpoint *bp;
+    int err;
+
+    bp = redqueen_find_breakpoint(cpu, addr);
+    if (bp) {
+        bp->use_count++;
+        return 0;
+    }
+
+    bp = g_malloc(sizeof(struct kvm_sw_breakpoint));
+    bp->pc = addr;
+    bp->use_count = 1;
+
+    err = redqueen_insert_sw_breakpoint(cpu, bp);
+    if (err) {
+        g_free(bp);
+        return err;
+    }
+
+    QTAILQ_INSERT_HEAD(&GET_GLOBAL_STATE()->redqueen_breakpoints, bp, entry);
+    
+    err = redqueen_update_guest_debug(cpu);
+    if(err){
+        return err;
+    }
+
+    return 0;
+}
+
+static int redqueen_remove_breakpoint(CPUState *cpu, target_ulong addr, target_ulong len){
+    struct kvm_sw_breakpoint *bp;
+    int err;
+
+    bp = redqueen_find_breakpoint(cpu, addr);
+    if (!bp) {
+        return -ENOENT;
+    }
+
+    if (bp->use_count > 1) {
+        bp->use_count--;
+        return 0;
+    }
+
+    err = redqueen_remove_sw_breakpoint(cpu, bp);
+    if (err) {
+        return err;
+    }
+
+    QTAILQ_REMOVE(&GET_GLOBAL_STATE()->redqueen_breakpoints, bp, entry);
+    g_free(bp);
+    
+    err = redqueen_update_guest_debug(cpu);
+    if(err){
+        return err;
+    }
+
+    return 0;
+}
+
+int insert_breakpoint(CPUState *cpu, uint64_t addr, uint64_t len){
+    redqueen_insert_breakpoint(cpu, addr, len);
+    redqueen_update_guest_debug(cpu);
+    return 0;
+}
+
+
+int remove_breakpoint(CPUState *cpu, uint64_t addr, uint64_t len){
+    //fprintf(stderr, "%s %lx\n", __func__, addr);
+    redqueen_remove_breakpoint(cpu, addr, len);
+    redqueen_update_guest_debug(cpu);
+    return 0;
+}
+
+void remove_all_breakpoints(CPUState *cpu){
+    redqueen_remove_all_breakpoints(cpu);
+}
+
+#define PPAGE_SIZE 0x1000
+#define PENTRIES 0x200
+#define PLEVEL_4_SHIFT 12
+#define PLEVEL_3_SHIFT 21
+#define PLEVEL_2_SHIFT 30
+#define PLEVEL_1_SHIFT 39
+#define SIGN_EXTEND_TRESHOLD 0x100
+#define SIGN_EXTEND 0xFFFF000000000000ULL
+#define PAGETABLE_MASK 0x1FFFFFFFFF000ULL
+#define PML4_ENTRY_MASK 0x1FFFFFFFFF000ULL
+#define PML3_ENTRY_MASK 0x1FFFFC0000000ULL
+#define PML2_ENTRY_MASK 0x1FFFFFFE00000ULL
+
+#define CHECK_BIT(var,pos) !!(((var) & (1ULL<<(pos))))
+
+
+static void write_address(uint64_t address, uint64_t size, uint64_t prot){
+    //fprintf(stderr, "%s %lx\n", __func__, address);
+    static uint64_t next_address = PAGETABLE_MASK;
+    static uint64_t last_address = 0x0; 
+    static uint64_t last_prot = 0;
+    if(address != next_address || prot != last_prot){
+        /* do not print guard pages or empty pages without any permissions */
+        if(last_address && (CHECK_BIT(last_prot, 1) || !CHECK_BIT(last_prot, 63))){
+            if(CHECK_BIT(last_prot, 1) && !CHECK_BIT(last_prot, 63)){
+                fprintf(stderr, "%016lx - %016lx %c%c%c [WARNING]\n",
+                    last_address, next_address,
+                    CHECK_BIT(last_prot, 1) ? 'W' : '-', 
+                    CHECK_BIT(last_prot, 2) ? 'U' : 'K', 
+                    !CHECK_BIT(last_prot, 63)? 'X' : '-');
+            }
+            else{
+                fprintf(stderr, "%016lx - %016lx %c%c%c\n",
+                    last_address, next_address,
+                    CHECK_BIT(last_prot, 1) ? 'W' : '-', 
+                    CHECK_BIT(last_prot, 2) ? 'U' : 'K', 
+                    !CHECK_BIT(last_prot, 63)? 'X' : '-');
+            }
+        }
+        last_address = address;
+    }
+    next_address = address+size;
+    last_prot = prot;
+    
+}
+
+void print_48_paging2(uint64_t cr3){
+    uint64_t paging_entries_level_1[PENTRIES];
+    uint64_t paging_entries_level_2[PENTRIES];
+    uint64_t paging_entries_level_3[PENTRIES];
+    uint64_t paging_entries_level_4[PENTRIES];
+
+    uint64_t address_identifier_1, address_identifier_2, address_identifier_3, address_identifier_4;
+    uint32_t i1, i2, i3,i4;
+
+    cpu_physical_memory_rw((cr3&PAGETABLE_MASK), (uint8_t *) paging_entries_level_1, PPAGE_SIZE, false);
+    for(i1 = 0; i1 < 512; i1++){
+        if(paging_entries_level_1[i1]){
+            address_identifier_1 = ((uint64_t)i1) << PLEVEL_1_SHIFT;
+            if (i1 & SIGN_EXTEND_TRESHOLD){
+                address_identifier_1 |= SIGN_EXTEND;
+            }
+            if(CHECK_BIT(paging_entries_level_1[i1], 0)){ /* otherwise swapped out */ 
+                cpu_physical_memory_rw((paging_entries_level_1[i1]&PAGETABLE_MASK), (uint8_t *) paging_entries_level_2, PPAGE_SIZE, false);
+                for(i2 = 0; i2 < PENTRIES; i2++){
+                    if(paging_entries_level_2[i2]){
+                        address_identifier_2 = (((uint64_t)i2) << PLEVEL_2_SHIFT) + address_identifier_1;
+                        if (CHECK_BIT(paging_entries_level_2[i2], 0)){ /* otherwise swapped out */ 
+                            if((paging_entries_level_2[i2]&PAGETABLE_MASK) == (paging_entries_level_1[i1]&PAGETABLE_MASK)){
+                                /* loop */
+                                continue;
+                            }
+
+                            if (CHECK_BIT(paging_entries_level_2[i2], 7)){
+                                    write_address(address_identifier_2, 0x40000000, (uint64_t)paging_entries_level_2[i2] & ((1ULL<<63) | (1ULL<<2) | (1ULL<<1)));
+                            }
+                            else{
+                                /* otherwise this PDPE references a 1GB page */
+                                cpu_physical_memory_rw((paging_entries_level_2[i2]&PAGETABLE_MASK), (uint8_t *) paging_entries_level_3, PPAGE_SIZE, false);
+                                for(i3 = 0; i3 < PENTRIES; i3++){
+                                    if(paging_entries_level_3[i3]){
+                                        address_identifier_3 = (((uint64_t)i3) << PLEVEL_3_SHIFT) + address_identifier_2;
+                                        if (CHECK_BIT(paging_entries_level_3[i3], 0)){ /* otherwise swapped out */ 
+                                            if (CHECK_BIT(paging_entries_level_3[i3], 7)){
+                                                write_address(address_identifier_3, 0x200000, (uint64_t)paging_entries_level_3[i3] & ((1ULL<<63) | (1ULL<<2) | (1ULL<<1)));
+                                            }
+                                            else{
+                                                cpu_physical_memory_rw((paging_entries_level_3[i3]&PAGETABLE_MASK), (uint8_t *) paging_entries_level_4, PPAGE_SIZE, false);
+                                                for(i4 = 0; i4 < PENTRIES; i4++){
+                                                    if(paging_entries_level_4[i4]){
+                                                        address_identifier_4 = (((uint64_t)i4) << PLEVEL_4_SHIFT) + address_identifier_3;
+                                                        if (CHECK_BIT(paging_entries_level_4[i4], 0)){
+                                                            write_address(address_identifier_4, 0x1000, (uint64_t)paging_entries_level_4[i4] & ((1ULL<<63) | (1ULL<<2) | (1ULL<<1)));
+                                                        }
+                                                    }
+                                                }
+                                            }
+                                        }
+                                    }
+                                }
+                            }
+                        }
+                    }
+                }
+            }
+        }
+    }
+    write_address(0, 0x1000, 0);
+}
+
+
+static uint64_t get_48_paging_phys_addr(uint64_t cr3, uint64_t addr){
+    static int once = 0;
+    if(once){
+        print_48_paging2(cr3);
+        once = 0;
+    }
+
+    uint16_t pml_4_index = (addr & 0xFF8000000000ULL) >> 39;
+    uint16_t pml_3_index = (addr & 0x0007FC0000000UL) >> 30;
+    uint16_t pml_2_index = (addr & 0x000003FE00000UL) >> 21;
+    uint16_t pml_1_index = (addr & 0x00000001FF000UL) >> 12;
+
+    uint64_t address_identifier_4;
+    uint64_t paging_entries_buffer[PENTRIES];
+
+    cpu_physical_memory_rw((cr3&PAGETABLE_MASK), (uint8_t *) paging_entries_buffer, PPAGE_SIZE, false);
+    if(paging_entries_buffer[pml_4_index]){
+        address_identifier_4 = ((uint64_t)pml_4_index) << PLEVEL_1_SHIFT;
+        if (pml_4_index & SIGN_EXTEND_TRESHOLD){
+            address_identifier_4 |= SIGN_EXTEND;
+        }
+        if(CHECK_BIT(paging_entries_buffer[pml_4_index], 0)){ /* otherwise swapped out */ 
+            cpu_physical_memory_rw((paging_entries_buffer[pml_4_index]&PAGETABLE_MASK), (uint8_t *) paging_entries_buffer, PPAGE_SIZE, false);
+            if(paging_entries_buffer[pml_3_index]){
+
+                if (CHECK_BIT(paging_entries_buffer[pml_3_index], 0)){ /* otherwise swapped out */ 
+
+                    if (CHECK_BIT(paging_entries_buffer[pml_3_index], 7)){
+                        /* 1GB PAGE */
+                        return (paging_entries_buffer[pml_3_index] & PML3_ENTRY_MASK) | (0x7FFFFFFF & addr); 
+                    }
+                    else{
+                        cpu_physical_memory_rw((paging_entries_buffer[pml_3_index]&PAGETABLE_MASK), (uint8_t *) paging_entries_buffer, PPAGE_SIZE, false);
+                        if(paging_entries_buffer[pml_2_index]){
+                            if (CHECK_BIT(paging_entries_buffer[pml_2_index], 0)){ /* otherwise swapped out */ 
+                                if (CHECK_BIT(paging_entries_buffer[pml_2_index], 7)){
+                                    /* 2MB PAGE */
+                                    return (paging_entries_buffer[pml_2_index] & PML2_ENTRY_MASK) | (0x3FFFFF & addr); 
+                                }
+                                else{
+                                    cpu_physical_memory_rw((paging_entries_buffer[pml_2_index]&PAGETABLE_MASK), (uint8_t *) paging_entries_buffer, PPAGE_SIZE, false);
+                                    if(paging_entries_buffer[pml_1_index]){
+                                        if (CHECK_BIT(paging_entries_buffer[pml_1_index], 0)){
+                                            /* 4 KB PAGE */
+                                            return (paging_entries_buffer[pml_1_index] & PML4_ENTRY_MASK) | (0xFFF & addr); 
+                                        }
+                                    }
+                                }
+                            }
+                        }
+                    }
+                }
+            }
+        }
+    }
+    return 0xFFFFFFFFFFFFFFFFULL; /* invalid */
+}
+
+static uint64_t get_48_paging_phys_addr_snapshot(uint64_t cr3, uint64_t addr){
+
+    uint16_t pml_4_index = (addr & 0xFF8000000000ULL) >> 39;
+    uint16_t pml_3_index = (addr & 0x0007FC0000000UL) >> 30;
+    uint16_t pml_2_index = (addr & 0x000003FE00000UL) >> 21;
+    uint16_t pml_1_index = (addr & 0x00000001FF000UL) >> 12;
+
+    fast_reload_t* snapshot = get_fast_reload_snapshot();
+
+    uint64_t address_identifier_4;
+    uint64_t paging_entries_buffer[PENTRIES];
+
+    read_snapshot_memory(snapshot, (cr3&PAGETABLE_MASK), (uint8_t *) paging_entries_buffer, PPAGE_SIZE);   
+    //cpu_physical_memory_rw((cr3&PAGETABLE_MASK), (uint8_t *) paging_entries_buffer, PPAGE_SIZE, false);
+    if(paging_entries_buffer[pml_4_index]){
+        address_identifier_4 = ((uint64_t)pml_4_index) << PLEVEL_1_SHIFT;
+        if (pml_4_index & SIGN_EXTEND_TRESHOLD){
+            address_identifier_4 |= SIGN_EXTEND;
+        }
+        if(CHECK_BIT(paging_entries_buffer[pml_4_index], 0)){ /* otherwise swapped out */ 
+            read_snapshot_memory(snapshot, (paging_entries_buffer[pml_4_index]&PAGETABLE_MASK), (uint8_t *) paging_entries_buffer, PPAGE_SIZE);   
+            //cpu_physical_memory_rw((paging_entries_buffer[pml_4_index]&PAGETABLE_MASK), (uint8_t *) paging_entries_buffer, PPAGE_SIZE, false);
+            if(paging_entries_buffer[pml_3_index]){
+
+                //address_identifier_3 = (((uint64_t)pml_3_index) << PLEVEL_2_SHIFT) + address_identifier_4;
+                if (CHECK_BIT(paging_entries_buffer[pml_3_index], 0)){ /* otherwise swapped out */ 
+
+                    if (CHECK_BIT(paging_entries_buffer[pml_3_index], 7)){
+                        /* 1GB PAGE */
+                        return (paging_entries_buffer[pml_3_index] & PML3_ENTRY_MASK) | (0x7FFFFFFF & addr); 
+                    }
+                    else{
+                        read_snapshot_memory(snapshot, (paging_entries_buffer[pml_3_index]&PAGETABLE_MASK), (uint8_t *) paging_entries_buffer, PPAGE_SIZE);   
+                        //cpu_physical_memory_rw((paging_entries_buffer[pml_3_index]&PAGETABLE_MASK), (uint8_t *) paging_entries_buffer, PPAGE_SIZE, false);
+                        if(paging_entries_buffer[pml_2_index]){
+                            //address_identifier_2 = (((uint64_t)pml_2_index) << PLEVEL_3_SHIFT) + address_identifier_3;
+                            if (CHECK_BIT(paging_entries_buffer[pml_2_index], 0)){ /* otherwise swapped out */ 
+                                if (CHECK_BIT(paging_entries_buffer[pml_2_index], 7)){
+                                    /* 2MB PAGE */
+                                    return (paging_entries_buffer[pml_2_index] & PML2_ENTRY_MASK) | (0x3FFFFF & addr); 
+                                }
+                                else{
+                                    read_snapshot_memory(snapshot, (paging_entries_buffer[pml_2_index]&PAGETABLE_MASK), (uint8_t *) paging_entries_buffer, PPAGE_SIZE);   
+                                    //cpu_physical_memory_rw((paging_entries_buffer[pml_2_index]&PAGETABLE_MASK), (uint8_t *) paging_entries_buffer, PPAGE_SIZE, false);
+                                    if(paging_entries_buffer[pml_1_index]){
+                                        //address_identifier_1 = (((uint64_t)pml_1_index) << PLEVEL_4_SHIFT) + address_identifier_2;
+                                        if (CHECK_BIT(paging_entries_buffer[pml_1_index], 0)){
+                                            /* 4 KB PAGE */
+                                            return (paging_entries_buffer[pml_1_index] & PML4_ENTRY_MASK) | (0xFFF & addr); 
+                            
+                                        }
+                                    }
+                                }
+                            }
+                        }
+                    }
+                }
+            }
+        }
+    }
+    
+    fprintf(stderr, "FAILED: %s %lx\n", __func__, addr);
+    return 0xFFFFFFFFFFFFFFFFULL; /* invalid */
+}
+
+bool read_virtual_memory(uint64_t address, uint8_t* data, uint32_t size, CPUState *cpu){
+    uint8_t tmp_buf[x86_64_PAGE_SIZE];
+    hwaddr phys_addr;
+    int asidx;
+    
+    uint64_t amount_copied = 0;
+    
+    kvm_arch_get_registers_fast(cpu);
+    CPUX86State *env = &(X86_CPU(cpu))->env;
+
+    // copy per page 
+    while(amount_copied < size){
+        uint64_t len_to_copy = (size - amount_copied);
+        if(len_to_copy > x86_64_PAGE_SIZE)
+            len_to_copy = x86_64_PAGE_SIZE;
+
+        asidx = cpu_asidx_from_attrs(cpu, MEMTXATTRS_UNSPECIFIED);
+#ifdef DEBUG_48BIT_WALK
+        phys_addr_2 = cpu_get_phys_page_attrs_debug(cpu, (address & x86_64_PAGE_MASK), &attrs);
+#endif
+        phys_addr = (hwaddr)get_48_paging_phys_addr(env->cr[3], address) & 0xFFFFFFFFFFFFF000ULL;// != 0xFFFFFFFFFFFFFFFFULL)
+
+#ifdef DEBUG_48BIT_WALK
+        assert(phys_addr == phys_addr_2);
+#endif
+
+        if (phys_addr == 0xFFFFFFFFFFFFFFFFULL){
+            uint64_t next_page = (address & x86_64_PAGE_MASK) + x86_64_PAGE_SIZE;
+            uint64_t len_skipped =next_page-address;  
+            if(len_skipped > size-amount_copied){
+                len_skipped = size-amount_copied;
+            }
+
+            fprintf(stderr, "Warning, read from unmapped memory:\t%lx, skipping to %lx", address, next_page);
+            QEMU_PT_PRINTF(MEM_PREFIX, "Warning, read from unmapped memory:\t%lx, skipping to %lx", address, next_page);
+            memset( data+amount_copied, ' ',  len_skipped);
+            address += len_skipped;
+            amount_copied += len_skipped;
+            continue;
+        }
+        
+        phys_addr += (address & ~x86_64_PAGE_MASK);
+        uint64_t remaining_on_page = x86_64_PAGE_SIZE - (address & ~x86_64_PAGE_MASK);
+        if(len_to_copy > remaining_on_page){
+            len_to_copy = remaining_on_page;
+        }
+
+        MemTxResult txt = address_space_rw(cpu_get_address_space(cpu, asidx), phys_addr, MEMTXATTRS_UNSPECIFIED, tmp_buf, len_to_copy, 0);
+        if(txt){
+            QEMU_PT_PRINTF(MEM_PREFIX, "Warning, read failed:\t%lx (%lx)", address, phys_addr);
+        }
+        
+        memcpy(data+amount_copied, tmp_buf, len_to_copy);
+        
+        address += len_to_copy;
+        amount_copied += len_to_copy;
+    }
+    
+    return true;
+}
+
+bool is_addr_mapped_cr3(uint64_t address, CPUState *cpu, uint64_t cr3){
+    return (get_48_paging_phys_addr(cr3, address) != 0xFFFFFFFFFFFFFFFFULL);
+} 
+
+bool is_addr_mapped(uint64_t address, CPUState *cpu){
+    CPUX86State *env = &(X86_CPU(cpu))->env;
+    kvm_arch_get_registers_fast(cpu);
+    return (get_48_paging_phys_addr(env->cr[3], address) != 0xFFFFFFFFFFFFFFFFULL);
+} 
+
+bool is_addr_mapped_cr3_snapshot(uint64_t address, CPUState *cpu, uint64_t cr3){
+    return (get_48_paging_phys_addr_snapshot(cr3, address) != 0xFFFFFFFFFFFFFFFFULL);
+} 
+
+bool dump_page_cr3_snapshot(uint64_t address, uint8_t* data, CPUState *cpu, uint64_t cr3){
+    fast_reload_t* snapshot = get_fast_reload_snapshot();
+    return read_snapshot_memory(snapshot, get_48_paging_phys_addr_snapshot(cr3, address), data, PPAGE_SIZE);   
+}
+
+
+bool dump_page_cr3_ht(uint64_t address, uint8_t* data, CPUState *cpu, uint64_t cr3){
+    hwaddr phys_addr = (hwaddr) get_48_paging_phys_addr(cr3, address);
+    int asidx = cpu_asidx_from_attrs(cpu, MEMTXATTRS_UNSPECIFIED);
+    if(phys_addr == 0xffffffffffffffffULL || address_space_rw(cpu_get_address_space(cpu, asidx), phys_addr, MEMTXATTRS_UNSPECIFIED, data, 0x1000, 0)){
+        if(phys_addr != 0xffffffffffffffffULL){
+            fprintf(stderr, "%s: Warning, read failed:\t%lx (%lx)\n", __func__, address, phys_addr);
+        }
+        return false;
+    }
+    return true;
+}
+
+bool dump_page_ht(uint64_t address, uint8_t* data, CPUState *cpu){
+    CPUX86State *env = &(X86_CPU(cpu))->env;
+    kvm_arch_get_registers_fast(cpu);
+    hwaddr phys_addr = (hwaddr) get_48_paging_phys_addr(env->cr[3], address);
+    int asidx = cpu_asidx_from_attrs(cpu, MEMTXATTRS_UNSPECIFIED);
+    if(phys_addr == 0xffffffffffffffffULL || address_space_rw(cpu_get_address_space(cpu, asidx), phys_addr, MEMTXATTRS_UNSPECIFIED, data, 0x1000, 0)){
+        if(phys_addr != 0xffffffffffffffffULL){
+            fprintf(stderr, "%s: Warning, read failed:\t%lx (%lx)\n", __func__, address, phys_addr);
+        }
+    }
+    return true;
+}
+
+uint64_t disassemble_at_rip(int fd, uint64_t address, CPUState *cpu, uint64_t cr3){
+
+	csh handle;
+
+	size_t code_size = 256;
+    uint8_t code_ptr[256];
+
+    /* don't => GET_GLOBAL_STATE()->disassembler_word_width */
+	if (cs_open(CS_ARCH_X86, get_capstone_mode(GET_GLOBAL_STATE()->disassembler_word_width), &handle) != CS_ERR_OK)
+		assert(false);
+	
+	cs_option(handle, CS_OPT_DETAIL, CS_OPT_ON);
+
+    cs_insn* insn = cs_malloc(handle);
+
+    read_virtual_memory(address, code_ptr, code_size, cpu);
+
+    int count = cs_disasm(handle, code_ptr, code_size, address, 5, &insn);
+    if(count > 0){
+        for(int i = 0; i < count; i++){
+            fprintf(stderr, "=> 0x%"PRIx64":\t%s\t\t%s\n", insn[i].address, insn[i].mnemonic, insn[i].op_str);
+        }
+    }
+    else{
+        fprintf(stderr, "ERROR in %s at %lx (cr3: %lx)\n", __func__, address, cr3);
+    }
+    
+    cs_free(insn, 1);
+    cs_close(&handle);
+    return 0;
+}
+
+
+
diff --new-file -ur qemu/pt/memory_access.h QEMU-PT/pt/memory_access.h
--- qemu/pt/memory_access.h	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/memory_access.h	2021-08-24 21:54:55.942586446 +0200
@@ -0,0 +1,63 @@
+/*
+
+Copyright (C) 2017 Sergej Schumilo
+
+This file is part of QEMU-PT (kAFL).
+
+QEMU-PT is free software: you can redistribute it and/or modify
+it under the terms of the GNU General Public License as published by
+the Free Software Foundation, either version 2 of the License, or
+(at your option) any later version.
+
+QEMU-PT is distributed in the hope that it will be useful,
+but WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with QEMU-PT.  If not, see <http://www.gnu.org/licenses/>.
+
+*/
+
+#ifndef MEMORY_ACCESS_H
+#define MEMORY_ACCESS_H
+
+#include "qemu/osdep.h"
+#include <linux/kvm.h>
+#include "qemu-common.h"
+#include "sysemu/kvm_int.h"
+
+bool read_physical_memory(uint64_t address, uint8_t* data, uint32_t size, CPUState *cpu);
+bool write_physical_memory(uint64_t address, uint8_t* data, uint32_t size, CPUState *cpu);
+
+bool remap_payload_slot(uint64_t phys_addr, uint32_t slot, CPUState *cpu);
+bool remap_payload_slot_protected(uint64_t phys_addr, uint32_t slot, CPUState *cpu);
+bool remap_payload_buffer(uint64_t virt_guest_addr, CPUState *cpu);
+void set_payload_buffer_mapping_fd(int fd, uint32_t size);
+
+bool read_virtual_memory_cr3(uint64_t address, uint8_t* data, uint32_t size, CPUState *cpu, uint64_t cr3);
+bool write_virtual_memory_cr3(uint64_t address, uint8_t* data, uint32_t size, CPUState *cpu, uint64_t cr3);
+bool write_virtual_shadow_memory_cr3(uint64_t address, uint8_t* data, uint32_t size, CPUState *cpu, uint64_t cr3);
+
+bool read_virtual_memory(uint64_t address, uint8_t* data, uint32_t size, CPUState *cpu);
+bool write_virtual_memory(uint64_t address, uint8_t* data, uint32_t size, CPUState *cpu);
+void hexdump_virtual_memory(uint64_t address, uint32_t size, CPUState *cpu);
+bool write_virtual_shadow_memory(uint64_t address, uint8_t* data, uint32_t size, CPUState *cpu);
+bool is_addr_mapped(uint64_t address, CPUState *cpu);
+bool is_addr_mapped_cr3(uint64_t address, CPUState *cpu, uint64_t cr3);
+
+int insert_breakpoint(CPUState *cpu, uint64_t addr, uint64_t len);
+int remove_breakpoint(CPUState *cpu, uint64_t addr, uint64_t len);
+void remove_all_breakpoints(CPUState *cpu);
+
+uint64_t disassemble_at_rip(int fd, uint64_t address, CPUState *cpu, uint64_t cr3);
+bool dump_page_cr3_snapshot(uint64_t address, uint8_t* data, CPUState *cpu, uint64_t cr3);
+bool dump_page_cr3_ht(uint64_t address, uint8_t* data, CPUState *cpu, uint64_t cr3);
+bool is_addr_mapped_cr3_snapshot(uint64_t address, CPUState *cpu, uint64_t cr3);
+
+void print_48_paging2(uint64_t cr3);
+
+bool dump_page_ht(uint64_t address, uint8_t* data, CPUState *cpu);
+
+
+#endif
diff --new-file -ur qemu/pt/mmh3.c QEMU-PT/pt/mmh3.c
--- qemu/pt/mmh3.c	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/mmh3.c	2021-08-24 21:54:55.938586449 +0200
@@ -0,0 +1,303 @@
+#include <stdio.h>
+#include <stdlib.h>
+#include "mmh3.h"
+
+#define FORCE_INLINE inline __attribute__((always_inline))
+
+FORCE_INLINE uint32_t rotl32(uint32_t x, int8_t r) {
+    return (x << r) | (x >> (32 - r));
+}
+
+FORCE_INLINE uint64_t rotl64(uint64_t x, int8_t r) {
+    return (x << r) | (x >> (64 - r));
+}
+
+#define ROTL32(x, y) rotl32(x, y)
+#define ROTL64(x, y) rotl64(x, y)
+#define BIG_CONSTANT(x) (x##LLU)
+
+/**
+ * Block read -- endian swapping, if required, or handle aligned reads
+ */
+FORCE_INLINE uint32_t getblock32(const uint32_t *p, int i) {
+    return p[i];
+}
+
+FORCE_INLINE uint64_t getblock64(const uint64_t *p, int i) {
+    return p[i];
+}
+
+/**
+ * Force all bits of a hash block to avalanche
+ */
+FORCE_INLINE uint32_t fmix32(uint32_t h) {
+    h ^= h >> 16;
+    h *= 0x85ebca6b;
+    h ^= h >> 13;
+    h *= 0xc2b2ae35;
+    h ^= h >> 16;
+    return h;
+}
+
+FORCE_INLINE uint64_t fmix64(uint64_t k) {
+    k ^= k >> 33;
+    k *= BIG_CONSTANT(0xff51afd7ed558ccd);
+    k ^= k >> 33;
+    k *= BIG_CONSTANT(0xc4ceb9fe1a85ec53);
+    k ^= k >> 33;
+    return k;
+}
+
+void mmh3_x86_32(const void *key, int len, uint32_t seed, void *out) {
+    const uint8_t *data = (const uint8_t *) key;
+    const int nblocks = len/4;
+    uint32_t h1 = seed;
+    const uint32_t c1 = 0xcc9e2d51;
+    const uint32_t c2 = 0x1b873593;
+
+    // Hashing -- body of the function
+    const uint32_t *blocks = (const uint32_t *) (data + 4*nblocks);
+    for (int i = -nblocks; i; i++) {
+        uint32_t k1 = getblock32(blocks, i);
+        k1 *= c1;
+        k1 = ROTL32(k1, 15);
+        k1 *= c2;
+        
+        h1 ^= k1;
+        h1 = ROTL32(h1, 13);
+        h1 = 5*h1 + 0xe6546b64;
+    }
+
+    const uint8_t *tail = (const uint8_t *) (data + 4*nblocks);
+    uint32_t k1 = 0;
+
+    switch (len & 3) {
+        case 3: k1 ^= tail[2] << 16;
+        case 2: k1 ^= tail[1] << 8;
+        case 1: k1 ^= tail[0];
+                k1 *= c1;
+                k1 = ROTL32(k1, 15);
+                k1 *= c2;
+                h1 ^= k1;
+    };
+
+    // Finalize
+    h1 ^= len;
+    h1 = fmix32(h1);
+    *(uint32_t *) out = h1;
+}
+
+void mmh3_x86_128(const void *key, const int len, uint32_t seed, void *out) {
+    const uint8_t *data = (const uint8_t *) key;
+    const int nblocks = len/16;
+
+    uint32_t h1 = seed;
+    uint32_t h2 = seed;
+    uint32_t h3 = seed;
+    uint32_t h4 = seed;
+
+    const uint32_t c1 = 0x239b961b;
+    const uint32_t c2 = 0xab0e9789;
+    const uint32_t c3 = 0x38b34ae5;
+    const uint32_t c4 = 0xa1e38b93;
+
+    const uint32_t *blocks = (const uint32_t *)(data + 16*nblocks);
+
+    for (int i = -nblocks; i; i++) {
+        uint32_t k1 = getblock32(blocks, i*4 + 0);
+        uint32_t k2 = getblock32(blocks, i*4 + 1);
+        uint32_t k3 = getblock32(blocks, i*4 + 2);
+        uint32_t k4 = getblock32(blocks, i*4 + 3);
+
+        k1 *= c1;
+        k1 = ROTL32(k1, 15);
+        k1 *= c2;
+        h1 ^= k1;
+
+        h1 = ROTL32(h1, 19);
+        h1 += h2; 
+        h1 = 5*h1 + 0x561ccd1b;
+
+        k2 *= c2;
+        k2 = ROTL32(k2, 16);
+        k2 *= c3;
+        h2 ^= k2;
+
+        h2 = ROTL32(h2, 17); 
+        h2 += h3;
+        h2 = 5*h2 + 0x0bcaa747;
+        
+        k3 *= c3; 
+        k3 = ROTL32(k3, 17);
+        k3 *= c4;
+        h3 ^= k3;
+        
+        h3 = ROTL32(h3, 15);
+        h3 += h4; 
+        h3 = 5*h3 + 0x96cd1c35;
+        
+        k4 *= c4;
+        k4  = ROTL32(k4, 18);
+        k4 *= c1;
+        h4 ^= k4;
+        
+        h4 = ROTL32(h4, 13);
+        h4 += h1;
+        h4 = 5*h4 + 0x32ac3b17;
+    }
+
+    // Tail
+    const uint8_t *tail = (const uint8_t *) (data + 16*nblocks);
+
+    uint32_t k1 = 0;
+    uint32_t k2 = 0;
+    uint32_t k3 = 0;
+    uint32_t k4 = 0;
+
+    switch (len & 15) {
+        case 15: k4 ^= tail[14] << 16;
+        case 14: k4 ^= tail[13] << 8;
+        case 13: k4 ^= tail[12] << 0;
+                 k4 *= c4;
+                 k4 = ROTL32(k4, 18);
+                 k4 *= c1;
+                 h4 ^= k4;
+        case 12: k3 ^= tail[11] << 24;
+        case 11: k3 ^= tail[10] << 16;
+        case 10: k3 ^= tail[9] << 8;
+        case 9:  k3 ^= tail[8] << 0;
+                 k3 *= c3;
+                 k3 = ROTL32(k3, 17);
+                 k3 *= c4;
+                 h3 ^= k3;
+        case 8:  k2 ^= tail[7] << 24;
+        case 7:  k2 ^= tail[6] << 16;
+        case 6:  k2 ^= tail[5] << 8;
+        case 5:  k2 ^= tail[4] << 0;
+                 k2 *= c2;
+                 k2 = ROTL32(k2, 16);
+                 k2 *= c3;
+                 h2 ^= k2;
+        case 4:  k1 ^= tail[3] << 24;
+        case 3:  k1 ^= tail[2] << 16;
+        case 2:  k1 ^= tail[1] << 8;
+        case 1:  k1 ^= tail[0] << 0;
+                 k1 *= c1;
+                 k1 = ROTL32(k1, 15);
+                 k1 *= c2;
+                 h1 ^= k1;
+    };
+
+    // Finalize
+    h1 ^= len;
+    h2 ^= len;
+    h3 ^= len;
+    h4 ^= len;
+
+    h1 += h2;
+    h1 += h3;
+    h1 += h4;
+    h2 += h1;
+    h3 += h1;
+    h4 += h1;
+
+    h1 = fmix32(h1);
+    h2 = fmix32(h2);
+    h3 = fmix32(h3);
+    h4 = fmix32(h4);
+
+    h1 += h2;
+    h1 += h3;
+    h1 += h4;
+    h2 += h1;
+    h3 += h1;
+    h4 += h1;
+
+    ((uint32_t *) out)[0] = h1;
+    ((uint32_t *) out)[1] = h2;
+    ((uint32_t *) out)[2] = h3;
+    ((uint32_t *) out)[3] = h4;
+}
+
+void mmh3_x64_128(const void *key, const int len, const uint32_t seed, void *out) {
+    const uint8_t *data = (const uint8_t *) key;
+    const int nblocks = len/16;
+    uint64_t h1 = seed;
+    uint64_t h2 = seed;
+
+    const uint64_t c1 = BIG_CONSTANT(0x87c37b91114253d5);
+    const uint64_t c2 = BIG_CONSTANT(0x4cf5ad432745937f);
+
+    // Body
+    const uint64_t *blocks = (const uint64_t *) (data);
+
+    for (int i = 0; i < nblocks; i++) {
+        uint64_t k1 = getblock64(blocks, i*2 + 0);
+        uint64_t k2 = getblock64(blocks, i*2 + 1);
+        
+        k1 *= c1;
+        k1 = ROTL64(k1, 31);
+        k1 *= c2;
+        h1 ^= k1;
+
+        h1 = ROTL64(h1, 27);
+        h1 += h2;
+        h1 = 5*h1 + 0x52dce729;
+
+        k2 *= c2;
+        k2 = ROTL64(k2, 33);
+        k2 *= c1;
+        h2 ^= k2;
+
+        h2 = ROTL64(h2, 31);
+        h2 += h1;
+        h2 = 5*h2 + 0x38495ab5;
+    }
+
+    // tail
+    const uint8_t *tail = (const uint8_t *) (data + 16*nblocks);
+    uint64_t k1 = 0;
+    uint64_t k2 = 0;
+
+    switch (len & 15) {
+        case 15: k2 ^= ((uint64_t) tail[14]) << 48;
+        case 14: k2 ^= ((uint64_t) tail[13]) << 40;
+        case 13: k2 ^= ((uint64_t) tail[12]) << 32;
+        case 12: k2 ^= ((uint64_t) tail[11]) << 24;
+        case 11: k2 ^= ((uint64_t) tail[10]) << 16;
+        case 10: k2 ^= ((uint64_t) tail[9]) << 8;
+        case 9:  k2 ^= ((uint64_t) tail[8]) << 0;
+                 k2 *= c2;
+                 k2 = ROTL64(k2, 33);
+                 k2 *= c1;
+                 h2 ^= k2;
+        case 8:  k1 ^= ((uint64_t) tail[7]) << 56;
+        case 7:  k1 ^= ((uint64_t) tail[6]) << 48;
+        case 6:  k1 ^= ((uint64_t) tail[5]) << 40;
+        case 5:  k1 ^= ((uint64_t) tail[4]) << 32;
+        case 4:  k1 ^= ((uint64_t) tail[3]) << 24;
+        case 3:  k1 ^= ((uint64_t) tail[2]) << 16;
+        case 2:  k1 ^= ((uint64_t) tail[1]) << 8;
+        case 1:  k1 ^= ((uint64_t) tail[0]) << 0;
+                 k1 *= c1;
+                 k1 = ROTL64(k1, 31);
+                 k1 *= c2;
+                 h1 ^= k1;
+    };
+
+    // finalize
+    h1 ^= len;
+    h2 ^= len;
+    
+    h1 += h2;
+    h2 += h1;
+
+    h1 = fmix64(h1);
+    h2 = fmix64(h2);
+
+    h1 += h2;
+    h2 += h1;
+
+    ((uint64_t *) out)[0] = h1;
+    ((uint64_t *) out)[1] = h2;
+}
diff --new-file -ur qemu/pt/mmh3.h QEMU-PT/pt/mmh3.h
--- qemu/pt/mmh3.h	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/mmh3.h	2021-08-24 21:54:55.938586449 +0200
@@ -0,0 +1,12 @@
+#ifndef _MMH3_H
+#define _MMH3_H
+
+#include <stdint.h>
+
+typedef unsigned __int128 uint128_t;
+
+void mmh3_x86_32(const void *key, int len, uint32_t seed, void *out);
+void mmh3_x86_128(const void *key, int len, uint32_t seed, void *out);
+void mmh3_x64_128(const void *key, int len, uint32_t seed, void *out);
+
+#endif
diff --new-file -ur qemu/pt/nested_hypercalls.c QEMU-PT/pt/nested_hypercalls.c
--- qemu/pt/nested_hypercalls.c	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/nested_hypercalls.c	2021-08-24 21:54:55.938586449 +0200
@@ -0,0 +1,169 @@
+#include <stdio.h>
+#include <stdint.h>
+#include "kvm_nested.h"
+#include "memory_access.h"
+#include "debug.h"
+#include "nested_hypercalls.h"
+#include "interface.h"
+#include "state.h"
+#include "trace_cache.h"
+#include "pt.h"
+#include "sysemu/sysemu.h"
+#include "sysemu/kvm.h"
+#include "qemu/main-loop.h"
+#include "pt/helpers.h"
+
+#include "pt/hypercall.h"
+
+bool hypercalls_enabled = false;
+
+bool create_snapshot = false;
+
+uint64_t htos_cr3 = 0;
+uint64_t htos_config = 0;
+
+int nested_once = 0;
+
+bool nested_setup_snapshot_once = false;
+
+void handle_hypercall_kafl_nested_config(struct kvm_run *run, CPUState *cpu){
+	uint32_t size = 0;
+	read_physical_memory(htos_config, (uint8_t*) &size, sizeof(uint32_t), cpu);
+	fprintf(stderr, "--> %x\n", size);
+	void* buffer = malloc(size);
+
+	read_physical_memory(htos_config+sizeof(uint32_t), buffer, size, cpu);
+
+	print_configuration(stderr, buffer, size);
+
+	FILE* f = fopen("/tmp/hypertrash_configration", "w");
+	print_configuration(f, buffer, size);
+	fclose(f);
+
+	free(buffer);
+}
+
+#define ANSI_COLOR_YELLOW  "\x1b[33m"
+#define ANSI_COLOR_RESET   "\x1b[0m"
+
+void handle_hypercall_kafl_nested_hprintf(struct kvm_run *run, CPUState *cpu){
+  char hprintf_buffer[0x1000];
+	read_physical_memory((uint64_t)run->hypercall.args[0], (uint8_t*)hprintf_buffer, 0x1000, cpu);
+
+	set_hprintf_auxiliary_buffer(GET_GLOBAL_STATE()->auxilary_buffer, hprintf_buffer, strnlen(hprintf_buffer, 0x1000)+1);
+	synchronization_lock_hprintf();
+}
+
+void handle_hypercall_kafl_nested_prepare(struct kvm_run *run, CPUState *cpu){
+	kvm_arch_get_registers(cpu);
+
+	if((uint64_t)run->hypercall.args[0]){
+		QEMU_PT_PRINTF(CORE_PREFIX, "handle_hypercall_kafl_nested_prepare:\t NUM:\t%lx\t ADDRESS:\t%lx\t CR3:\t%lx", (uint64_t)run->hypercall.args[0], (uint64_t)run->hypercall.args[1], (uint64_t)run->hypercall.args[2]);
+	}
+	else{
+		abort();
+	}
+	size_t buffer_size = (size_t)((uint64_t)run->hypercall.args[0] * sizeof(uint64_t));
+	uint64_t* buffer = malloc(buffer_size);
+	memset(buffer, 0x0, buffer_size);
+
+	read_physical_memory((uint64_t)run->hypercall.args[1], (uint8_t*)buffer, buffer_size, cpu);
+	htos_cr3 = (uint64_t)run->hypercall.args[0];
+
+	for(uint64_t i = 0; i < (uint64_t)run->hypercall.args[0]; i++){
+		if(i == 0){
+			htos_config = buffer[i];
+		}
+		QEMU_PT_PRINTF(CORE_PREFIX, "ADDRESS: %lx", buffer[i]);
+		remap_payload_slot(buffer[i], i, cpu);
+	}
+
+	set_payload_pages(buffer, (uint32_t)run->hypercall.args[0]);
+
+	// wipe memory 
+	memset(buffer, 0x00, buffer_size);
+	write_physical_memory((uint64_t)run->hypercall.args[1], (uint8_t*)buffer, buffer_size, cpu);
+
+	free(buffer);
+}
+
+bool acquired = false;
+
+void handle_hypercall_kafl_nested_early_release(struct kvm_run *run, CPUState *cpu){
+	if(!hypercalls_enabled){
+		return;
+	}
+	bool state = GET_GLOBAL_STATE()->in_reload_mode;
+	if(!state){
+		GET_GLOBAL_STATE()->in_reload_mode = true;
+		synchronization_disable_pt(cpu);
+		GET_GLOBAL_STATE()->in_reload_mode = false;
+	}
+	else{
+		synchronization_disable_pt(cpu);
+	}
+}
+
+void handle_hypercall_kafl_nested_release(struct kvm_run *run, CPUState *cpu){
+	hypercalls_enabled = true;
+	static int rcount = 0;
+	if((rcount%100) == 0){
+			kvm_arch_get_registers(cpu);
+	}
+	rcount++;
+	synchronization_disable_pt(cpu);
+}
+
+static inline void set_page_dump_bp_nested(CPUState *cpu, uint64_t cr3, uint64_t addr){
+	kvm_remove_all_breakpoints(cpu);
+	kvm_insert_breakpoint(cpu, addr, 1, 1);
+	kvm_update_guest_debug(cpu, 0);
+
+	kvm_vcpu_ioctl(cpu, KVM_VMX_PT_SET_PAGE_DUMP_CR3, cr3);
+	kvm_vcpu_ioctl(cpu, KVM_VMX_PT_ENABLE_PAGE_DUMP_CR3);
+}
+
+void handle_hypercall_kafl_nested_acquire(struct kvm_run *run, CPUState *cpu){
+	if (!acquired){
+		acquired = true;
+
+		create_fast_snapshot(cpu, true);
+		for(int i = 0; i < INTEL_PT_MAX_RANGES; i++){
+			if(GET_GLOBAL_STATE()->pt_ip_filter_configured[i]){
+				pt_enable_ip_filtering(cpu, i, true, false);
+			}
+		}
+		
+		qemu_mutex_lock_iothread();
+		fast_reload_restore(get_fast_reload_snapshot());
+		qemu_mutex_unlock_iothread();
+
+		kvm_arch_get_registers(cpu);
+
+		X86CPU *x86_cpu = X86_CPU(cpu);
+	  CPUX86State *env = &x86_cpu->env;
+				
+		printf("IN FUZZING LOOP! %lx\n", env->eip);
+		GET_GLOBAL_STATE()->in_fuzzing_mode = true;
+		set_state_auxiliary_result_buffer(GET_GLOBAL_STATE()->auxilary_buffer, 3);
+
+		if(GET_GLOBAL_STATE()->protect_payload_buffer){
+			for(int i = 0; i < GET_GLOBAL_STATE()->nested_payload_pages_num; i++){
+				remap_payload_slot_protected(GET_GLOBAL_STATE()->nested_payload_pages[i], i, cpu);
+			}
+		}
+	}
+
+	synchronization_lock();
+	
+	uint64_t cr3 = get_nested_host_cr3(cpu) & 0xFFFFFFFFFFFFF000ULL;
+	pt_set_cr3(cpu, cr3, false);
+	GET_GLOBAL_STATE()->parent_cr3 = cr3;
+
+	if(GET_GLOBAL_STATE()->dump_page){
+		set_page_dump_bp_nested(cpu, cr3, GET_GLOBAL_STATE()->dump_page_addr);
+	}
+
+	synchronization_enter_fuzzing_loop(cpu);
+
+}
\ No newline at end of file
diff --new-file -ur qemu/pt/nested_hypercalls.h QEMU-PT/pt/nested_hypercalls.h
--- qemu/pt/nested_hypercalls.h	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/nested_hypercalls.h	2021-08-24 21:54:55.938586449 +0200
@@ -0,0 +1,9 @@
+#pragma once 
+
+/* HyperTrash! */
+void handle_hypercall_kafl_nested_hprintf(struct kvm_run *run, CPUState *cpu);
+void handle_hypercall_kafl_nested_prepare(struct kvm_run *run, CPUState *cpu);
+void handle_hypercall_kafl_nested_config(struct kvm_run *run, CPUState *cpu);
+void handle_hypercall_kafl_nested_release(struct kvm_run *run, CPUState *cpu);
+void handle_hypercall_kafl_nested_acquire(struct kvm_run *run, CPUState *cpu);
+void handle_hypercall_kafl_nested_early_release(struct kvm_run *run, CPUState *cpu);
\ No newline at end of file
diff --new-file -ur qemu/pt/page_cache.c QEMU-PT/pt/page_cache.c
--- qemu/pt/page_cache.c	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/page_cache.c	2021-08-24 21:54:55.938586449 +0200
@@ -0,0 +1,420 @@
+#ifndef _GNU_SOURCE
+#define _GNU_SOURCE  
+#endif
+#include <errno.h>
+#include <capstone/capstone.h>
+#include <capstone/x86.h>
+#include <sys/file.h>
+#include <sys/types.h>
+#include <unistd.h>
+#include <stdio.h>
+#include <sys/mman.h>
+#include <assert.h>
+#include "page_cache.h"
+#include "debug.h"
+#ifndef STANDALONE_DECODER
+#include "cpu.h"
+#include "memory_access.h"
+#include "fast_vm_reload.h"
+#include "kvm_nested.h"
+#include "pt/state.h"
+#endif
+
+
+#define PAGE_SIZE 0x1000UL
+#define PAGE_CACHE_ADDR_LINE_SIZE sizeof(uint64_t)
+
+#define UNMAPPED_PAGE 0xFFFFFFFFFFFFFFFFULL
+
+#ifndef STANDALONE_DECODER
+static bool reload_addresses(page_cache_t* self){
+#else
+bool reload_addresses(page_cache_t* self){
+#endif
+	khiter_t k;
+	int ret;
+	uint64_t addr, offset;
+	uint64_t value = 0;
+
+	size_t self_offset = lseek(self->fd_address_file, 0, SEEK_END);
+
+	if(self_offset != self->num_pages*PAGE_CACHE_ADDR_LINE_SIZE){
+
+		lseek(self->fd_address_file, self->num_pages*PAGE_CACHE_ADDR_LINE_SIZE, SEEK_SET);
+		offset = self->num_pages;
+		while(read(self->fd_address_file, &value, PAGE_CACHE_ADDR_LINE_SIZE)){
+			addr = value & 0xFFFFFFFFFFFFF000ULL; 
+			offset++;
+
+			k = kh_get(PC_CACHE, self->lookup, addr); 
+			if(k == kh_end(self->lookup)){
+
+				if(value & 0xFFF){
+					fprintf(stderr, "Load page: %lx (UMAPPED)\n", addr);
+				}
+				else{
+					k = kh_put(PC_CACHE, self->lookup, addr, &ret); 
+					kh_value(self->lookup, k) = (offset-1)*PAGE_SIZE;
+				}
+			}
+			else{
+				fprintf(stderr, "----------> Page duplicate found ...skipping! %lx\n", addr);
+			}
+		}
+
+		/* reload page dump file */
+		munmap(self->page_data, self->num_pages*PAGE_SIZE);
+		self->num_pages = self_offset/PAGE_CACHE_ADDR_LINE_SIZE;
+		self->page_data = mmap(NULL, (self->num_pages)*PAGE_SIZE, PROT_READ | PROT_WRITE, MAP_SHARED, self->fd_page_file, 0);
+
+		return true;
+	}
+
+	return false;
+}
+
+#ifndef STANDALONE_DECODER
+static bool append_page(page_cache_t* self, uint64_t page, uint64_t cr3){
+	bool success = true;
+	if(!self->num_pages){
+		assert(!ftruncate(self->fd_page_file, (self->num_pages+1)*PAGE_SIZE));
+		self->page_data = mmap(NULL, (self->num_pages+1)*PAGE_SIZE, PROT_READ | PROT_WRITE, MAP_SHARED, self->fd_page_file, 0);
+	}
+	else{
+		munmap(self->page_data, self->num_pages*PAGE_SIZE);
+		assert(!ftruncate(self->fd_page_file, (self->num_pages+1)*PAGE_SIZE));
+		self->page_data = mmap(NULL, (self->num_pages+1)*PAGE_SIZE, PROT_READ | PROT_WRITE, MAP_SHARED, self->fd_page_file, 0);
+	}
+	
+	if(!dump_page_cr3_ht(page, self->page_data+(PAGE_SIZE*self->num_pages), self->cpu, GET_GLOBAL_STATE()->pt_c3_filter)){
+		if(!dump_page_cr3_ht(page, self->page_data+(PAGE_SIZE*self->num_pages), self->cpu, GET_GLOBAL_STATE()->parent_cr3)){
+			if(!dump_page_cr3_snapshot(page, self->page_data+(PAGE_SIZE*self->num_pages), self->cpu, GET_GLOBAL_STATE()->parent_cr3)){
+
+				fprintf(stderr, "FAILED DUMP PROCESS of PAGE %lx\n", page);
+
+				munmap(self->page_data, (self->num_pages+1)*PAGE_SIZE);
+				assert(!ftruncate(self->fd_page_file, (self->num_pages)*PAGE_SIZE));
+				self->page_data = mmap(NULL, (self->num_pages)*PAGE_SIZE, PROT_READ | PROT_WRITE, MAP_SHARED, self->fd_page_file, 0);
+
+				success = false;
+				return success;
+			}
+		}
+	}
+	fsync(self->fd_page_file);
+	self->num_pages++;
+	return success;
+}
+#else
+bool append_page(page_cache_t* self, uint64_t page, uint8_t* ptr){
+	self->last_page = 0xFFFFFFFFFFFFFFFF;
+	self->last_addr = 0xFFFFFFFFFFFFFFFF;
+	page &= 0xFFFFFFFFFFFFF000ULL;
+	bool success = true;
+	if(!self->num_pages){
+		assert(!ftruncate(self->fd_page_file, (self->num_pages+1)*PAGE_SIZE));
+		self->page_data = mmap(NULL, (self->num_pages+1)*PAGE_SIZE, PROT_READ | PROT_WRITE, MAP_SHARED, self->fd_page_file, 0);
+	}
+	else{
+		munmap(self->page_data, self->num_pages*PAGE_SIZE);
+		assert(!ftruncate(self->fd_page_file, (self->num_pages+1)*PAGE_SIZE));
+		self->page_data = mmap(NULL, (self->num_pages+1)*PAGE_SIZE, PROT_READ | PROT_WRITE, MAP_SHARED, self->fd_page_file, 0);
+	}
+
+	memcpy(self->page_data+(PAGE_SIZE*self->num_pages), ptr, PAGE_SIZE);
+
+	fsync(self->fd_page_file);
+
+	int ret;
+	khiter_t k;
+	k = kh_put(PC_CACHE, self->lookup, page, &ret); 
+	kh_value(self->lookup, k) = self->num_pages*PAGE_SIZE;
+	assert(write(self->fd_address_file, &page, PAGE_CACHE_ADDR_LINE_SIZE) == PAGE_CACHE_ADDR_LINE_SIZE);
+
+	self->num_pages++;
+
+	return success;
+}
+#endif
+
+static void page_cache_lock(page_cache_t* self){
+#ifndef STANDALONE_DECODER
+	int ret = 0;
+	while (true){
+		ret = flock(self->fd_lock, LOCK_EX);
+		if (ret == 0){
+			return;
+		}
+		else if (ret == EINTR){ 
+			fprintf(stderr, "%s: interrupted by signal...\n", __func__);
+		}
+		else{
+			assert(false);
+		}
+	}
+#endif
+}
+
+static void page_cache_unlock(page_cache_t* self){
+#ifndef STANDALONE_DECODER
+	int ret = 0;
+	while (true){
+		ret = flock(self->fd_lock, LOCK_UN);
+		if (ret == 0){
+			return;
+		}
+		else if (ret == EINTR){ 
+			fprintf(stderr, "%s: interrupted by signal...\n", __func__);
+		}
+		else{
+			assert(false);
+		}
+	}
+#endif
+}
+
+static bool update_page_cache(page_cache_t* self, uint64_t page, khiter_t* k){
+
+	page_cache_lock(self);
+#ifdef DEBUG_PAGE_CACHE_LOCK
+	fprintf(stderr, "%d: LOCKING PAGE CACHE\n", getpid());
+#endif
+
+	if(reload_addresses(self)){
+		*k = kh_get(PC_CACHE, self->lookup, page); 
+	}
+
+
+	if(*k == kh_end(self->lookup)){
+#ifndef STANDALONE_DECODER
+		int ret;
+
+		uint64_t cr3 = GET_GLOBAL_STATE()->parent_cr3; //self->cpu->parent_cr3;
+		if(!is_addr_mapped_cr3_snapshot(page, self->cpu, GET_GLOBAL_STATE()->parent_cr3) && !is_addr_mapped_cr3_snapshot(page, self->cpu, GET_GLOBAL_STATE()->pt_c3_filter)){ //self->cpu->parent_cr3)){
+		}
+
+		*k = kh_get(PC_CACHE, self->lookup, page); 
+
+		if(*k == kh_end(self->lookup) && reload_addresses(self)){
+			/* reload sucessful */
+			*k = kh_get(PC_CACHE, self->lookup, page); 
+		}
+		else{
+			
+
+			if(append_page(self, page, cr3)){
+				*k = kh_put(PC_CACHE, self->lookup, page, &ret); 
+				assert(write(self->fd_address_file, &page, PAGE_CACHE_ADDR_LINE_SIZE) == PAGE_CACHE_ADDR_LINE_SIZE);
+				kh_value(self->lookup, *k) = (self->num_pages-1)*PAGE_SIZE;
+			}
+			else{
+				fprintf(stderr, "Fail!!!!\n");
+				page_cache_unlock(self);
+				return false;
+			}
+
+			*k = kh_get(PC_CACHE, self->lookup, page); 
+		}
+#else
+		page_cache_unlock(self);
+		return false;
+		abort();
+#endif
+	}
+	
+#ifdef DEBUG_PAGE_CACHE_LOCK
+	fprintf(stderr, "%d: UNLOCKING PAGE CACHE\n", getpid());
+#endif
+
+	page_cache_unlock(self);
+	return true;
+}
+
+uint64_t page_cache_fetch(page_cache_t* self, uint64_t page, bool* success, bool test_mode){	
+	page &= 0xFFFFFFFFFFFFF000ULL;
+
+	if (self->last_page == page){
+		return self->last_addr;
+	}
+
+	khiter_t k;
+	k = kh_get(PC_CACHE, self->lookup, page); 
+	if(k == kh_end(self->lookup)){
+		if(test_mode || update_page_cache(self, page, &k) == false){
+			*success = false;
+			return 0;
+		}
+	}
+
+	self->last_page = page;
+
+	if(kh_value(self->lookup, k) == UNMAPPED_PAGE){
+		self->last_addr = UNMAPPED_PAGE;
+	}
+	else{
+		self->last_addr = (uint64_t)self->page_data+kh_value(self->lookup, k);
+	}
+
+	*success = true;
+	return self->last_addr;
+}
+
+#ifndef STANDALONE_DECODER
+page_cache_t* page_cache_new(CPUState *cpu, const char* cache_file){
+#else
+page_cache_t* page_cache_new(const char* cache_file, uint8_t disassembler_word_width){
+#endif
+	page_cache_t* self = malloc(sizeof(page_cache_t));
+#ifndef STANDALONE_DECODER
+	int disassembler_word_width = GET_GLOBAL_STATE()->disassembler_word_width;
+#endif
+
+	char* tmp1;
+	char* tmp2;
+	char* tmp3;
+	assert(asprintf(&tmp1, "%s.dump", cache_file) != -1);
+	assert(asprintf(&tmp2, "%s.addr", cache_file) != -1);
+	assert(asprintf(&tmp3, "%s.lock", cache_file) != -1);
+
+
+	self->lookup = kh_init(PC_CACHE);
+	self->fd_page_file = open(tmp1, O_CLOEXEC | O_RDWR, S_IRWXU);
+	self->fd_address_file = open(tmp2, O_CLOEXEC | O_RDWR, S_IRWXU);
+
+#ifndef STANDALONE_DECODER
+	self->cpu = cpu;
+	self->fd_lock = open(tmp3, O_CLOEXEC);
+	assert(self->fd_lock > 0);
+#else
+	if(self->fd_page_file == -1 || self->fd_address_file == -1){
+		printf("[ ] Page cache files not found...\n");
+		exit(1);
+	}
+#endif
+
+	memset(self->disassemble_cache, 0x0, 16);
+
+	self->page_data = NULL;
+	self->num_pages = 0;
+
+	self->last_page = 0xFFFFFFFFFFFFFFFF;
+	self->last_addr = 0xFFFFFFFFFFFFFFFF;
+
+	QEMU_PT_PRINTF(PAGE_CACHE_PREFIX, "%s (%s - %s) WORD_WIDTH: %d", __func__, tmp1, tmp2, disassembler_word_width);
+
+	free(tmp3);
+	free(tmp2);
+	free(tmp1);
+
+	if (cs_open(CS_ARCH_X86, CS_MODE_16, &self->handle_16) != CS_ERR_OK)
+		assert(false);
+
+	if (cs_open(CS_ARCH_X86, CS_MODE_32, &self->handle_32) != CS_ERR_OK)
+		assert(false);
+
+	if (cs_open(CS_ARCH_X86, CS_MODE_64, &self->handle_64) != CS_ERR_OK)
+		assert(false);
+
+	cs_option(self->handle_16, CS_OPT_DETAIL, CS_OPT_ON);
+	cs_option(self->handle_32, CS_OPT_DETAIL, CS_OPT_ON);
+	cs_option(self->handle_64, CS_OPT_DETAIL, CS_OPT_ON);
+
+	return self;
+}
+
+#ifdef STANDALONE_DECODER
+void page_cache_destroy(page_cache_t* self){
+	munmap(self->page_data, self->num_pages * 0x1000);
+	kh_destroy(PC_CACHE, self->lookup);
+
+	cs_close(&self->handle_16);
+	cs_close(&self->handle_32);
+	cs_close(&self->handle_64);
+	free(self);
+}
+#endif
+
+
+bool page_cache_disassemble(page_cache_t* self, uint64_t address, cs_insn **insn){
+	return true;
+}
+
+cs_insn* page_cache_cs_malloc(page_cache_t* self, disassembler_mode_t mode){
+	switch(mode){
+		case mode_16:
+			return cs_malloc(self->handle_16);
+		case mode_32:
+			return cs_malloc(self->handle_32);
+		case mode_64:
+			return cs_malloc(self->handle_64);
+		default:
+			assert(false);
+	}
+	return NULL;
+}
+
+bool page_cache_disassemble_iter(page_cache_t* self, uint64_t* address, cs_insn *insn, uint64_t* failed_page, disassembler_mode_t mode){
+
+	*failed_page = 0xFFFFFFFFFFFFFFFFULL;
+
+	bool success = true;
+	size_t code_size = 16;
+
+#if defined(STANDALONE_DECODER) || !defined(EXPERIMENTAL_PAGE_FETCH)
+	uint8_t* code = (uint8_t*)page_cache_fetch(self, *address, &success, false);
+#else
+	uint8_t* code = (uint8_t*)page_cache_fetch(self, *address, &success, true);
+#endif
+	uint8_t* code_ptr = 0;
+
+	csh* current_handle = NULL;
+
+	switch(mode){
+		case mode_16:
+			current_handle = &self->handle_16;
+			break;
+		case mode_32:
+			current_handle = &self->handle_32;
+			break;
+		case mode_64:
+			current_handle = &self->handle_64;
+			break;
+		default:
+			assert(false);
+	}
+
+	if (code == (void*)UNMAPPED_PAGE || success == false){
+		*failed_page = *address;// & 0xFFFFFFFFFFFFF000ULL;
+		return false;
+	}
+
+	if ((*address & 0xFFF) >= (0x1000-16)){
+		memcpy((void*)self->disassemble_cache, (void*)((uint64_t)code+(0x1000-16)), 16);
+		code_ptr = self->disassemble_cache + 0xf-(0xfff-(*address&0xfff));
+
+#if defined(STANDALONE_DECODER) || !defined(EXPERIMENTAL_PAGE_FETCH)
+		code = (uint8_t*)page_cache_fetch(self, *address+0x1000, &success, false);
+#else
+		code = (uint8_t*)page_cache_fetch(self, *address+0x1000, &success, true);
+#endif
+
+		if(success == true){
+			memcpy((void*)(self->disassemble_cache+16), (void*)code, 16);
+			return cs_disasm_iter(*current_handle, (const uint8_t**) &code_ptr, &code_size, address, insn);
+		}
+		else{
+			code_size = (0xfff-(*address&0xfff));
+			if(!cs_disasm_iter(*current_handle, (const uint8_t**) &code_ptr, &code_size, address, insn)){
+				*failed_page = (*address+0x1000) & 0xFFFFFFFFFFFFF000ULL;
+				return false;
+			}
+			return true;
+		}
+	} 
+	else {
+		code_ptr = code + (*address&0xFFF);
+		return cs_disasm_iter(*current_handle, (const uint8_t**) &code_ptr, &code_size, address, insn);
+	}
+}
+
+
diff --new-file -ur qemu/pt/page_cache.h QEMU-PT/pt/page_cache.h
--- qemu/pt/page_cache.h	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/page_cache.h	2021-08-24 21:54:55.938586449 +0200
@@ -0,0 +1,52 @@
+#pragma once
+
+#include <capstone/capstone.h>
+#include <capstone/x86.h>
+#ifndef STANDALONE_DECODER
+#include "qemu/osdep.h"
+#endif
+#include "khash.h"
+
+KHASH_MAP_INIT_INT64(PC_CACHE, uint64_t)
+
+typedef struct page_cache_s{
+#ifndef STANDALONE_DECODER
+	CPUState *cpu;
+#endif
+	khash_t(PC_CACHE) *lookup;
+	int fd_page_file;
+	int fd_address_file; 
+	int fd_lock;
+	uint8_t disassemble_cache[32];
+	void* page_data;
+	uint32_t num_pages;
+
+	csh handle_16;
+	csh handle_32;
+	csh handle_64;
+
+	uint64_t last_page;
+	uint64_t last_addr;  
+} page_cache_t;
+
+#ifndef STANDALONE_DECODER
+page_cache_t* page_cache_new(CPUState *cpu, const char* cache_file);
+#else
+page_cache_t* page_cache_new(const char* cache_file, uint8_t disassembler_word_width);
+void page_cache_destroy(page_cache_t* self);
+bool append_page(page_cache_t* self, uint64_t page, uint8_t* ptr);
+#endif
+
+
+typedef enum disassembler_mode_s { 
+	mode_16, 
+	mode_32, 
+	mode_64,
+} disassembler_mode_t;
+
+uint64_t page_cache_fetch(page_cache_t* self, uint64_t page, bool* success, bool test_mode);
+
+bool page_cache_disassemble(page_cache_t* self, uint64_t address, cs_insn **insn);
+bool page_cache_disassemble_iter(page_cache_t* self, uint64_t* address, cs_insn *insn, uint64_t* failed_page, disassembler_mode_t mode);
+
+cs_insn* page_cache_cs_malloc(page_cache_t* self, disassembler_mode_t mode);
\ No newline at end of file
diff --new-file -ur qemu/pt/patcher.c QEMU-PT/pt/patcher.c
--- qemu/pt/patcher.c	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/patcher.c	2021-08-24 21:54:55.938586449 +0200
@@ -0,0 +1,184 @@
+#include "patcher.h"
+#include "pt/memory_access.h"
+#include "pt/disassembler.h"
+#include "debug.h"
+#include "pt/state.h"
+
+uint8_t cmp_patch_data[] = { 0x38, 0xC0, [2 ... MAX_INSTRUCTION_SIZE]=0x90 }; // CMP AL,AL; NOP, NOP ... 
+const uint8_t *cmp_patch = &cmp_patch_data[0];
+
+///////////////////////////////////////////////////////////////////////////////////
+// Private Helper Functions Declarations
+///////////////////////////////////////////////////////////////////////////////////
+//
+static void _patcher_apply_patch(patcher_t *self, size_t index);
+
+static void _patcher_restore_patch(patcher_t *self, size_t index);
+
+static void _patcher_save_patch(patcher_t *self, size_t index, uint8_t* data, size_t instruction_size, uint64_t addr);
+
+static size_t _patcher_disassemble_size(patcher_t *self, uint8_t* data,  uint64_t addr, x86_insn id);
+
+static void _patcher_alloc_patch_infos(patcher_t *self, size_t num_patches);
+
+static void _patcher_free_patch_infos(patcher_t *self);
+
+static redqueen_t* _redq_ptr(patcher_t *self);
+
+
+///////////////////////////////////////////////////////////////////////////////////
+// Public Functions
+///////////////////////////////////////////////////////////////////////////////////
+
+patcher_t* patcher_new(CPUState *cpu){
+    patcher_t *res = malloc(sizeof(patcher_t));
+    res->cpu = cpu;
+    res->num_patches = 0;
+    res->patches = NULL;
+    res->is_currently_applied = false;
+    return res;
+}
+
+void patcher_free(patcher_t* self){
+    assert(!self->is_currently_applied);
+    _patcher_free_patch_infos(self);
+    free(self);
+}
+
+void patcher_apply_all(patcher_t *self){
+  assert(!self->is_currently_applied);
+  assert(!_redq_ptr(self)->hooks_applied);
+  //assert(patcher_validate_patches(self));
+  for(size_t i=0; i < self->num_patches; i++){
+      _patcher_apply_patch(self, i);
+  }
+  self->is_currently_applied = true;
+}
+
+void patcher_restore_all(patcher_t *self){
+  assert(self->is_currently_applied);
+  assert(!_redq_ptr(self)->hooks_applied);
+  //assert(patcher_validate_patches(self));
+  for(size_t i = 0; i < self->num_patches; i++){
+    _patcher_restore_patch(self, i);
+  }
+  self->is_currently_applied = false;
+}
+
+void patcher_set_addrs(patcher_t *self, uint64_t* addrs, size_t num_addrs){
+  _patcher_free_patch_infos(self);
+  _patcher_alloc_patch_infos(self, num_addrs);
+  uint8_t curr_instruction_code[MAX_INSTRUCTION_SIZE];
+  memset(&curr_instruction_code[0], 0, MAX_INSTRUCTION_SIZE);
+
+  for(size_t i=0; i < self->num_patches; i++){
+    //QEMU_PT_PRINTF(REDQUEEN_PREFIX, "patching %lx", addrs[i]);
+    if( read_virtual_memory(addrs[i], &curr_instruction_code[0], MAX_INSTRUCTION_SIZE, self->cpu) ) {
+      size_t size =_patcher_disassemble_size(self, &curr_instruction_code[0], addrs[i], X86_INS_CMP);
+      assert(size != 0); //csopen failed, shouldn't happen
+      _patcher_save_patch(self, i, &curr_instruction_code[0], size, addrs[i]);
+    }
+  }
+}
+
+static void print_hexdump(const uint8_t* addr, size_t size){
+  for(size_t i = 0; i < size; i++){
+	  printf (" %02x", addr[i]);
+  }
+  printf("\n");
+}
+
+bool patcher_validate_patches(patcher_t *self){
+  bool was_rq = _redq_ptr(self)->hooks_applied;
+  if(was_rq)
+    redqueen_remove_hooks(_redq_ptr(self));
+  if(!self->patches){return true;}
+  for(size_t i=0; i<self->num_patches; i++){
+    uint8_t buf[MAX_INSTRUCTION_SIZE];
+    read_virtual_memory(self->patches[i].addr, &buf[0], MAX_INSTRUCTION_SIZE, self->cpu);
+    const uint8_t* should_value = NULL;
+    if(self->is_currently_applied){
+      should_value = cmp_patch;
+    } else {
+      should_value = &self->patches[i].orig_bytes[0];
+    }
+
+    QEMU_PT_PRINTF(REDQUEEN_PREFIX, "Validating, mem:");
+    print_hexdump(&buf[0], self->patches[i].size);
+    QEMU_PT_PRINTF(REDQUEEN_PREFIX, "should_be:");
+    print_hexdump(should_value, self->patches[i].size);
+    if(0 != memcmp(&buf[0], should_value, self->patches[i].size)){
+      QEMU_PT_PRINTF(REDQUEEN_PREFIX, "validating patches failed self->is_currently_applied = %d",  self->is_currently_applied);
+      return false;
+    }
+  }
+  if(was_rq)
+    redqueen_insert_hooks(_redq_ptr(self));
+  return true;
+}
+
+
+///////////////////////////////////////////////////////////////////////////////////
+// Private Helper Functions Definitions
+///////////////////////////////////////////////////////////////////////////////////
+
+
+static void _patcher_apply_patch(patcher_t *self, size_t index) {
+  patch_info_t *info = &self->patches[index];
+	write_virtual_shadow_memory_cr3(info->addr, (uint8_t*)cmp_patch, info->size, self->cpu, GET_GLOBAL_STATE()->parent_cr3); //self->cpu->parent_cr3);
+}
+
+static void _patcher_restore_patch(patcher_t *self, size_t index){
+  patch_info_t *info = &self->patches[index];
+	write_virtual_shadow_memory_cr3(info->addr, (uint8_t*)&info->orig_bytes[0], info->size, self->cpu, GET_GLOBAL_STATE()->parent_cr3); //self->cpu->parent_cr3);
+}
+
+static void _patcher_save_patch(patcher_t *self, size_t index, uint8_t* data, size_t instruction_size, uint64_t addr) {
+  assert(instruction_size >= 2);
+  assert(instruction_size < MAX_INSTRUCTION_SIZE);
+  patch_info_t *info = &self->patches[index];
+  memset(&info->orig_bytes[0], 0, MAX_INSTRUCTION_SIZE);
+  memcpy(&info->orig_bytes[0], data, instruction_size);
+  info->addr = addr;
+  info->size = instruction_size;
+}
+
+static size_t _patcher_disassemble_size(patcher_t *self, uint8_t* data, uint64_t addr, x86_insn type){
+
+    csh handle;
+    if (cs_open(CS_ARCH_X86, get_capstone_mode(GET_GLOBAL_STATE()->disassembler_word_width), &handle) == CS_ERR_OK){
+      cs_insn *insn = cs_malloc(handle);
+      uint8_t* cur_offset = data;
+      uint64_t cs_address = addr;
+      uint64_t code_size = MAX_INSTRUCTION_SIZE;
+      cs_disasm_iter(handle, (const uint8_t **) &cur_offset, &code_size, &cs_address, insn);
+      size_t size = insn->size;
+      if(type != X86_INS_INVALID){
+        assert(insn->id == type);
+      }
+      cs_free(insn, 1);
+      cs_close(&handle);
+      return size;
+    }
+    return 0;
+}
+
+static void _patcher_alloc_patch_infos(patcher_t *self, size_t num_patches){
+  assert(self->num_patches == 0);
+  assert(self->patches == NULL);
+  assert(num_patches < 10000);
+  self->num_patches = num_patches;
+  self->patches = malloc(sizeof(patch_info_t)*num_patches);
+}
+
+static void _patcher_free_patch_infos(patcher_t *self){
+  assert(!self->is_currently_applied);
+  free(self->patches);
+  self->patches = NULL;
+  self->num_patches = 0;
+}
+
+static redqueen_t* _redq_ptr(patcher_t *self){
+  redqueen_t* res = GET_GLOBAL_STATE()->redqueen_state; //self->cpu->redqueen_state;
+  return res;
+}
diff --new-file -ur qemu/pt/patcher.h QEMU-PT/pt/patcher.h
--- qemu/pt/patcher.h	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/patcher.h	2021-08-24 21:54:55.938586449 +0200
@@ -0,0 +1,45 @@
+#ifndef __GUARD_REDQUEEN_PATCHER_STRUCT__
+#define __GUARD_REDQUEEN_PATCHER_STRUCT__
+
+#include <stdint.h>
+#include <stddef.h>
+
+#include <capstone/capstone.h>
+#include <capstone/x86.h>
+
+#include "qemu/osdep.h"
+
+#define MAX_INSTRUCTION_SIZE 64
+//Patch used to replace cmp instructions. It encodes CMP AL, AL a comparision which always evaluates to true. This can
+//be used to remove hash checks that we suspsect can later on be patched.
+extern const uint8_t* cmp_patch; 
+
+typedef struct patch_info_s{
+  uint64_t addr;
+  size_t size;
+  uint8_t orig_bytes[MAX_INSTRUCTION_SIZE];
+} patch_info_t;
+
+typedef struct patcher_s{
+
+  CPUState *cpu;
+
+  patch_info_t *patches;
+  size_t num_patches;
+  bool is_currently_applied;
+} patcher_t;
+
+patcher_t* patcher_new(CPUState *cpu);
+
+void patcher_free(patcher_t *self);
+
+void patcher_apply_all(patcher_t *self);
+
+void patcher_restore_all(patcher_t *self);
+
+//Doesn't take ownership of addrs
+void patcher_set_addrs(patcher_t *self, uint64_t* addrs, size_t num_addrs);
+
+bool patcher_validate_patches(patcher_t *self);
+
+#endif
diff --new-file -ur qemu/pt/redqueen.c QEMU-PT/pt/redqueen.c
--- qemu/pt/redqueen.c	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/redqueen.c	2021-08-24 21:54:55.942586446 +0200
@@ -0,0 +1,728 @@
+/*
+
+Copyright (C) 2017 Sergej Schumilo
+
+This file is part of QEMU-PT (kAFL).
+
+QEMU-PT is free software: you can redistribute it and/or modify
+it under the terms of the GNU General Public License as published by
+the Free Software Foundation, either version 2 of the License, or
+(at your option) any later version.
+
+QEMU-PT is distributed in the hope that it will be useful,
+but WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with QEMU-PT.  If not, see <http://www.gnu.org/licenses/>.
+
+*/
+
+#include <assert.h>
+#include "pt/redqueen.h"
+#include "pt/memory_access.h"
+#include "pt/disassembler.h"
+#include "pt/interface.h"
+#include <inttypes.h>
+#include "file_helper.h"
+#include "patcher.h"
+#include "debug.h"
+#include "redqueen_trace.h"
+#include "pt/state.h"
+#include <capstone/capstone.h>
+#include <capstone/x86.h>
+
+redqueen_workdir_t redqueen_workdir = {0};
+
+void setup_redqueen_workdir(char* workdir){
+   assert(asprintf(&redqueen_workdir.redqueen_results,"%s/redqueen_results.txt", workdir)>0);
+   assert(asprintf(&redqueen_workdir.symbolic_results,"%s/symbolic_results.txt", workdir)>0);
+   assert(asprintf(&redqueen_workdir.pt_trace_results,"%s/pt_trace_results.txt", workdir)>0);
+   assert(asprintf(&redqueen_workdir.redqueen_patches,"%s/redqueen_patches.txt", workdir)>0);
+   assert(asprintf(&redqueen_workdir.breakpoint_white,"%s/breakpoint_white.txt", workdir)>0);
+   assert(asprintf(&redqueen_workdir.breakpoint_black,"%s/breakpoint_black.txt", workdir)>0);
+   assert(asprintf(&redqueen_workdir.target_code_dump,"%s/target_code_dump.img", workdir)>0);
+}
+
+redqueen_t* new_rq_state(CPUState *cpu,  page_cache_t* page_cache){
+	redqueen_t* res = malloc(sizeof(redqueen_t));
+
+	res->cpu = cpu;
+	res->intercept_mode = false;
+	res->trace_mode = false;
+	res->singlestep_enabled = false;
+  	res->hooks_applied = 0;
+  	res->page_cache = page_cache;
+
+  	res->lookup = kh_init(RQ);
+	res->last_rip = 0x0;
+	res->next_rip = 0x0;
+	res->num_breakpoint_whitelist=0;
+	res->breakpoint_whitelist=NULL;
+
+	res->trace_state=redqueen_trace_new();
+
+	return res;
+}
+
+void redqueen_set_trace_mode(redqueen_t* self){
+	delete_trace_files();
+	self->trace_mode = true;
+}
+
+void redqueen_unset_trace_mode(redqueen_t* self){
+	self->trace_mode = false;
+}
+
+void destroy_rq_state(redqueen_t* self){
+	redqueen_trace_free(self->trace_state);
+	kh_destroy(RQ, self->lookup);
+	free(self);
+}
+
+static void redqueen_set_addr_flags(redqueen_t* self, uint64_t addr, uint32_t flags){
+	int unused = 0;
+	khiter_t k = kh_get(RQ, self->lookup, addr); 
+	if(k == kh_end(self->lookup)){
+		k = kh_put(RQ, self->lookup, addr, &unused); 
+		kh_value(self->lookup, k) = 0;
+	}
+	kh_value(self->lookup, k) |= flags;
+}	
+
+static bool redqueen_check_addr_flags(redqueen_t* self, uint64_t addr, uint32_t flags){
+	khiter_t k = kh_get(RQ, self->lookup, addr); 
+	if(k != kh_end(self->lookup)){
+		return !!(kh_value(self->lookup, k) & flags);
+	}
+	else{
+		return false;
+	}
+}
+
+static bool redqueen_check_addr(redqueen_t* self, uint64_t addr){
+	khiter_t k = kh_get(RQ, self->lookup, addr); 
+	if(k != kh_end(self->lookup)){
+		return true;
+	}
+	else{
+		return false;
+	}
+}
+
+static uint32_t redqueen_update_addr_count(redqueen_t* self, uint64_t addr){
+	int unused __attribute__((unused));
+	uint32_t value = 0;
+	khiter_t k = kh_get(RQ, self->lookup, addr); 
+	if(k != kh_end(self->lookup)){
+		value = kh_value(self->lookup, k); 
+	}
+	else{
+		k = kh_put(RQ, self->lookup, addr, &unused); 
+	}
+	value++;
+	kh_value(self->lookup, k) = value;
+	return value & 0xFF000000UL;
+}
+
+void set_rq_instruction(redqueen_t* self, uint64_t addr){
+	if( !redqueen_check_addr_flags(self, addr, CMP_BITMAP_BLACKLISTED)){
+		redqueen_set_addr_flags(self, addr, CMP_BITMAP_RQ_INSTRUCTION);
+	}
+}
+
+void set_rq_blacklist(redqueen_t* self, uint64_t addr){
+	redqueen_set_addr_flags(self, addr, CMP_BITMAP_BLACKLISTED);
+}
+
+static void insert_hooks_whitelist(redqueen_t* self){
+	fprintf(stderr, "%s\n", __func__);
+  for(size_t i = 0; i < self->num_breakpoint_whitelist; i++){
+		insert_breakpoint(self->cpu, self->breakpoint_whitelist[i], 1);
+  }
+}
+
+static void insert_hooks_bitmap(redqueen_t* self){
+	uint64_t c = 0;
+
+	uint64_t addr;
+	uint32_t value __attribute__((unused));
+  uint32_t mode = GET_GLOBAL_STATE()->redqueen_instrumentation_mode;
+	kh_foreach(self->lookup, addr, value, {
+		if(redqueen_check_addr_flags(self, addr, CMP_BITMAP_BLACKLISTED)){ continue; }
+
+		bool should_hook_rq = (mode == REDQUEEN_LIGHT_INSTRUMENTATION ) && redqueen_check_addr_flags(self, addr, CMP_BITMAP_SHOULD_HOOK_RQ);
+
+		if( should_hook_rq ){
+			insert_breakpoint(self->cpu, addr, 1);
+			c++;
+		}
+	});
+}
+
+void redqueen_insert_hooks(redqueen_t* self){
+
+  QEMU_PT_PRINTF(REDQUEEN_PREFIX, "insert hooks");
+  assert(!self->hooks_applied);
+  switch(GET_GLOBAL_STATE()->redqueen_instrumentation_mode){
+    case(REDQUEEN_LIGHT_INSTRUMENTATION):
+      insert_hooks_bitmap(self);
+      break;
+    case(REDQUEEN_WHITELIST_INSTRUMENTATION):
+      insert_hooks_whitelist(self);
+      break;
+    case(REDQUEEN_NO_INSTRUMENTATION):
+      break;
+    default:
+      assert(false);
+  }
+  self->hooks_applied = 1;
+}
+
+void redqueen_remove_hooks(redqueen_t* self){
+  QEMU_PT_PRINTF(REDQUEEN_PREFIX, "remove hooks");
+  assert(self->hooks_applied);
+	remove_all_breakpoints(self->cpu);
+
+	for (khiter_t i = kh_begin(self->lookup); i != kh_end(self->lookup); ++i) {
+		if (!kh_exist(self->lookup,i)) continue;
+		kh_val(self->lookup,i) &= 0xFF000000UL;
+	}
+  self->hooks_applied = 0;
+  return;
+}
+static uint64_t get_segment_register(x86_reg reg) {
+  X86CPU *cpu = X86_CPU(qemu_get_cpu(0));
+  CPUX86State *env = &cpu->env;
+  switch(reg){
+    case X86_REG_GS: return env->segs[R_GS].base;
+    case X86_REG_FS: return env->segs[R_FS].base;
+    case X86_REG_CS: return env->segs[R_CS].base;
+    case X86_REG_DS: return env->segs[R_DS].base;
+    case X86_REG_SS: return env->segs[R_SS].base;
+    default:
+      break;
+  }
+  assert(false);
+}
+
+static inline uint64_t sign_extend_from_size(uint64_t value, uint8_t size){
+  switch(size){
+    case 64: return value;
+    case 32: return ((int32_t)(value)<0) ? 0xffffffff00000000 | value : value;
+    case 16: return ((int16_t)(value)<0) ? 0xffffffffffff0000 | value : value;
+    case 8: return  (( int8_t)(value)<0) ? 0xffffffffffffff00 | value : value;
+  }
+  assert(false);
+}
+
+static uint64_t eval_reg(x86_reg reg, uint8_t *size){
+  uint64_t value = 0;
+  CPUX86State *env = &(X86_CPU(qemu_get_cpu(0)))->env;
+
+  switch(reg) {
+    case X86_REG_RAX: value=env->regs[RAX]; *size=64; break;
+    case X86_REG_RCX: value=env->regs[RCX]; *size=64;  break;
+    case X86_REG_RDX: value=env->regs[RDX]; *size=64;  break;
+    case X86_REG_RBX: value=env->regs[RBX]; *size=64;  break;
+    case X86_REG_RSP: value=env->regs[RSP]; *size=64;  break;
+    case X86_REG_RBP: value=env->regs[RBP]; *size=64;  break;
+    case X86_REG_RSI: value=env->regs[RSI]; *size=64;  break;
+    case X86_REG_RDI: value=env->regs[RDI]; *size=64;  break;
+    case X86_REG_R8: value=env->regs[R8]; *size=64;  break;
+    case X86_REG_R9: value=env->regs[R9]; *size=64;  break;
+    case X86_REG_R10: value=env->regs[R10]; *size=64;  break;
+    case X86_REG_R11: value=env->regs[R11]; *size=64;  break;
+    case X86_REG_R12: value=env->regs[R12]; *size=64;  break;
+    case X86_REG_R13: value=env->regs[R13]; *size=64;  break;
+    case X86_REG_R14: value=env->regs[R14]; *size=64;  break;
+    case X86_REG_R15: value=env->regs[R15]; *size=64;  break;
+    case X86_REG_EAX: value=env->regs[RAX]&0xffffffff; *size=32; break;
+    case X86_REG_ECX: value=env->regs[RCX]&0xffffffff; *size=32; break;
+    case X86_REG_EDX: value=env->regs[RDX]&0xffffffff; *size=32; break;
+    case X86_REG_EBX: value=env->regs[RBX]&0xffffffff; *size=32; break;
+    case X86_REG_ESP: value=env->regs[RSP]&0xffffffff; *size=32; break;
+    case X86_REG_EBP: value=env->regs[RBP]&0xffffffff; *size=32; break;
+    case X86_REG_ESI: value=env->regs[RSI]&0xffffffff; *size=32; break;
+    case X86_REG_EDI: value=env->regs[RDI]&0xffffffff; *size=32; break;
+    case X86_REG_R8D: value=env->regs[R8]&0xffffffff; *size=32; break;
+    case X86_REG_R9D: value=env->regs[R9]&0xffffffff; *size=32; break;
+    case X86_REG_R10D: value=env->regs[R10]&0xffffffff; *size=32; break;
+    case X86_REG_R11D: value=env->regs[R11]&0xffffffff; *size=32; break;
+    case X86_REG_R12D: value=env->regs[R12]&0xffffffff; *size=32; break;
+    case X86_REG_R13D: value=env->regs[R13]&0xffffffff; *size=32; break;
+    case X86_REG_R14D: value=env->regs[R14]&0xffffffff; *size=32; break;
+    case X86_REG_R15D: value=env->regs[R15]&0xffffffff; *size=32; break;
+    case X86_REG_AX: value=env->regs[RAX]&0xffff; *size=16; break;
+    case X86_REG_CX: value=env->regs[RCX]&0xffff; *size=16; break;
+    case X86_REG_DX: value=env->regs[RDX]&0xffff; *size=16; break;
+    case X86_REG_BX: value=env->regs[RBX]&0xffff; *size=16; break;
+    case X86_REG_SP: value=env->regs[RSP]&0xffff; *size=16; break;
+    case X86_REG_BP: value=env->regs[RBP]&0xffff; *size=16; break;
+    case X86_REG_SI: value=env->regs[RSI]&0xffff; *size=16; break;
+    case X86_REG_DI: value=env->regs[RDI]&0xffff; *size=16; break;
+    case X86_REG_R8W: value=env->regs[R8]&0xffff; *size=16; break;
+    case X86_REG_R9W: value=env->regs[R9]&0xffff; *size=16; break;
+    case X86_REG_R10W: value=env->regs[R10]&0xffff; *size=16; break;
+    case X86_REG_R11W: value=env->regs[R11]&0xffff; *size=16; break;
+    case X86_REG_R12W: value=env->regs[R12]&0xffff; *size=16; break;
+    case X86_REG_R13W: value=env->regs[R13]&0xffff; *size=16; break;
+    case X86_REG_R14W: value=env->regs[R14]&0xffff; *size=16; break;
+    case X86_REG_R15W: value=env->regs[R15]&0xffff; *size=16; break;
+    case X86_REG_AL: value=env->regs[RAX]&0xff; *size=8; break;
+    case X86_REG_CL: value=env->regs[RCX]&0xff; *size=8; break;
+    case X86_REG_DL: value=env->regs[RDX]&0xff; *size=8; break;
+    case X86_REG_BL: value=env->regs[RBX]&0xff; *size=8; break;
+    case X86_REG_SPL: value=env->regs[RSP]&0xff; *size=8; break;
+    case X86_REG_BPL: value=env->regs[RBP]&0xff; *size=8; break;
+    case X86_REG_SIL: value=env->regs[RSI]&0xff; *size=8; break;
+    case X86_REG_DIL: value=env->regs[RDI]&0xff; *size=8; break;
+    case X86_REG_R8B: value=env->regs[R8]&0xff; *size=8; break;
+    case X86_REG_R9B: value=env->regs[R9]&0xff; *size=8; break;
+    case X86_REG_R10B: value=env->regs[R10]&0xff; *size=8; break;
+    case X86_REG_R11B: value=env->regs[R11]&0xff; *size=8; break;
+    case X86_REG_R12B: value=env->regs[R12]&0xff; *size=8; break;
+    case X86_REG_R13B: value=env->regs[R13]&0xff; *size=8; break;
+    case X86_REG_R14B: value=env->regs[R14]&0xff; *size=8; break;
+    case X86_REG_R15B: value=env->regs[R15]&0xff; *size=8; break;
+    case X86_REG_AH: value=(env->regs[RAX]>>8)&0xff; *size=8;  break;
+    case X86_REG_CH: value=(env->regs[RCX]>>8)&0xff; *size=8;  break;
+    case X86_REG_DH: value=(env->regs[RDX]>>8)&0xff; *size=8;  break;
+    case X86_REG_BH: value=(env->regs[RBX]>>8)&0xff; *size=8;  break;
+    case X86_REG_RIP: value=env->eip; *size=64; break;
+    case X86_REG_EIP: value=env->eip&0xffffffff; *size=32; break;
+    case X86_REG_IP:  value=env->eip&0xfffff; *size=16; break;
+    default:
+      assert(false);
+  }
+  return value;
+}
+
+static uint64_t eval_addr(cs_x86_op* op){
+  uint8_t size=0;
+  uint64_t base = 0; 
+  uint64_t index = 0;
+  uint64_t segment = 0;
+
+  assert(op->type == X86_OP_MEM);
+
+  if(op->mem.base != X86_REG_INVALID){
+    base = eval_reg(op->mem.base, &size);
+  }
+  if(op->mem.index != X86_REG_INVALID){
+    index = eval_reg(op->mem.index, &size);
+  }
+
+  if(op->mem.segment != X86_REG_INVALID){
+    segment = get_segment_register(op->mem.segment);
+  }
+
+  uint64_t addr = segment + base + index*op->mem.scale + op->mem.disp;
+  return addr;
+}
+
+static uint64_t eval_mem(cs_x86_op* op){
+
+  uint64_t val = 0;
+  assert(op->size == 1 || op->size == 2 || op->size == 4  || op->size == 8);
+
+  /* TODO @ sergej: replace me later */
+  read_virtual_memory(eval_addr(op), (uint8_t*) &val, op->size, qemu_get_cpu(0));
+  return val;
+}
+
+static uint64_t eval(cs_x86_op* op, uint8_t* size){
+  switch((int)op->type) {
+    case X86_OP_REG:
+      return eval_reg(op->reg, size);
+    case X86_OP_IMM:
+      *size=0;
+      return op->imm;
+    case X86_OP_MEM:
+      switch(op->size){
+        case 1: *size =8;  return eval_mem(op)&0xff;
+        case 2: *size =16; return eval_mem(op)&0xffff;
+        case 4: *size =32; return eval_mem(op)&0xffffffff;
+        case 8: *size =64; return eval_mem(op);      
+      }
+  }
+  
+  /* unreachable! */
+  assert(false);
+  return 0;
+}
+
+static void print_comp_result(uint64_t addr, const char* type, uint64_t val1, uint64_t val2, uint8_t size, bool is_imm){
+
+	char result_buf[256]; 
+  const char *format = NULL;
+	uint8_t pos = 0;
+			pos += snprintf(result_buf+pos, 256-pos, "%lx\t\t %s", addr, type);
+      uint64_t mask = 0;
+			switch(size){
+				case 64: format = " 64\t%016lX-%016lX"; mask = 0xffffffffffffffff;  break;
+				case 32: format = " 32\t%08X-%08X";     mask = 0xffffffff;          break;
+				case 16: format = " 16\t%04X-%04X";     mask = 0xffff;              break;
+				case 8:  format = " 8\t%02X-%02X";      mask = 0xff;                break;
+        default:
+          assert(false);
+			}
+			pos += snprintf(result_buf+pos, 256-pos, format, val1 & mask, val2 & mask);
+			if(is_imm){
+				pos += snprintf(result_buf+pos, 256-pos, " IMM");
+			}
+			pos += snprintf(result_buf+pos, 256-pos, "\n");
+			write_re_result(result_buf);
+}
+
+static void get_cmp_value(cs_insn *ins, const char* type){
+  uint8_t size_1=0;
+  uint8_t size_2=0;
+
+	assert(ins);
+	cs_x86 *x86 = &(ins->detail->x86);
+
+	assert(x86->op_count == 2);
+	cs_x86_op *op1 = &(x86->operands[0]);
+	cs_x86_op *op2 = &(x86->operands[1]);
+
+  uint64_t v1 = eval(op1, &size_1);
+  uint64_t v2 = eval(op2, &size_2);
+
+  if(GET_GLOBAL_STATE()->redqueen_instrumentation_mode == REDQUEEN_WHITELIST_INSTRUMENTATION  ||  v1 != v2){
+     print_comp_result(ins->address, type, v1, v2, (size_1 ? size_1 : size_2), op2->type == X86_OP_IMM);
+  }
+}
+
+static void get_cmp_value_add(cs_insn *ins){
+  uint8_t size_1=0;
+  uint8_t size_2=0;
+
+	assert(ins);
+	cs_x86 *x86 = &(ins->detail->x86);
+
+	assert(x86->op_count == 2);
+	cs_x86_op *op1 = &(x86->operands[0]);
+	cs_x86_op *op2 = &(x86->operands[1]);
+
+  uint64_t v1 = eval(op1, &size_1);
+  uint64_t v2 = -sign_extend_from_size(eval(op2, &size_2), size_1);
+
+  if(op2->type != X86_OP_IMM){
+    return;
+  }
+
+  if(GET_GLOBAL_STATE()->redqueen_instrumentation_mode == REDQUEEN_WHITELIST_INSTRUMENTATION  ||  v1 != v2){
+    bool is_imm = true;
+    print_comp_result(ins->address, "SUB", v1, v2, size_1, is_imm);
+  }
+}
+
+static void get_cmp_value_lea(cs_insn *ins){
+  uint64_t index_val = 0;
+
+	assert(ins);
+	cs_x86 *x86 = &(ins->detail->x86);
+
+	assert(x86->op_count == 2);
+	cs_x86_op *op2 = &(x86->operands[1]);
+
+  assert(op2->type == X86_OP_MEM);
+
+  uint8_t size=0;
+  if(op2->mem.base != X86_REG_INVALID && op2->mem.index != X86_REG_INVALID){
+    return;
+  }
+
+  if(op2->mem.base == X86_REG_INVALID && op2->mem.index == X86_REG_INVALID){
+    return;
+  }
+
+  if(op2->mem.base != X86_REG_INVALID ){
+    index_val = eval_reg(op2->mem.base, &size);
+  }
+
+  if(op2->mem.index != X86_REG_INVALID ){
+    index_val = eval_reg(op2->mem.index, &size);
+  }
+
+  if(GET_GLOBAL_STATE()->redqueen_instrumentation_mode == REDQUEEN_WHITELIST_INSTRUMENTATION  ||  index_val != -op2->mem.disp){
+      bool is_imm = false;
+    print_comp_result(ins->address, "LEA", index_val, -op2->mem.disp, op2->size*8, is_imm);
+  }
+}
+
+
+static uint64_t limit_to_word_width(uint64_t val){
+	switch(GET_GLOBAL_STATE()->disassembler_word_width){
+	case 64:
+		return val;
+	case 32: 
+		return val & 0xffffffff;
+	default:
+		assert(false);
+	}
+}
+
+static uint64_t word_width_to_bytes(void){
+	switch(GET_GLOBAL_STATE()->disassembler_word_width){
+	case 64:
+		return 8;
+	case 32: 
+		return 4;
+	default:
+		assert(false);
+	}
+}
+
+static uint64_t read_stack(uint64_t word_index){
+	CPUX86State *env = &(X86_CPU(qemu_get_cpu(0)))->env;
+	uint64_t rsp = env->regs[RSP];
+	rsp = limit_to_word_width(rsp);
+	uint64_t res = 0;
+	uint64_t stack_ptr = rsp + word_index * word_width_to_bytes();
+  /* TODO @ sergej */
+	assert(read_virtual_memory(stack_ptr, (uint8_t*)(&res), 8, qemu_get_cpu(0)));
+	return limit_to_word_width(res);
+}
+
+static void format_strcmp(uint8_t* buf1, uint8_t* buf2){
+	char out_buf[REDQUEEN_MAX_STRCMP_LEN*4 + 2];
+	char* tmp_hex_buf = &out_buf[0];
+	for(int i = 0; i < REDQUEEN_MAX_STRCMP_LEN; i++){
+		tmp_hex_buf += sprintf(tmp_hex_buf, "%02X", (uint8_t)buf1[i]);
+	}
+	*tmp_hex_buf++ = '-';
+	for(int i = 0; i < REDQUEEN_MAX_STRCMP_LEN; i++){
+		tmp_hex_buf += sprintf(tmp_hex_buf, "%02X", (uint8_t)buf2[i]);
+	}
+	char *res=0;
+	CPUX86State *env = &(X86_CPU(qemu_get_cpu(0)))->env;
+	uint64_t rip = env->eip;
+	assert(asprintf( &res, "%lx\t\tSTR %d\t%s\n", rip, REDQUEEN_MAX_STRCMP_LEN*8, out_buf ) != -1);
+	write_re_result(res);
+	free(res);
+}
+
+static bool test_strchr(uint64_t arg1, uint64_t arg2){
+  CPUState *cpu = qemu_get_cpu(0);
+
+  /* TODO @ sergej */
+	if(!is_addr_mapped(arg1, cpu) || arg2 & (~0xff)){
+    return false;
+  }
+	uint8_t buf1[REDQUEEN_MAX_STRCMP_LEN];
+	uint8_t buf2[REDQUEEN_MAX_STRCMP_LEN];
+
+  /* TODO @ sergej */
+	assert(read_virtual_memory(arg1, &buf1[0], REDQUEEN_MAX_STRCMP_LEN, cpu));
+  if(!memchr(buf1,'\0',REDQUEEN_MAX_STRCMP_LEN) ){return false;}
+  memset(buf2,'\0',REDQUEEN_MAX_STRCMP_LEN);
+  buf2[0]=  (uint8_t)(arg2);
+  format_strcmp(buf1, buf2);
+  return true;
+}
+
+static bool test_strcmp(uint64_t arg1, uint64_t arg2){
+  CPUState *cpu = qemu_get_cpu(0);
+	if(!is_addr_mapped(arg1, cpu) || ! is_addr_mapped(arg2, cpu)){
+		return false;
+	}
+	uint8_t buf1[REDQUEEN_MAX_STRCMP_LEN];
+	uint8_t buf2[REDQUEEN_MAX_STRCMP_LEN];
+  /* TODO @ sergej */
+	assert(read_virtual_memory(arg1, &buf1[0], REDQUEEN_MAX_STRCMP_LEN, cpu));
+	assert(read_virtual_memory(arg2, &buf2[0], REDQUEEN_MAX_STRCMP_LEN, cpu));
+  format_strcmp(buf1,buf2);
+	return true;
+}
+
+static bool test_strcmp_cdecl(void){
+	uint64_t arg1 = read_stack(0);
+	uint64_t arg2 = read_stack(1);
+  test_strchr(arg1, arg2);
+	return test_strcmp(arg1, arg2) ;
+
+}
+
+static bool test_strcmp_fastcall(void){
+	CPUX86State *env = &(X86_CPU(qemu_get_cpu(0)))->env;
+	uint64_t arg1 = env->regs[RCX]; //rcx
+	uint64_t arg2 = env->regs[RDX]; //rdx
+  test_strchr(arg1, arg2);
+	return test_strcmp(arg1, arg2);
+}
+
+static bool test_strcmp_sys_v(void){
+	if(GET_GLOBAL_STATE()->disassembler_word_width != 64 ){return false;}
+	CPUX86State *env = &(X86_CPU(qemu_get_cpu(0)))->env;
+	uint64_t arg1 = env->regs[RDI]; //rdx
+	uint64_t arg2 = env->regs[RSI]; //rsi
+  test_strchr(arg1, arg2);
+	return test_strcmp(arg1, arg2);
+}
+
+static void extract_call_params(void){
+	test_strcmp_cdecl();
+	test_strcmp_fastcall();
+	test_strcmp_sys_v();
+}
+
+static void handle_hook_redqueen_light(redqueen_t* self, uint64_t ip, cs_insn *insn){
+	if(insn->id == X86_INS_CMP || insn->id == X86_INS_XOR){ //handle original redqueen case
+		get_cmp_value(insn, "CMP");
+  } else if(insn->id == X86_INS_SUB){ //handle original redqueen case
+		get_cmp_value(insn, "SUB");
+  } else if(insn->id == X86_INS_LEA){ //handle original redqueen case
+		get_cmp_value_lea(insn);
+  } else if(insn->id == X86_INS_ADD){ //handle original redqueen case
+		get_cmp_value_add(insn);
+	} else if (insn->id == X86_INS_CALL || insn->id == X86_INS_LCALL){
+		extract_call_params();
+	}
+}
+
+static uint8_t handle_hook_breakpoint(redqueen_t* self, bool write_data){
+
+  X86CPU *cpu = X86_CPU(self->cpu);
+  CPUX86State *env = &cpu->env;
+
+  cs_insn *insn = NULL;
+  switch(GET_GLOBAL_STATE()->disassembler_word_width){
+    case 64:
+      insn = page_cache_cs_malloc(self->page_cache, mode_64);
+      break;
+    case 32: 
+      insn = page_cache_cs_malloc(self->page_cache, mode_32);
+      break;
+    default:
+      abort();
+  }
+  uint8_t ins_size = 0;
+  uint64_t ip = env->eip;
+  uint64_t code = ip;
+  uint64_t failed_page = 0;
+
+  switch(GET_GLOBAL_STATE()->disassembler_word_width){
+      case 64:
+        assert(page_cache_disassemble_iter(self->page_cache, &code, insn, &failed_page, mode_64));
+        break;
+      case 32: 
+        assert(page_cache_disassemble_iter(self->page_cache, &code, insn, &failed_page, mode_32));
+        break;
+      default:
+        abort();
+    }
+
+	ins_size = insn->size;
+
+	if(write_data){
+        int mode = GET_GLOBAL_STATE()->redqueen_instrumentation_mode;
+        if(mode == REDQUEEN_LIGHT_INSTRUMENTATION || mode == REDQUEEN_WHITELIST_INSTRUMENTATION || mode == REDQUEEN_SE_INSTRUMENTATION){
+          handle_hook_redqueen_light(self, ip, insn);
+        }
+        if(mode == REDQUEEN_SE_INSTRUMENTATION){
+          assert(false);
+        }
+    }
+	cs_free(insn, 1);
+
+    assert(ins_size != 0);
+    return ins_size;
+}
+
+void handle_hook(redqueen_t* self){
+  X86CPU *cpu = X86_CPU(self->cpu);
+  CPUX86State *env = &cpu->env;
+
+  if (self->next_rip){
+
+  	remove_breakpoint(self->cpu, self->next_rip, 1);
+
+  	if(self->last_rip && redqueen_update_addr_count(self, self->last_rip) < REDQUEEN_TRAP_LIMIT){
+	  insert_breakpoint(self->cpu, self->last_rip, 1);
+    }
+
+    kvm_update_guest_debug(self->cpu, 0);
+
+    self->last_rip = 0;
+    self->next_rip = 0;
+  }
+
+  if(redqueen_check_addr(self, env->eip)){
+
+  	self->last_rip = env->eip;
+    remove_breakpoint(self->cpu, env->eip, 1);
+
+    if(self->cpu->pt_enabled && GET_GLOBAL_STATE()->pt_c3_filter == env->cr[3]){
+    	self->next_rip = handle_hook_breakpoint(self, true);
+    }
+    else{
+    	self->next_rip = handle_hook_breakpoint(self, true);
+    }
+  }
+}
+
+uint64_t last_ip = 0;
+
+void redqueen_register_transition(redqueen_t* self, uint64_t src, uint64_t target){
+	int unused __attribute__((unused));
+	if(self->trace_mode){
+		redqueen_trace_register_transition(self->trace_state, src, target);
+    last_ip = target;
+	}
+}
+
+void redqueen_trace_enabled(redqueen_t* self, uint64_t ip){
+	int unused __attribute__((unused));
+	if(self->trace_mode){
+		redqueen_trace_register_transition(self->trace_state, INIT_TRACE_IP, ip);
+    last_ip = ip;
+	} 
+}
+
+void redqueen_trace_disabled(redqueen_t* self, uint64_t ip){
+  int unused __attribute__((unused));
+	if(self->trace_mode){
+		redqueen_trace_register_transition(self->trace_state, last_ip, ip);
+		redqueen_trace_register_transition(self->trace_state, ip, INIT_TRACE_IP);
+	}
+}
+
+static void _redqueen_update_whitelist(redqueen_t* self){
+  if(GET_GLOBAL_STATE()->redqueen_instrumentation_mode == REDQUEEN_WHITELIST_INSTRUMENTATION){
+    free(self->breakpoint_whitelist);
+    parse_address_file(redqueen_workdir.breakpoint_white, &self->num_breakpoint_whitelist, &self->breakpoint_whitelist);
+  }
+}
+
+static void _redqueen_update_blacklist(redqueen_t* self){
+  if(GET_GLOBAL_STATE()->redqueen_update_blacklist){
+    size_t num_addrs = 0;
+    uint64_t *addrs;
+    parse_address_file(redqueen_workdir.breakpoint_black, &num_addrs, &addrs);
+    for(size_t i = 0; i< num_addrs; i++){
+      set_rq_blacklist(self, addrs[i]);
+    }
+    free(addrs);
+  }
+}
+
+void enable_rq_intercept_mode(redqueen_t* self){
+	if(!self->intercept_mode){
+		delete_redqueen_files();
+		//unlink("/tmp/redqueen_result.txt");
+    _redqueen_update_whitelist(self);
+    _redqueen_update_blacklist(self);
+		redqueen_insert_hooks(self);
+		self->intercept_mode = true;
+	}
+}
+
+void disable_rq_intercept_mode(redqueen_t* self){
+	if(self->intercept_mode){
+		redqueen_remove_hooks(self);
+		self->intercept_mode = false;
+	}
+}
diff --new-file -ur qemu/pt/redqueen.h QEMU-PT/pt/redqueen.h
--- qemu/pt/redqueen.h	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/redqueen.h	2021-08-24 21:54:55.938586449 +0200
@@ -0,0 +1,128 @@
+/*
+
+Copyright (C) 2017 Sergej Schumilo
+
+This file is part of QEMU-PT (kAFL).
+
+QEMU-PT is free software: you can redistribute it and/or modify
+it under the terms of the GNU General Public License as published by
+the Free Software Foundation, either version 2 of the License, or
+(at your option) any later version.
+
+QEMU-PT is distributed in the hope that it will be useful,
+but WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with QEMU-PT.  If not, see <http://www.gnu.org/licenses/>.
+
+*/
+
+#ifndef REDQUEEN_H
+#define REDQUEEN_H
+
+#include <stddef.h>
+#include <stdlib.h>
+#include <string.h>
+#include <stdint.h>
+#include <stdbool.h>
+#include "qemu/osdep.h"
+#include <linux/kvm.h>
+#include <capstone/capstone.h>
+#include <capstone/x86.h>
+#include "redqueen_trace.h"
+#include "khash.h"
+#include "page_cache.h"
+
+//#define RQ_DEBUG
+
+#define REDQUEEN_MAX_STRCMP_LEN 64
+#define REDQUEEN_TRAP_LIMIT	16
+
+#define REG64_NUM 16
+#define REG32_NUM 16
+//seems we don't want to include rip, since this index is used to acces the qemu cpu structure or something?
+#define REG16_NUM 16 
+#define REG8L_NUM 16
+#define REG8H_NUM  8
+
+#define EXTRA_REG_RIP 16
+#define EXTRA_REG_NOP 17
+
+#define REDQUEEN_NO_INSTRUMENTATION 0
+#define REDQUEEN_LIGHT_INSTRUMENTATION 1
+#define REDQUEEN_SE_INSTRUMENTATION 2
+#define REDQUEEN_WHITELIST_INSTRUMENTATION 3
+
+enum reg_types{RAX, RCX, RDX, RBX, RSP, RBP, RSI, RDI, R8, R9, R10, R11, R12, R13, R14, R15};
+
+#define CMP_BITMAP_NOP					0x0000000UL  
+#define CMP_BITMAP_RQ_INSTRUCTION			0x1000000UL 
+#define CMP_BITMAP_SE_INSTRUCTION			0x2000000UL
+#define CMP_BITMAP_BLACKLISTED	  			0x4000000UL
+#define CMP_BITMAP_TRACE_ENABLED  			0x8000000UL
+#define CMP_BITMAP_SHOULD_HOOK_SE 			(CMP_BITMAP_SE_INSTRUCTION|CMP_BITMAP_TRACE_ENABLED)
+#define CMP_BITMAP_SHOULD_HOOK_RQ 			(CMP_BITMAP_RQ_INSTRUCTION)
+
+KHASH_MAP_INIT_INT64(RQ, uint32_t)
+
+typedef struct redqueen_s{
+	khash_t(RQ) *lookup;
+	bool intercept_mode;
+	bool trace_mode;
+	bool singlestep_enabled;
+	int hooks_applied;
+	CPUState *cpu;
+	uint64_t last_rip;
+	uint64_t next_rip;
+  uint64_t *breakpoint_whitelist;
+  uint64_t num_breakpoint_whitelist;
+  redqueen_trace_t* trace_state; 
+  page_cache_t* page_cache;
+} redqueen_t;
+
+typedef struct redqueen_workdir_s{
+  char* redqueen_results;
+  char* symbolic_results;
+  char* pt_trace_results;
+  char* redqueen_patches;
+  char* breakpoint_white;
+  char* breakpoint_black;
+  char* target_code_dump;
+} redqueen_workdir_t;
+
+extern redqueen_workdir_t redqueen_workdir;
+
+void setup_redqueen_workdir(char* workdir);
+
+redqueen_t* new_rq_state(CPUState *cpu, page_cache_t* page_cache);
+void destroy_rq_state(redqueen_t* self);
+
+void set_rq_instruction(redqueen_t* self, uint64_t addr);
+void set_rq_blacklist(redqueen_t* self, uint64_t addr);
+
+void handle_hook(redqueen_t* self);
+void handel_se_hook(redqueen_t* self);
+
+void enable_rq_intercept_mode(redqueen_t* self);
+void disable_rq_intercept_mode(redqueen_t* self);
+
+
+void redqueen_register_transition(redqueen_t* self, uint64_t ip, uint64_t transition_val);
+void redqueen_trace_enabled(redqueen_t* self, uint64_t ip);
+void redqueen_trace_disabled(redqueen_t* self, uint64_t ip);
+void redqueen_set_trace_mode(redqueen_t* self);
+void redqueen_unset_trace_mode(redqueen_t* self);
+
+void set_se_instruction(redqueen_t* self, uint64_t addr);
+
+void dump_se_registers(redqueen_t* self);
+void dump_se_memory_access(redqueen_t* self, cs_insn* insn);
+void dump_se_return_access(redqueen_t* self, cs_insn* insn);
+void dump_se_memory_access_at(redqueen_t* self, uint64_t instr_addr, uint64_t mem_addr);
+
+void redqueen_insert_hooks(redqueen_t* self);
+void redqueen_remove_hooks(redqueen_t* self);
+
+#endif
diff --new-file -ur qemu/pt/redqueen_patch.c QEMU-PT/pt/redqueen_patch.c
--- qemu/pt/redqueen_patch.c	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/redqueen_patch.c	2021-08-24 21:54:55.938586449 +0200
@@ -0,0 +1,40 @@
+#include "redqueen_patch.h"
+#include "redqueen.h"
+#include "patcher.h"
+#include "file_helper.h"
+#include "debug.h"
+
+///////////////////////////////////////////////////////////////////////////////////
+// Private Helper Functions Declarations
+///////////////////////////////////////////////////////////////////////////////////
+
+void _load_and_set_patches(patcher_t* self);
+
+///////////////////////////////////////////////////////////////////////////////////
+// Public Functions
+///////////////////////////////////////////////////////////////////////////////////
+
+void pt_enable_patches(patcher_t *self){
+  _load_and_set_patches(self);
+  patcher_apply_all(self);
+}
+
+void pt_disable_patches(patcher_t *self){
+  patcher_restore_all(self);
+}
+
+
+///////////////////////////////////////////////////////////////////////////////////
+// Private Helper Functions Definitions
+///////////////////////////////////////////////////////////////////////////////////
+
+
+void _load_and_set_patches(patcher_t* self){
+  size_t num_addrs = 0;
+  uint64_t *addrs = NULL;
+  parse_address_file(redqueen_workdir.redqueen_patches, &num_addrs, &addrs);
+  if(num_addrs){
+    patcher_set_addrs(self, addrs, num_addrs);
+    free(addrs);
+  }
+}
diff --new-file -ur qemu/pt/redqueen_patch.h QEMU-PT/pt/redqueen_patch.h
--- qemu/pt/redqueen_patch.h	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/redqueen_patch.h	2021-08-24 21:54:55.942586446 +0200
@@ -0,0 +1,11 @@
+#ifndef __GUARD_REDQUEEN_PATCH__
+#define __GUARD_REDQUEEN_PATCH__
+
+#include "qemu/osdep.h"
+#include <linux/kvm.h>
+#include "pt/patcher.h"
+
+void pt_enable_patches(patcher_t *self);
+
+void pt_disable_patches(patcher_t *self);
+#endif
diff --new-file -ur qemu/pt/redqueen_trace.c QEMU-PT/pt/redqueen_trace.c
--- qemu/pt/redqueen_trace.c	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/redqueen_trace.c	2021-08-24 21:54:55.938586449 +0200
@@ -0,0 +1,73 @@
+#include <stdint.h>
+#include <unistd.h>
+#include <stdio.h>
+#include <assert.h>
+#include "redqueen_trace.h"
+
+redqueen_trace_t* redqueen_trace_new(void){
+	redqueen_trace_t* self = malloc(sizeof(redqueen_trace_t));
+	self->lookup = kh_init(RQ_TRACE);
+	self->num_ordered_transitions = 0;
+	self->max_ordered_transitions = INIT_NUM_OF_STORED_TRANSITIONS;
+	self->ordered_transitions = malloc(INIT_NUM_OF_STORED_TRANSITIONS*sizeof(uint128_t));
+	return self;
+}
+
+void redqueen_trace_reset(redqueen_trace_t* self){
+	kh_destroy(RQ_TRACE, self->lookup);
+	self->lookup = kh_init(RQ_TRACE);
+	self->num_ordered_transitions = 0;
+}
+
+void redqueen_trace_free(redqueen_trace_t* self){
+	kh_destroy(RQ_TRACE, self->lookup);
+	free(self->ordered_transitions);
+	free(self);
+}
+
+void redqueen_trace_register_transition(redqueen_trace_t* self, uint64_t from, uint64_t to){
+	khiter_t k;
+	int ret;
+	uint128_t key = (((uint128_t)from)<<64) | ((uint128_t)to);
+	k = kh_get(RQ_TRACE, self->lookup, key); 
+	if(k != kh_end(self->lookup)){
+		kh_value(self->lookup, k) += 1; 
+	} else{
+		k = kh_put(RQ_TRACE, self->lookup, key, &ret); 
+		kh_value(self->lookup, k) = 1;
+		self->ordered_transitions[self->num_ordered_transitions] = key;
+		self->num_ordered_transitions++;
+		assert(self->num_ordered_transitions < self->max_ordered_transitions);
+	}
+}	
+
+void redqueen_trace_write_file(redqueen_trace_t* self, int fd){
+	for(size_t i = 0; i < self->num_ordered_transitions; i++){
+		khiter_t k;
+		uint128_t key = self->ordered_transitions[i];
+		k = kh_get(RQ_TRACE, self->lookup, key); 
+		assert(k != kh_end(self->lookup));
+		dprintf(fd, "%lx,%lx,%lx\n",  (uint64_t)(key>>64), (uint64_t)key, kh_value(self->lookup, k) );
+	}
+}
+
+
+#ifdef DEBUG_MAIN
+int main(int argc, char** argv){
+
+	redqueen_trace_t* rq_obj = redqueen_trace_new();
+
+	for (uint64_t j = 0; j < 0x5; j++){
+		redqueen_trace_register_transition(rq_obj, 0xBADF, 0xC0FFEE);
+		redqueen_trace_register_transition(rq_obj, 0xBADBEEF, 0xC0FFEE);
+		for (uint64_t i = 0; i < 0x10000; i++){
+			redqueen_trace_register_transition(rq_obj, 0xBADBEEF, 0xC0FFEE);
+		}
+		redqueen_trace_write_file(rq_obj, STDOUT_FILENO);
+		redqueen_trace_reset(rq_obj);
+	}
+
+	redqueen_trace_free(rq_obj);
+	return 0;
+}
+#endif
diff --new-file -ur qemu/pt/redqueen_trace.h QEMU-PT/pt/redqueen_trace.h
--- qemu/pt/redqueen_trace.h	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/redqueen_trace.h	2021-08-24 21:54:55.938586449 +0200
@@ -0,0 +1,42 @@
+#pragma once
+#include "khash.h"
+
+typedef unsigned __int128 uint128_t;
+typedef uint128_t khint128_t;
+
+#define INIT_NUM_OF_STORED_TRANSITIONS 0xfffff
+
+/*! @function
+  @abstract     64-bit integer hash function
+  @param  key   The integer [khint64_t]
+  @return       The hash value [khint_t]
+ */
+#define kh_int128_hash_func(key) (khint32_t)((key)>>33^(key)^(key)<<11) ^ (((key>>64))>>33^((key>>64))^((key>>64))<<11)
+/*! @function
+  @abstract     64-bit integer comparison function
+ */
+#define kh_int128_hash_equal(a, b) ((a) == (b))
+/*! @function
+  @abstract     Instantiate a hash map containing 64-bit integer keys
+  @param  name  Name of the hash table [symbol]
+  @param  khval_t  Type of values [type]
+ */
+#define KHASH_MAP_INIT_INT128(name, khval_t)								\
+	KHASH_INIT(name, khint128_t, khval_t, 1, kh_int128_hash_func, kh_int128_hash_equal)
+
+KHASH_MAP_INIT_INT128(RQ_TRACE, uint64_t)
+
+#define INIT_TRACE_IP 0xFFFFFFFFFFFFFFFFULL
+
+typedef struct redqueen_trace_s{
+	khash_t(RQ_TRACE) *lookup;
+	size_t num_ordered_transitions;
+	size_t max_ordered_transitions;
+	uint128_t* ordered_transitions;
+} redqueen_trace_t;
+
+redqueen_trace_t* redqueen_trace_new(void);
+void redqueen_trace_reset(redqueen_trace_t* self);
+void redqueen_trace_free(redqueen_trace_t* self);
+void redqueen_trace_register_transition(redqueen_trace_t* self, uint64_t from, uint64_t to);
+void redqueen_trace_write_file(redqueen_trace_t* self, int fd);
diff --new-file -ur qemu/pt/sharedir.c QEMU-PT/pt/sharedir.c
--- qemu/pt/sharedir.c	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/sharedir.c	2021-08-24 21:54:55.938586449 +0200
@@ -0,0 +1,168 @@
+#include "sharedir.h"
+#include <assert.h>
+#include <stdio.h>
+#include <dirent.h>
+#include <stdbool.h>
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <unistd.h>
+
+//#define SHAREDIR_DEBUG
+
+sharedir_t* sharedir_new(void){
+	sharedir_t* self = malloc(sizeof(sharedir_t));
+  self->dir = NULL;
+  self->lookup = kh_init(SHAREDIR_LOOKUP);
+  self->last_file_f = NULL;
+  self->last_file_obj_ptr = NULL;
+  return self;
+}
+
+void sharedir_set_dir(sharedir_t* self, const char* dir){
+  assert(!self->dir);
+  assert(asprintf(&self->dir, "%s", dir) != -1);
+}
+
+static bool file_exits(const char* file){
+	struct stat sb;   
+	return (stat (file, &sb) == 0);
+}
+
+static time_t get_file_mod_time(char *file){
+    struct stat attr;
+    stat(file, &attr);
+    return attr.st_mtime;
+}
+
+static size_t get_file_size(const char* file){
+  struct stat st;
+  stat(file, &st);
+  return st.st_size;
+}
+
+static char* sharedir_scan(sharedir_t* self, const char* file){
+
+  char* path = NULL;
+  assert(asprintf(&path, "%s/%s", self->dir, file) != -1);
+
+  char* real_path = realpath(path, NULL);
+
+  free(path);
+  if(real_path && !strncmp(self->dir, real_path, strlen(self->dir)) && file_exits(real_path)){
+    return real_path;
+  }
+
+  if(real_path){
+    free(real_path);
+  }
+  return NULL;
+}
+
+static sharedir_file_t* sharedir_get_object(sharedir_t* self, const char* file){
+  khiter_t k;
+  int ret;
+  sharedir_file_t* obj = NULL;
+
+  k = kh_get(SHAREDIR_LOOKUP, self->lookup, file); 
+
+  if(k != kh_end(self->lookup)){
+    /* file already exists in our hash map */
+    obj = kh_value(self->lookup, k);
+
+    /* check if file still exists */
+    assert(file_exits(obj->path));
+
+    /* check if mod time matches */
+    assert(get_file_mod_time(obj->path) == obj->mod_time);
+
+    /* check if file size matches */
+    assert(get_file_size(obj->path) == obj->size);
+
+    return obj;
+  }
+  else{
+    /* nope ! */
+    char* realpath = sharedir_scan(self, file);
+    if(realpath != NULL){
+      obj = malloc(sizeof(sharedir_file_t));
+      memset(obj, 0x0, sizeof(sharedir_file_t));
+      assert(asprintf(&obj->file, "%s", basename(realpath)) != -1);
+      obj->path = realpath;
+      obj->size = get_file_size(obj->path);
+      obj->bytes_left = (uint64_t) obj->size;
+      obj->mod_time = get_file_mod_time(obj->path);
+
+      /* add to to hash_list */
+
+      char* new_file = NULL;
+      assert(asprintf(&new_file, "%s", file) != -1);
+      k = kh_put(SHAREDIR_LOOKUP, self->lookup, new_file, &ret);
+      kh_value(self->lookup, k) = obj;
+
+      return obj;
+    }
+
+    /* file not found */
+    return NULL;
+  }
+}
+
+static FILE* get_file_ptr(sharedir_t* self, sharedir_file_t* obj){
+  if(obj == self->last_file_obj_ptr && self->last_file_f){
+    return self->last_file_f;
+  }
+  else{
+    if(self->last_file_f){
+      fclose(self->last_file_f);
+    }
+    FILE* f = fopen(obj->path, "r");
+    self->last_file_f = f;
+    self->last_file_obj_ptr = obj;
+    return f;
+  }
+}
+
+uint64_t sharedir_request_file(sharedir_t* self, const char* file, uint8_t* page_buffer){
+  if(!self->dir){
+    fprintf(stderr, "WARNING: New file request received, but no sharedir configured!\n");
+    return 0xFFFFFFFFFFFFFFFFUL;
+  }
+
+  FILE* f = NULL;
+
+  sharedir_file_t* obj = sharedir_get_object(self, file);
+  if(obj != NULL){
+#ifdef SHAREDIR_DEBUG
+    printf("sharedir_get_object->file: %s\n", obj->file);
+    printf("sharedir_get_object->path: %s\n", obj->path);
+    printf("sharedir_get_object->size: %ld\n", obj->size);
+    printf("sharedir_get_object->bytes_left: %ld\n", obj->bytes_left);
+#endif
+    if(obj->bytes_left >= 0x1000){
+      f = get_file_ptr(self, obj);
+      fseek(f, obj->size-obj->bytes_left, SEEK_SET);
+      assert(fread(page_buffer, 1, 0x1000, f) == 0x1000);
+      obj->bytes_left -= 0x1000;
+      return 0x1000;
+    }
+    else {
+      if (obj->bytes_left != 0){
+        f = get_file_ptr(self, obj);
+        fseek(f, obj->size-obj->bytes_left, SEEK_SET);
+        assert(fread(page_buffer, 1, obj->bytes_left, f) == obj->bytes_left);
+
+        uint64_t ret_value = obj->bytes_left;
+        obj->bytes_left = 0;
+
+        return ret_value;
+      }
+      else {
+        obj->bytes_left = (uint_fast64_t)obj->size;
+        return 0;
+      }
+    }
+  }
+  else{
+    return 0xFFFFFFFFFFFFFFFFUL;
+  }
+}
\ No newline at end of file
diff --new-file -ur qemu/pt/sharedir.h QEMU-PT/pt/sharedir.h
--- qemu/pt/sharedir.h	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/sharedir.h	2021-08-24 21:54:55.938586449 +0200
@@ -0,0 +1,26 @@
+#pragma once
+#include <stdio.h>
+#include "khash.h"
+#include <stdint.h>
+
+
+typedef struct sharedir_file_s{
+  char* file;
+  char* path;
+  size_t size;
+  uint64_t bytes_left; 
+  time_t mod_time;
+} sharedir_file_t;
+
+KHASH_MAP_INIT_STR(SHAREDIR_LOOKUP, sharedir_file_t*)
+
+typedef struct sharedir_s{
+  char* dir;
+	khash_t(SHAREDIR_LOOKUP) *lookup;
+  FILE* last_file_f;
+  sharedir_file_t* last_file_obj_ptr;
+} sharedir_t;
+
+sharedir_t* sharedir_new(void);
+void sharedir_set_dir(sharedir_t* self, const char* dir);
+uint64_t sharedir_request_file(sharedir_t* self, const char* file, uint8_t* page_buffer);
diff --new-file -ur qemu/pt/state.c QEMU-PT/pt/state.c
--- qemu/pt/state.c	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/state.c	2021-08-24 21:54:55.938586449 +0200
@@ -0,0 +1,306 @@
+/*
+
+Copyright (C) 2019 Sergej Schumilo
+
+This file is part of QEMU-PT (HyperTrash / kAFL).
+
+QEMU-PT is free software: you can redistribute it and/or modify
+it under the terms of the GNU General Public License as published by
+the Free Software Foundation, either version 2 of the License, or
+(at your option) any later version.
+
+QEMU-PT is distributed in the hope that it will be useful,
+but WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with QEMU-PT.  If not, see <http://www.gnu.org/licenses/>.
+
+*/
+
+#include "pt/state.h"
+#include "pt/debug.h"
+#include "pt/memory_access.h"
+#include "sysemu/kvm.h"
+#include "pt/auxiliary_buffer.h"
+#include "pt/sharedir.h"
+
+//#define STATE_VERBOSE
+
+/* global singleton */
+struct state_qemu_pt global_state;
+
+void state_init_global(void){
+#ifdef STATE_VERBOSE
+    fprintf(stderr, "--> %s <--\n", __func__);
+#endif
+    global_state.fast_reload_enabled = false;
+    global_state.fast_reload_mode = false;
+    global_state.fast_reload_path = NULL;
+    global_state.fast_reload_pre_path = NULL;
+    global_state.fast_reload_pre_image = false;
+
+    global_state.fast_reload_snapshot = fast_reload_new();
+
+    global_state.page_cache = NULL;
+
+    global_state.redqueen_enable_pending = false;
+    global_state.redqueen_disable_pending = false;
+    global_state.redqueen_instrumentation_mode = 0;
+    global_state.redqueen_update_blacklist = false;
+    global_state.patches_enable_pending = false;
+    global_state.patches_disable_pending = false;
+    global_state.redqueen_state = NULL;
+    global_state.redqueen_patch_state = NULL;
+
+    for(uint8_t i = 0; i < INTEL_PT_MAX_RANGES; i++){
+        global_state.pt_ip_filter_configured[i] = false;
+		global_state.pt_ip_filter_enabled[i] = false;
+		global_state.pt_ip_filter_a[i] = 0x0;
+		global_state.pt_ip_filter_b[i] = 0x0;
+    }
+    global_state.pt_c3_filter = 0;
+    
+    global_state.enable_hprintf = false;
+    global_state.parent_cr3 = 0;
+    global_state.disassembler_word_width = 64;
+    global_state.nested = false;
+    global_state.payload_buffer = 0;
+    global_state.nested_payload_pages = NULL;
+    global_state.nested_payload_pages_num = 0;
+    global_state.protect_payload_buffer = 1; 
+
+    init_timeout_detector(&(global_state.timeout_detector));
+
+    global_state.in_fuzzing_mode = false;
+    global_state.in_reload_mode = true;
+    global_state.shutdown_requested = false;
+    global_state.cow_cache_full = false;
+
+    global_state.auxilary_buffer = NULL;
+    memset(&global_state.shadow_config, 0x0, sizeof(auxilary_buffer_config_t));
+
+    global_state.decoder_page_fault = false;
+    global_state.decoder_page_fault_addr = 0x0;
+
+    global_state.dump_page = false;
+    global_state.dump_page_addr = 0x0;
+
+    global_state.in_redqueen_reload_mode = false;
+
+    global_state.sharedir = sharedir_new();
+
+
+    QTAILQ_INIT(&global_state.redqueen_breakpoints);
+}
+
+
+fast_reload_t* get_fast_reload_snapshot(void){
+    return global_state.fast_reload_snapshot;
+}
+
+void set_fast_reload_mode(bool mode){
+    global_state.fast_reload_mode = mode;
+}
+
+void set_fast_reload_path(const char* path){
+    assert(global_state.fast_reload_path == NULL);
+    global_state.fast_reload_path = malloc(strlen(path)+1);
+    strcpy(global_state.fast_reload_path, path);
+}
+
+void set_fast_reload_pre_path(const char* path){
+    assert(global_state.fast_reload_pre_path == NULL);
+    global_state.fast_reload_pre_path = malloc(strlen(path)+1);
+    strcpy(global_state.fast_reload_pre_path, path);
+}
+
+void set_fast_reload_pre_image(void){
+    assert(global_state.fast_reload_pre_path != NULL);
+    global_state.fast_reload_pre_image = true;
+}
+
+void enable_fast_reloads(void){
+    assert(global_state.fast_reload_path != NULL);
+    global_state.fast_reload_enabled = true;
+}
+
+void init_page_cache(char* path){
+    assert(global_state.page_cache == NULL);
+    global_state.page_cache = page_cache_new((CPUState *)qemu_get_cpu(0), path);
+#ifdef STATE_VERBOSE
+    fprintf(stderr, "\n\nINIT PAGE_CACHE => %s\n", path);
+#endif
+}
+
+page_cache_t* get_page_cache(void){
+    assert(global_state.page_cache);
+    return global_state.page_cache;
+}
+
+void init_redqueen_state(void){
+    global_state.redqueen_state = new_rq_state((CPUState *)qemu_get_cpu(0), get_page_cache());
+}
+
+void init_redqueen_patch_state(void){
+    global_state.redqueen_patch_state = patcher_new((CPUState *)qemu_get_cpu(0));
+}
+
+redqueen_t* get_redqueen_state(void){
+    assert(global_state.redqueen_state != NULL);
+    return global_state.redqueen_state;
+}
+
+patcher_t* get_redqueen_patch_state(void){
+    assert(global_state.redqueen_patch_state != NULL);
+    return global_state.redqueen_patch_state;
+}
+
+void dump_global_state(const char* filename_prefix){
+
+	char* tmp;
+
+	assert(asprintf(&tmp, "%s/global.state", filename_prefix) != -1);
+	printf("%s\n", tmp);
+
+	FILE *fp = fopen(tmp, "wb");
+	if(fp == NULL) {                                                
+    	fprintf(stderr, "[%s] Could not open file %s.\n", __func__, tmp);
+        assert(false);
+    }
+
+
+    fwrite(&global_state.pt_ip_filter_configured, sizeof(bool)*4, 1, fp);
+
+    fwrite(&global_state.pt_ip_filter_a, sizeof(uint64_t)*4, 1, fp);
+
+    fwrite(&global_state.pt_ip_filter_b, sizeof(uint64_t)*4, 1, fp);
+
+    fwrite(&global_state.enable_hprintf, sizeof(bool), 1, fp);
+    fwrite(&global_state.parent_cr3, sizeof(uint64_t), 1, fp);
+    
+    fwrite(&global_state.disassembler_word_width, sizeof(uint8_t), 1, fp);
+    fwrite(&global_state.fast_reload_pre_image, sizeof(bool), 1, fp);
+
+    fwrite(&global_state.nested, sizeof(bool), 1, fp);
+
+    if(!global_state.nested){
+        fwrite(&global_state.payload_buffer, sizeof(uint64_t), 1, fp);
+    }
+    else{
+        assert(global_state.nested_payload_pages != NULL && global_state.nested_payload_pages_num != 0);
+        fwrite(&global_state.nested_payload_pages_num, sizeof(uint32_t), 1, fp);
+
+        if(global_state.nested_payload_pages_num != 0){
+            fwrite(&global_state.protect_payload_buffer, sizeof(bool), 1, fp);
+        }
+
+        for(uint32_t i = 0; i < global_state.nested_payload_pages_num; i++){
+            fwrite(&global_state.nested_payload_pages[i], sizeof(uint64_t), 1, fp);
+        }
+    }
+
+
+    fclose(fp);
+
+	free(tmp);
+}
+
+void load_global_state(const char* filename_prefix){
+
+	char* tmp;
+
+	assert(asprintf(&tmp, "%s/global.state", filename_prefix) != -1);
+
+	FILE *fp = fopen(tmp, "rb");
+	if(fp == NULL) {                                                
+    	fprintf(stderr, "[%s] Could not open file %s.\n", __func__, tmp);
+        assert(false);
+    }
+    
+
+    assert(fread(&global_state.pt_ip_filter_configured, sizeof(bool)*4, 1, fp) == 1);
+
+    assert(fread(&global_state.pt_ip_filter_a, sizeof(uint64_t)*4, 1, fp) == 1);
+
+    assert(fread(&global_state.pt_ip_filter_b, sizeof(uint64_t)*4, 1, fp) == 1);
+
+    assert(fread(&global_state.enable_hprintf, sizeof(bool), 1, fp) == 1);
+    assert(fread(&global_state.parent_cr3, sizeof(uint64_t), 1, fp) == 1);
+    assert(fread(&global_state.disassembler_word_width, sizeof(uint8_t), 1, fp) == 1);
+    assert(fread(&global_state.fast_reload_pre_image, sizeof(bool), 1, fp) == 1);
+
+    assert(fread(&global_state.nested, sizeof(bool), 1, fp) == 1);
+
+    if(!global_state.nested){
+        assert(fread(&global_state.payload_buffer, sizeof(uint64_t), 1, fp) == 1);
+
+        if(!global_state.fast_reload_pre_image){
+            assert(global_state.payload_buffer != 0);
+            remap_payload_buffer(global_state.payload_buffer, ((CPUState *)qemu_get_cpu(0)) );
+        }
+    }
+    else{
+        assert(fread(&global_state.nested_payload_pages_num, sizeof(uint32_t), 1, fp) == 1);
+
+        global_state.in_fuzzing_mode = true; /* haaaeeeeh ??? */
+        if(!global_state.fast_reload_pre_image){
+
+            assert(fread(&global_state.protect_payload_buffer, sizeof(bool), 1, fp) == 1);
+
+            global_state.nested_payload_pages = (uint64_t*)malloc(sizeof(uint64_t)*global_state.nested_payload_pages_num);
+            
+            for(uint32_t i = 0; i < global_state.nested_payload_pages_num; i++){
+                assert(fread(&global_state.nested_payload_pages[i], sizeof(uint64_t), 1, fp) == 1);
+                if(global_state.protect_payload_buffer){
+                    assert(remap_payload_slot_protected(GET_GLOBAL_STATE()->nested_payload_pages[i], i, ((CPUState *)qemu_get_cpu(0))) == true);
+		        }
+                else{
+                    remap_payload_slot(global_state.nested_payload_pages[i], i, ((CPUState *)qemu_get_cpu(0)));
+                }
+            }
+            
+        }
+    }
+   
+    fclose(fp);
+
+	free(tmp);
+}
+
+static void* alloc_auxiliary_buffer(const char* file){
+	void* ptr;
+	struct stat st;
+	int fd = open(file, O_CREAT|O_RDWR, S_IRWXU|S_IRWXG|S_IRWXO);
+	assert(ftruncate(fd, AUX_BUFFER_SIZE) == 0);
+	stat(file, &st);
+	QEMU_PT_PRINTF(INTERFACE_PREFIX, "new aux buffer file: (max size: %x) %lx", AUX_BUFFER_SIZE, st.st_size);
+	
+	assert(AUX_BUFFER_SIZE == st.st_size);
+	ptr = mmap(0, AUX_BUFFER_SIZE, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0);
+	if (ptr == MAP_FAILED) {
+		fprintf(stderr, "aux buffer allocation failed!\n");
+		return (void*)-1;
+	}
+	return ptr;
+}
+
+void init_aux_buffer(const char* filename){
+    global_state.auxilary_buffer = (auxilary_buffer_t*)alloc_auxiliary_buffer(filename);
+    init_auxiliary_buffer(global_state.auxilary_buffer);
+}
+
+void set_payload_buffer(uint64_t payload_buffer){
+    assert(global_state.payload_buffer == 0 && global_state.nested == false);
+    global_state.payload_buffer = payload_buffer;
+    global_state.nested = false;
+}
+
+void set_payload_pages(uint64_t* payload_pages, uint32_t pages){
+    assert(global_state.nested_payload_pages == NULL && global_state.nested_payload_pages_num == 0);
+    global_state.nested_payload_pages = (uint64_t*)malloc(sizeof(uint64_t)*pages);
+    global_state.nested_payload_pages_num = pages;
+    memcpy(global_state.nested_payload_pages, payload_pages, sizeof(uint64_t)*pages);
+    global_state.nested = true;
+}
diff --new-file -ur qemu/pt/state.h QEMU-PT/pt/state.h
--- qemu/pt/state.h	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/state.h	2021-08-24 21:54:55.942586446 +0200
@@ -0,0 +1,130 @@
+/*
+
+Copyright (C) 2019 Sergej Schumilo
+
+This file is part of QEMU-PT (HyperTrash / kAFL).
+
+QEMU-PT is free software: you can redistribute it and/or modify
+it under the terms of the GNU General Public License as published by
+the Free Software Foundation, either version 2 of the License, or
+(at your option) any later version.
+
+QEMU-PT is distributed in the hope that it will be useful,
+but WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with QEMU-PT.  If not, see <http://www.gnu.org/licenses/>.
+
+*/
+
+#pragma once
+
+#include "pt/redqueen.h"
+#include "pt/redqueen_patch.h"
+#include "pt/fast_vm_reload.h"
+#include "pt/page_cache.h"
+#include "pt/synchronization.h"
+#include "pt/auxiliary_buffer.h"
+#include "pt/sharedir.h"
+
+#define INTEL_PT_MAX_RANGES	4
+
+struct state_qemu_pt{
+
+    /* FAST VM RELOAD */
+    bool fast_reload_enabled;
+    bool fast_reload_mode;
+    char* fast_reload_path;
+    char* fast_reload_pre_path;
+    bool fast_reload_pre_image;
+    fast_reload_t* fast_reload_snapshot;
+
+    /* PAGE CACHE */
+    page_cache_t* page_cache; 
+
+    /* REDQUEEN */
+    bool redqueen_enable_pending;
+    bool redqueen_disable_pending;
+    int redqueen_instrumentation_mode;
+    bool redqueen_update_blacklist;
+    bool patches_enable_pending;
+    bool patches_disable_pending;
+    redqueen_t* redqueen_state;
+    patcher_t* redqueen_patch_state;
+
+    /* Intel PT Options (not migratable) */
+    uint64_t pt_c3_filter;
+    volatile bool pt_ip_filter_enabled[4];
+
+    /* Intel PT Options (migratable) */
+    bool pt_ip_filter_configured[4];
+    uint64_t pt_ip_filter_a[4];
+    uint64_t pt_ip_filter_b[4];
+
+    /* OPTIONS (MIGRATABLE VIA FAST SNAPSHOTS) */
+    bool enable_hprintf; 
+    uint64_t parent_cr3;
+    uint8_t disassembler_word_width;
+    bool nested; 
+    uint64_t payload_buffer;
+    uint32_t nested_payload_pages_num;
+    uint64_t* nested_payload_pages; 
+    bool protect_payload_buffer;
+
+    /* NON MIGRATABLE OPTION */
+    timeout_detector_t timeout_detector;
+
+    bool decoder_page_fault; 
+    uint64_t decoder_page_fault_addr;
+
+    bool dump_page; 
+    uint64_t dump_page_addr;
+
+    bool in_fuzzing_mode;
+    bool in_reload_mode; 
+
+    bool shutdown_requested;
+    bool cow_cache_full;
+
+    bool in_redqueen_reload_mode;
+
+    auxilary_buffer_t* auxilary_buffer;
+    auxilary_buffer_config_t shadow_config;
+    sharedir_t* sharedir;
+
+    QTAILQ_HEAD(, kvm_sw_breakpoint) redqueen_breakpoints;
+};
+
+extern struct state_qemu_pt global_state;
+
+#define GET_GLOBAL_STATE() (&global_state)
+
+void state_init_global(void);
+fast_reload_t* get_fast_reload_snapshot(void);
+void set_fast_reload_mode(bool mode);
+void set_fast_reload_path(const char* path);
+void set_fast_reload_pre_image(void);
+
+
+void enable_fast_reloads(void);
+
+/* Page Cache */
+void init_page_cache(char* path);
+page_cache_t* get_page_cache(void);
+
+void init_redqueen_state(void);
+void init_redqueen_patch_state(void);
+
+redqueen_t* get_redqueen_state(void);
+patcher_t* get_redqueen_patch_state(void);
+
+void dump_global_state(const char* filename_prefix);
+void load_global_state(const char* filename_prefix);
+
+void init_aux_buffer(const char* filename);
+void set_fast_reload_pre_path(const char* path);
+
+void set_payload_buffer(uint64_t payload_buffer);
+void set_payload_pages(uint64_t* payload_pages, uint32_t pages);
\ No newline at end of file
diff --new-file -ur qemu/pt/state_reallocation.c QEMU-PT/pt/state_reallocation.c
--- qemu/pt/state_reallocation.c	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/state_reallocation.c	2021-08-24 21:54:55.938586449 +0200
@@ -0,0 +1,618 @@
+/*
+
+Copyright (C) 2017 Sergej Schumilo
+
+This file is part of QEMU-PT (HyperTrash / kAFL).
+
+QEMU-PT is free software: you can redistribute it and/or modify
+it under the terms of the GNU General Public License as published by
+the Free Software Foundation, either version 2 of the License, or
+(at your option) any later version.
+
+QEMU-PT is distributed in the hope that it will be useful,
+but WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with QEMU-PT.  If not, see <http://www.gnu.org/licenses/>.
+
+*/
+
+#include "pt/state_reallocation.h"
+#include "sysemu/kvm_int.h"
+#include "migration/vmstate.h"
+#include "migration/register.h"
+#include "migration/savevm.h"
+#include "migration/qemu-file.h"
+
+#define QEMU_VM_SUBSECTION           0x05
+
+typedef struct CompatEntry {
+    char idstr[256];
+    int instance_id;
+} CompatEntry;
+
+typedef struct SaveStateEntry {
+    QTAILQ_ENTRY(SaveStateEntry) entry;
+    char idstr[256];
+    int instance_id;
+    int alias_id;
+    int version_id;
+    int load_version_id;
+    int section_id;
+    int load_section_id;
+    SaveVMHandlers *ops;
+    const VMStateDescription *vmsd;
+    CompatEntry *opaque;
+    CompatEntry *compat;
+    int is_ram;
+} SaveStateEntry;
+
+struct LoadStateEntry {
+    QLIST_ENTRY(LoadStateEntry) entry;
+    SaveStateEntry *se;
+    int section_id;
+    int version_id;
+};
+
+typedef struct SaveState {
+    QTAILQ_HEAD(, SaveStateEntry) handlers;
+    int global_section_id;
+    bool skip_configuration;
+    uint32_t len;
+    const char *name;
+    uint32_t target_page_bits;
+} SaveState;
+
+extern void* vmstate_configuration;
+extern SaveState savevm_state;
+
+extern int vmstate_n_elems(void *opaque, VMStateField *field);
+extern int vmstate_size(void *opaque, VMStateField *field);
+extern void vmstate_handle_alloc(void *ptr, VMStateField *field, void *opaque);
+extern int vmstate_load(QEMUFile *f, SaveStateEntry *se);
+
+static void fast_timer_get(void* data, size_t size, void* opaque)
+{
+    QEMUTimer *ts = (QEMUTimer*) opaque;
+    uint64_t expire_time = *((uint64_t*)data); 
+
+    if (expire_time != -1) {
+        timer_mod_ns(ts, expire_time);
+    } else {
+        timer_del(ts);
+    }
+}
+
+
+
+
+static SaveStateEntry *fdl_find_se(const char *idstr, int instance_id)
+{
+    SaveStateEntry *se;
+
+    QTAILQ_FOREACH(se, &savevm_state.handlers, entry) {
+        if (!strcmp(se->idstr, idstr) &&
+            (instance_id == se->instance_id ||
+             instance_id == se->alias_id)){
+            return se;
+        }
+        /* Migrating from an older version? */
+        if (strstr(se->idstr, idstr) && se->compat) {
+            if (!strcmp(se->compat->idstr, idstr) &&
+                (instance_id == se->compat->instance_id ||
+                 instance_id == se->alias_id)){
+                return se;
+            }
+        }
+    }
+    return NULL;
+}
+
+static int fdl_vmstate_load_state(state_reallocation_t* self, QEMUFile *f, const VMStateDescription *vmsd, void *opaque, int version_id, uintptr_t* opaque_ptr);
+
+
+static inline VMStateDescription* fdl_vmstate_get_subsection(VMStateDescription **sub, char *idstr)
+{
+    while (sub && *sub && (*sub)->needed) {
+        if (strcmp(idstr, (*sub)->name) == 0) {
+            return *sub;
+        }
+        sub++;
+    }
+    return NULL;
+}
+
+static int fdl_vmstate_subsection_load(state_reallocation_t* self, QEMUFile *f, const VMStateDescription *vmsd, void *opaque)
+{
+    while (qemu_peek_byte(f, 0) == QEMU_VM_SUBSECTION) {
+        char idstr[256], *idstr_ret;
+        int ret;
+        uint8_t version_id, len, size;
+        const VMStateDescription *sub_vmsd;
+
+        len = qemu_peek_byte(f, 1);
+        if (len < strlen(vmsd->name) + 1) {
+            return 0;
+        }
+        size = qemu_peek_buffer(f, (uint8_t **)&idstr_ret, len, 2);
+        if (size != len) {
+            return 0;
+        }
+        memcpy(idstr, idstr_ret, size);
+        idstr[size] = 0;
+
+        if (strncmp(vmsd->name, idstr, strlen(vmsd->name)) != 0) {
+            /* it doesn't have a valid subsection name */
+            return 0;
+        }
+        sub_vmsd = fdl_vmstate_get_subsection((VMStateDescription **)vmsd->subsections, idstr);
+        if (sub_vmsd == NULL) {
+            return -ENOENT;
+        }
+        qemu_file_skip(f, 1); /* subsection */
+        qemu_file_skip(f, 1); /* len */
+        qemu_file_skip(f, len); /* idstr */
+        version_id = qemu_get_be32(f);
+
+        ret = fdl_vmstate_load_state(self, f, sub_vmsd, opaque, version_id, NULL);
+        if (ret) {
+            return ret;
+        }
+    }
+    return 0;
+}
+
+uint32_t post_counter = 0;
+void* post_fptr_array[256];
+uint32_t post_version_id_array[256];
+void* post_opaque_array[256];
+
+static void add_post_fptr(state_reallocation_t* self, void* fptr, uint32_t version_id, void* opaque, const char* name){
+    
+    if(!self){
+        return;
+    }
+    
+    if(!strcmp("I440FX", name)){
+        /* unsupported */
+        return;
+    }
+    
+
+    self->fptr[self->fast_state_fptr_pos] = fptr;
+    self->opaque[self->fast_state_fptr_pos] = opaque;
+    self->version[self->fast_state_fptr_pos] = version_id;
+    self->fast_state_fptr_pos++;
+
+    if(self->fast_state_fptr_pos >= self->fast_state_fptr_size){
+        printf("RESIZE %s\n", __func__);
+        self->fast_state_fptr_size += REALLOC_SIZE;
+        self->fptr = realloc(self->fptr, self->fast_state_fptr_size * sizeof(void*));
+        self->opaque = realloc(self->opaque, self->fast_state_fptr_size * sizeof(void*));
+        self->version = realloc(self->version, self->fast_state_fptr_size * sizeof(uint32_t));
+    }
+}
+
+extern void fast_get_pci_config_device(void* data, size_t size, void* opaque);
+void fast_get_pci_irq_state(void* data, size_t size, void* opaque);
+
+static void add_get(state_reallocation_t* self, void* fptr, void* opaque, size_t size, void* field, QEMUFile* f, const char* name){
+    if(!self){
+        return;
+    }
+
+    void (*handler)(void* , size_t, void*) = NULL;
+    void* data = NULL; 
+
+    if(!strcmp(name, "timer")){
+        qemu_file_skip(f, size * -1);
+        handler = fast_timer_get;
+        data = malloc(sizeof(uint64_t));
+        *((uint64_t*)data) = qemu_get_be64(f);
+    }
+    
+    else if(!strcmp(name, "pci irq state")){
+        qemu_file_skip(f, size * -1);
+        handler = fast_get_pci_irq_state;
+        data = malloc(sizeof(uint8_t)*size);
+
+        ((uint32_t*)data)[0] = qemu_get_be32(f);
+        ((uint32_t*)data)[1] = qemu_get_be32(f);
+        ((uint32_t*)data)[2] = qemu_get_be32(f);
+        ((uint32_t*)data)[3] = qemu_get_be32(f);
+    }
+    else if(!strcmp(name, "pci config")){
+        qemu_file_skip(f, size * -1);
+        handler = fast_get_pci_config_device;
+        data = malloc(sizeof(uint8_t)*size);
+        qemu_get_buffer(f, (uint8_t*)data, size);
+    }
+    
+    else{
+        return;
+    }
+
+    self->get_fptr[self->fast_state_get_fptr_pos] = handler;
+    self->get_opaque[self->fast_state_get_fptr_pos] = opaque;
+    self->get_size[self->fast_state_get_fptr_pos] = size;
+    self->get_data[self->fast_state_get_fptr_pos] = data;
+
+    self->fast_state_get_fptr_pos++;
+
+    if(self->fast_state_get_fptr_pos >= self->fast_state_get_fptr_size){
+        self->fast_state_get_fptr_size += REALLOC_SIZE;
+        self->get_fptr = realloc(self->get_fptr, self->fast_state_get_fptr_size * sizeof(void*));
+        self->get_opaque = realloc(self->get_opaque, self->fast_state_get_fptr_size * sizeof(void*));
+        self->get_size = realloc(self->get_size, self->fast_state_get_fptr_size * sizeof(size_t));
+        self->get_data = realloc(self->get_data, self->fast_state_get_fptr_size * sizeof(void*));
+    }
+
+}
+
+static void add_mblock(state_reallocation_t* self, char* foo, const char* bar, size_t offset, uint64_t start, uint64_t size){
+
+    if(!self){
+        return;
+    }
+
+    if(self->fast_state_pos && (uint64_t)(self->ptr[self->fast_state_pos-1]+self->size[self->fast_state_pos-1]) == start){
+        void* new  = (void*)(self->pre_alloc_block+self->pre_alloc_block_offset);
+        self->pre_alloc_block_offset += size;
+        memcpy(new, (void*)start, size);
+
+        self->size[self->fast_state_pos-1] = size + self->size[self->fast_state_pos-1];
+    }
+    else{
+        self->ptr[self->fast_state_pos] = (void*)start;
+        self->copy[self->fast_state_pos] = (void*)(self->pre_alloc_block+self->pre_alloc_block_offset);
+        self->pre_alloc_block_offset += size;
+
+        memcpy(self->copy[self->fast_state_pos], (void*)start, size);
+        self->size[self->fast_state_pos] = size;
+        self->fast_state_pos++;
+        if(self->fast_state_pos >= self->fast_state_size){
+            self->fast_state_size += REALLOC_SIZE;
+            self->ptr = realloc(self->ptr, self->fast_state_size * sizeof(void*));
+            self->copy = realloc(self->copy, self->fast_state_size * sizeof(void*));
+            self->size = realloc(self->size, self->fast_state_size * sizeof(size_t));
+        }
+    }
+}
+
+static inline int get_handler(state_reallocation_t* self, QEMUFile* f, void* curr_elem, size_t size, VMStateField *field, char* vmsd_name){
+
+    int ret;
+
+    ret = field->info->get(f, curr_elem, size, field);
+
+
+    if (!strcmp(field->info->name, "bool")){
+        assert(size == 1);
+        add_mblock(self, vmsd_name, field->name, field->offset, (uint64_t)curr_elem, 1);
+    }
+    else if(!strcmp(field->info->name, "int8")){
+        assert(size == 1);
+        add_mblock(self, vmsd_name, field->name, field->offset, (uint64_t)curr_elem, 1);
+    }
+    else if(!strcmp(field->info->name, "int16")){
+        assert(size == 2);
+        add_mblock(self, vmsd_name, field->name, field->offset, (uint64_t)curr_elem, 2);
+    }
+    else if(!strcmp(field->info->name, "int32")){
+        assert(size == 4);
+        add_mblock(self, vmsd_name, field->name, field->offset, (uint64_t)curr_elem, 4);
+    }
+    else if(!strcmp(field->info->name, "int32 equal")){
+        assert(size == 4);
+        add_mblock(self, vmsd_name, field->name, field->offset, (uint64_t)curr_elem, 4);
+    }
+    else if(!strcmp(field->info->name, "int32 le")){
+        assert(size == 4);
+        add_mblock(self, vmsd_name, field->name, field->offset, (uint64_t)curr_elem, 4);
+    }
+    else if(!strcmp(field->info->name, "int64")){
+        assert(size == 8);
+        add_mblock(self, vmsd_name, field->name, field->offset, (uint64_t)curr_elem, 8);
+    }
+    else if(!strcmp(field->info->name, "uint8")){
+        assert(size == 1);
+        add_mblock(self, vmsd_name, field->name, field->offset, (uint64_t)curr_elem, 1);
+    }
+    else if(!strcmp(field->info->name, "uint16")){
+        assert(size == 2);
+        add_mblock(self, vmsd_name, field->name, field->offset, (uint64_t)curr_elem, 2);
+    }
+    else if(!strcmp(field->info->name, "uint32")){
+        assert(size == 4);
+        add_mblock(self, vmsd_name, field->name, field->offset, (uint64_t)curr_elem, 4);
+    }
+    else if(!strcmp(field->info->name, "uint32 equal")){
+        assert(size == 4);
+        add_mblock(self, vmsd_name, field->name, field->offset, (uint64_t)curr_elem, 4);
+    }
+    else if(!strcmp(field->info->name, "uint64")){
+        assert(size == 8);
+        add_mblock(self, vmsd_name, field->name, field->offset, (uint64_t)curr_elem, 8);
+    }
+    else if(!strcmp(field->info->name, "int64 equal")){
+        assert(size == 8);
+        add_mblock(self, vmsd_name, field->name, field->offset, (uint64_t)curr_elem, 8);
+    }
+    else if(!strcmp(field->info->name, "uint8 equal")){
+        assert(size == 1);
+        add_mblock(self, vmsd_name, field->name, field->offset, (uint64_t)curr_elem, 1);
+    }
+    else if(!strcmp(field->info->name, "uint16 equal")){
+        assert(size == 16);
+        add_mblock(self, vmsd_name, field->name, field->offset, (uint64_t)curr_elem, 2);
+    }
+    else if(!strcmp(field->info->name, "float64")){
+        assert(size == 64);
+        add_mblock(self, vmsd_name, field->name, field->offset, (uint64_t)curr_elem, 8);
+    }
+    else if(!strcmp(field->info->name, "CPU_Double_U")){
+        assert(0);
+        add_mblock(self, vmsd_name, field->name, field->offset, (uint64_t)curr_elem, 8);
+    }
+    else if(!strcmp(field->info->name, "buffer")){
+        add_mblock(self, vmsd_name, field->name, field->offset, (uint64_t)curr_elem, size);
+    }
+    else if(!strcmp(field->info->name, "unused_buffer")){
+        /* save nothing */
+    }
+    else if(!strcmp(field->info->name, "tmp")){
+        add_mblock(self, vmsd_name, field->name, field->offset, (uint64_t)curr_elem, size);
+        /* save nothing */
+    }
+    else if(!strcmp(field->info->name, "bitmap")){
+        assert(0);
+    }
+    else if(!strcmp(field->info->name, "qtailq")){
+        assert(0);
+    }
+    else if(!strcmp(field->info->name, "timer")){
+        add_get(self, (void*) field->info->get, curr_elem, size, (void*) field, f, field->info->name);
+    }
+    else if(!strcmp(field->info->name, "fpreg")){
+        assert(0);
+        //add_get(self, (void*) field->info->get, curr_elem, size, (void*) field, f, field->info->name);
+    }
+    else if(!strcmp(field->info->name, "pci config")){
+        add_get(self, (void*) field->info->get, curr_elem, size, (void*) field, f, field->info->name);
+    }
+    else if(!strcmp(field->info->name, "pci irq state")){
+        add_get(self, (void*) field->info->get, curr_elem, size, (void*) field, f, field->info->name);
+    }
+    else{
+        printf("FAIL field->info->name: %s\n", field->info->name);
+        assert(0);
+    }
+    return ret; 
+}
+
+static int fdl_vmstate_load_state(state_reallocation_t* self, QEMUFile *f, const VMStateDescription *vmsd, void *opaque, int version_id, uintptr_t* opaque_ptr) {
+    VMStateField *field = (VMStateField *)vmsd->fields;
+    int ret = 0;
+
+    uint64_t total_size = 0;
+
+    if (version_id > vmsd->version_id) {
+        return -EINVAL;
+    }
+    if  (version_id < vmsd->minimum_version_id) {
+
+        if (vmsd->load_state_old &&
+            version_id >= vmsd->minimum_version_id_old) {
+            assert(0);
+            ret = vmsd->load_state_old(f, opaque, version_id);
+            return ret;
+        }
+        return -EINVAL;
+    }
+    if (vmsd->pre_load) {
+        add_post_fptr(self, vmsd->pre_load, 1337, opaque, vmsd->name);    
+    }
+    while (field->name) {
+        if ((field->field_exists &&
+             field->field_exists(opaque, version_id)) ||
+            (!field->field_exists &&
+             field->version_id <= version_id)) {
+            void *first_elem = opaque + field->offset;
+            int i, n_elems = vmstate_n_elems(opaque, field);
+            int size = vmstate_size(opaque, field);
+
+            vmstate_handle_alloc(first_elem, field, opaque);
+            if (field->flags & VMS_POINTER) {
+                first_elem = *(void **)first_elem;
+                assert(first_elem || !n_elems || !size);
+            }
+            for (i = 0; i < n_elems; i++) {
+                uint64_t* tmp_opaque_ptr = 0;
+                total_size += size;
+                void *curr_elem = first_elem + size * i;
+
+                if (field->flags & VMS_ARRAY_OF_POINTER) {
+
+                    tmp_opaque_ptr = curr_elem;
+                    curr_elem = *(void **)curr_elem;
+                    add_mblock(self, (char*)vmsd->name, (const char*)field->name, field->offset, (uint64_t)(curr_elem), (uint64_t)(size));
+                }
+
+                if (!curr_elem && size) {
+                    assert(field->flags & VMS_ARRAY_OF_POINTER);
+                    ret = vmstate_info_nullptr.get(f, curr_elem, size, NULL);
+                    add_mblock(self, (char*)vmsd->name, (const char*)field->name, field->offset, (uint64_t)(curr_elem), (uint64_t)(size));
+                } else if (field->flags & VMS_STRUCT) {
+                    ret = fdl_vmstate_load_state(self, f, field->vmsd, curr_elem, field->vmsd->version_id, tmp_opaque_ptr);
+                } else {
+                    ret = get_handler(self, f, curr_elem, size, field, (char*)vmsd->name);
+                }
+                if (ret >= 0) {
+                    ret = qemu_file_get_error(f);
+                }
+                if (ret < 0) {
+                    return ret;
+                }
+            }
+        } else if (field->flags & VMS_MUST_EXIST) {
+            printf("Input validation failed: %s/%s", vmsd->name, field->name);
+            return -1;
+        }
+        field++;
+    }
+
+    ret = fdl_vmstate_subsection_load(self, f, vmsd, opaque);
+    if (ret != 0) {
+        return ret;
+    }
+
+    if (vmsd->post_load) {
+        add_post_fptr(self, vmsd->post_load, version_id, opaque, vmsd->name);
+        ret = vmsd->post_load(opaque, version_id);
+    }
+    return ret;
+}
+
+
+static int fdl_vmstate_load(state_reallocation_t* self, QEMUFile *f, SaveStateEntry *se, int version_id)
+{
+    if (!se->vmsd) {         /* Old style */
+        return se->ops->load_state(f, se->opaque, version_id);
+    }
+    uintptr_t* t = (uintptr_t*)&(se->opaque);
+    return fdl_vmstate_load_state(self, f, se->vmsd, se->opaque, version_id, (uintptr_t*)t);
+}
+
+static int fdl_enumerate_section(state_reallocation_t* self, QEMUFile *f, MigrationIncomingState *mis){
+    uint32_t instance_id, version_id, section_id;
+    SaveStateEntry *se;
+
+    char idstr[256];
+    int ret;
+
+    /* Read section start */
+    section_id = qemu_get_be32(f);
+    if (!qemu_get_counted_string(f, idstr)) {
+        printf("Unable to read ID string for section %u", section_id);
+        return -EINVAL;
+    }
+    instance_id = qemu_get_be32(f);
+    version_id = qemu_get_be32(f);
+
+    /* Find savevm section */
+    se = fdl_find_se(idstr, instance_id);
+    if (se == NULL) {
+        printf("Unknown savevm section or instance '%s' %d", idstr, instance_id);
+        return -EINVAL;
+    }
+
+    /* Validate version */
+    if (version_id > se->version_id) {
+        printf("savevm: unsupported version %d for '%s' v%d", version_id, idstr, se->version_id);
+        return -EINVAL;
+    }
+
+    se->load_version_id = version_id;
+    se->load_section_id = section_id;
+    
+    if(se->vmsd &&  ((strcmp("tiMer", (const char*)(VMStateDescription *)(se->vmsd)->name)) )){
+        ret = fdl_vmstate_load(self, f, se, version_id);
+    }
+    else{
+        ret = vmstate_load(f, se);
+    }
+    
+    if (ret < 0) {
+        printf("error while loading state for instance 0x%x of device '%s'", instance_id, idstr);
+        return ret;
+    }
+
+    qemu_get_byte(f);
+    qemu_get_be32(f);
+
+    return 0;
+}
+
+static void fdl_enumerate_global_states(state_reallocation_t* self, QEMUFile *f){
+    ((struct QEMUFile_tmp*)f)->pos = 0;
+    ((struct QEMUFile_tmp*)f)->buf_index = 0;
+    ((struct QEMUFile_tmp*)f)->buf_size = 0;
+
+    uint8_t section_type;
+
+    MigrationIncomingState *mis = migration_incoming_get_current();
+
+    qemu_get_be32(f);
+    qemu_get_be32(f);
+    qemu_get_byte(f);
+
+    /* migration state */
+    vmstate_load_state(f, (VMStateDescription*) &vmstate_configuration, (void*)&savevm_state, 0);
+
+    while ((section_type = qemu_get_byte(f)) != QEMU_VM_EOF) {
+        switch (section_type) {
+            case QEMU_VM_SECTION_START:
+            case QEMU_VM_SECTION_FULL:
+                fdl_enumerate_section(self, f, mis);
+                break;
+            default:
+                break;
+        } 
+    }
+}
+
+state_reallocation_t* state_reallocation_new(QEMUFile *f){
+    state_reallocation_t* self = malloc(sizeof(state_reallocation_t));
+    self->fast_state_pos = 0;
+    self->fast_state_size = REALLOC_SIZE;
+    self->ptr = malloc(sizeof(void*) * REALLOC_SIZE);
+    self->copy = malloc(sizeof(void*) * REALLOC_SIZE);
+    self->size = malloc(sizeof(size_t) * REALLOC_SIZE);
+
+    self->fast_state_fptr_pos = 0;
+    self->fast_state_fptr_size = REALLOC_SIZE;
+
+    self->fptr = malloc(sizeof(void*) * REALLOC_SIZE);
+    self->opaque = malloc(sizeof(void*) * REALLOC_SIZE);
+    self->version = malloc(sizeof(uint32_t) * REALLOC_SIZE);
+
+    self->fast_state_get_fptr_pos = 0;
+    self->fast_state_get_fptr_size = REALLOC_SIZE;
+
+    self->get_fptr = malloc(sizeof(void*) * REALLOC_SIZE);
+    self->get_opaque = malloc(sizeof(void*) * REALLOC_SIZE);
+    self->get_size = malloc(sizeof(size_t) * REALLOC_SIZE);
+    self->get_data = malloc(sizeof(void*) * REALLOC_SIZE);
+
+    self->pre_alloc_block = (uint32_t*)mmap(NULL, PRE_ALLOC_BLOCK_SIZE, PROT_READ | PROT_WRITE, MAP_ANONYMOUS | MAP_PRIVATE, 0, 0);
+    assert(self->pre_alloc_block != (void *) -1);
+    self->pre_alloc_block_offset = 0;
+
+    fdl_enumerate_global_states(self, f);
+    return self;
+}
+
+void fdl_fast_reload(state_reallocation_t* self){
+    
+    for(uint32_t i = 0; i < self->fast_state_fptr_pos; i++){
+        if((self->version[i]) == 1337){
+            ((int (*)(void *opaque))self->fptr[i])(self->opaque[i]);
+        }
+    }
+    
+
+    for(uint32_t i = 0; i < self->fast_state_pos; i++){
+        memcpy(self->ptr[i], self->copy[i], self->size[i]);
+    }
+
+    for(uint32_t i = 0; i < self->fast_state_fptr_pos; i++){
+        if((self->version[i]) == 1337){
+        }
+        else{
+            ((int (*)(void *opaque, int version_id))self->fptr[i])(self->opaque[i], self->version[i]);
+        }
+    }
+}
+
diff --new-file -ur qemu/pt/state_reallocation.h QEMU-PT/pt/state_reallocation.h
--- qemu/pt/state_reallocation.h	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/state_reallocation.h	2021-08-24 21:54:55.942586446 +0200
@@ -0,0 +1,96 @@
+/*
+
+Copyright (C) 2017 Sergej Schumilo
+
+This file is part of QEMU-PT (HyperTrash / kAFL).
+
+QEMU-PT is free software: you can redistribute it and/or modify
+it under the terms of the GNU General Public License as published by
+the Free Software Foundation, either version 2 of the License, or
+(at your option) any later version.
+
+QEMU-PT is distributed in the hope that it will be useful,
+but WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with QEMU-PT.  If not, see <http://www.gnu.org/licenses/>.
+
+*/
+
+#ifndef STATE_REALLOCATION
+#define STATE_REALLOCATION
+
+#include "qemu/osdep.h"
+#include "monitor/monitor.h"
+#include "qemu-common.h"
+#include "migration/migration.h"
+#include "pt/khash.h"
+
+
+#define IO_BUF_SIZE 32768
+
+struct QEMUFile_tmp {
+    void *ops;
+    void *hooks;
+    void *opaque;
+
+    int64_t bytes_xfer;
+    int64_t xfer_limit;
+
+    int64_t pos; /* start of buffer when writing, end of buffer
+                    when reading */
+    volatile int buf_index;
+    int buf_size; /* 0 when writing */
+    uint8_t buf[IO_BUF_SIZE];
+};
+
+struct fast_savevm_opaque_t{
+    FILE* f;
+    uint8_t* buf;
+    uint64_t pos;
+};
+
+#define REALLOC_SIZE 0x8000
+
+#define PRE_ALLOC_BLOCK_SIZE 0x8000000 /* 128 MB */
+
+typedef struct state_reallocation_s{
+    void **ptr;
+    void **copy;
+    size_t *size;
+
+    uint32_t fast_state_size; 
+    uint32_t fast_state_pos;
+
+
+    void **fptr;
+    void **opaque;
+    uint32_t *version;
+    
+    uint32_t fast_state_fptr_size; 
+    uint32_t fast_state_fptr_pos;
+
+
+    void **get_fptr;
+    void **get_opaque;
+    size_t *get_size;
+    void **get_data;
+
+    //QEMUFile** file; 
+    
+    uint32_t fast_state_get_fptr_size; 
+    uint32_t fast_state_get_fptr_pos;
+
+    /* prevents heap fragmentation and additional 2GB mem usage */
+    void* pre_alloc_block;
+    uint32_t pre_alloc_block_offset;
+
+} state_reallocation_t;
+
+state_reallocation_t* state_reallocation_new(QEMUFile *f);
+
+void fdl_fast_reload(state_reallocation_t* self);
+
+#endif
\ No newline at end of file
diff --new-file -ur qemu/pt/synchronization.c QEMU-PT/pt/synchronization.c
--- qemu/pt/synchronization.c	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/synchronization.c	2021-08-24 21:54:55.942586446 +0200
@@ -0,0 +1,392 @@
+#include "pt/synchronization.h"
+#include "pt/hypercall.h"
+#include "pt/interface.h"
+#include "pt/fast_vm_reload.h"
+#include "qemu-common.h"
+#include "qemu/osdep.h"
+#include "cpu.h"
+#include "sysemu/sysemu.h"
+#include "sysemu/kvm.h"
+#include "pt/debug.h"
+#include "pt/state.h"
+#include <sys/syscall.h>
+#include <linux/kvm.h>
+#include "qemu/main-loop.h"
+#include "pt/helpers.h"
+#include "pt/file_helper.h"
+
+
+#include "pt.h"
+
+pthread_mutex_t synchronization_lock_mutex = PTHREAD_MUTEX_INITIALIZER;
+pthread_cond_t synchronization_lock_condition = PTHREAD_COND_INITIALIZER;
+pthread_mutex_t synchronization_disable_pt_mutex = PTHREAD_MUTEX_INITIALIZER;
+
+volatile bool synchronization_reload_pending = false;
+volatile bool synchronization_kvm_loop_waiting = false;
+
+
+/* new SIGALRM based timeout detection */
+
+//#define DEBUG_TIMEOUT_DETECTOR
+
+void init_timeout_detector(timeout_detector_t* timeout_detector){
+	timeout_detector->kvm_tid = 0;
+	timeout_detector->reload_pending = false;
+	timeout_detector->detection_enabled = false;
+
+	timeout_detector->timeout_sec = 0;
+	timeout_detector->timeout_usec = 0; /* default: disabled */
+
+	timeout_detector->arm_timeout.it_interval.tv_sec = 0;
+	timeout_detector->arm_timeout.it_interval.tv_usec = 0;
+	timeout_detector->arm_timeout.it_value.tv_sec = 0;
+	timeout_detector->arm_timeout.it_value.tv_usec = 0;
+
+	timeout_detector->disarm_timeout.it_interval.tv_sec = 0;
+	timeout_detector->disarm_timeout.it_interval.tv_usec = 0;
+	timeout_detector->arm_timeout.it_value.tv_sec = timeout_detector->timeout_sec;
+	timeout_detector->arm_timeout.it_value.tv_usec = timeout_detector->timeout_usec;
+
+}
+
+static void sigalarm_handler(int signum) {
+		/* ensure that SIGALARM is ALWAYS handled by kvm thread */
+    assert(GET_GLOBAL_STATE()->timeout_detector.kvm_tid == syscall(SYS_gettid));
+#ifdef DEBUG_TIMEOUT_DETECTOR
+#endif
+    //fprintf(stderr, "Handled! %d %ld\n", signum, syscall(SYS_gettid));
+}
+
+void install_timeout_detector(timeout_detector_t* timeout_detector){
+		timeout_detector->kvm_tid = syscall(SYS_gettid);
+    if(signal(SIGALRM, sigalarm_handler) == SIG_ERR) {
+      fprintf(stderr, "%s failed!\n", __func__);
+			assert(false);
+    }
+    //fprintf(stderr, "SIGALRM HANDLER INSTALLED! %ld\n", syscall(SYS_gettid));
+}
+
+void reset_timeout_detector(timeout_detector_t* timeout_detector){
+#ifdef DEBUG_TIMEOUT_DETECTOR
+    fprintf(stderr, "%s!\n", __func__);
+#endif
+	timeout_detector->reload_pending = false;
+	if(timeout_detector->timeout_sec || timeout_detector->timeout_usec){
+		timeout_detector->arm_timeout.it_value.tv_sec = timeout_detector->timeout_sec;
+		timeout_detector->arm_timeout.it_value.tv_usec = timeout_detector->timeout_usec;
+		timeout_detector->detection_enabled = true;
+	}
+	else{
+			timeout_detector->detection_enabled = false;
+	}
+}
+
+void enable_timeout_detector(timeout_detector_t* timeout_detector){
+	timeout_detector->detection_enabled = true;
+}
+
+/*
+static void disable_timeout_detector(timeout_detector_t* timeout_detector){
+	timeout_detector->detection_enabled = false;
+
+	struct itimerval tmp;
+
+	timeout_detector->disarm_timeout.it_interval.tv_sec = 0;
+	timeout_detector->disarm_timeout.it_interval.tv_usec = 0;
+  assert(setitimer(ITIMER_REAL, &timeout_detector->disarm_timeout, &tmp) == 0);
+}
+*/
+
+
+void update_itimer(timeout_detector_t* timeout_detector, uint8_t sec, uint32_t usec){
+	//fprintf(stderr, "%s: %x %x\n", __func__, sec, usec);
+	if(sec || usec){
+		timeout_detector->timeout_sec = (time_t) sec;
+		timeout_detector->timeout_usec = (suseconds_t) usec;
+		timeout_detector->detection_enabled = true;
+	}
+	else{
+		timeout_detector->detection_enabled = false;
+	}
+}
+
+bool arm_sigprof_timer(timeout_detector_t* timeout_detector){
+    if(timeout_detector->detection_enabled){
+			if(timeout_detector->reload_pending || (!timeout_detector->arm_timeout.it_value.tv_sec && !timeout_detector->arm_timeout.it_value.tv_usec)){
+					fprintf(stderr, "TIMER EXPIRED 1! %d %ld %ld\n", timeout_detector->reload_pending, timeout_detector->arm_timeout.it_value.tv_sec, timeout_detector->arm_timeout.it_value.tv_usec);
+					reset_timeout_detector(timeout_detector);
+					return true;
+			}
+#ifdef DEBUG_TIMEOUT_DETECTOR
+				fprintf(stderr, "%s (%ld %ld)\n", __func__, timeout_detector->arm_timeout.it_value.tv_sec, timeout_detector->arm_timeout.it_value.tv_usec);
+#endif
+				timeout_detector->arm_timeout.it_interval.tv_sec = 0;
+				timeout_detector->arm_timeout.it_interval.tv_usec = 0;
+
+
+        assert(setitimer(ITIMER_REAL, &timeout_detector->arm_timeout, 0) == 0);
+    }
+		return false;
+}
+
+bool disarm_sigprof_timer(timeout_detector_t* timeout_detector){
+		struct itimerval tmp;
+
+    if(timeout_detector->detection_enabled){
+				timeout_detector->disarm_timeout.it_interval.tv_sec = 0;
+				timeout_detector->disarm_timeout.it_interval.tv_usec = 0;
+        assert(setitimer(ITIMER_REAL, &timeout_detector->disarm_timeout, &tmp) == 0);
+
+				timeout_detector->arm_timeout.it_value.tv_sec = tmp.it_value.tv_sec;
+				timeout_detector->arm_timeout.it_value.tv_usec = tmp.it_value.tv_usec;
+
+#ifdef DEBUG_TIMEOUT_DETECTOR
+				fprintf(stderr, "%s (%ld %ld)\n", __func__, timeout_detector->arm_timeout.it_value.tv_sec, timeout_detector->arm_timeout.it_value.tv_usec);
+#endif
+			if(timeout_detector->reload_pending || (!timeout_detector->arm_timeout.it_value.tv_sec && !timeout_detector->arm_timeout.it_value.tv_usec)){	
+					reset_timeout_detector(timeout_detector);
+					//timeout_detector->detection_enabled = false;
+					return true;
+			}
+	  }
+    return false;
+}
+
+void block_signals(void){
+  sigset_t set;
+
+	sigemptyset(&set);
+	sigaddset(&set, SIGALRM);
+	sigaddset(&set, SIGABRT);
+	sigaddset(&set, SIGSEGV);
+	pthread_sigmask(SIG_BLOCK, &set, NULL);
+}
+
+void unblock_signals(void){
+	sigset_t set;
+
+	sigemptyset(&set);
+	sigaddset(&set, SIGABRT);
+	sigaddset(&set, SIGSEGV);
+	sigaddset(&set, SIGALRM);
+	sigprocmask(SIG_UNBLOCK, &set, NULL);
+}
+
+static inline bool synchronization_check_page_not_found(void){
+	bool failure = false;
+
+	/* page is unavailable during the current execution */
+	if(GET_GLOBAL_STATE()->decoder_page_fault){		
+		set_page_not_found_result_buffer(GET_GLOBAL_STATE()->auxilary_buffer, GET_GLOBAL_STATE()->decoder_page_fault_addr);
+		GET_GLOBAL_STATE()->decoder_page_fault = false;
+		GET_GLOBAL_STATE()->decoder_page_fault_addr = 0;
+		failure = true;
+	}
+
+	/* page was dumped during this execution */
+	if(GET_GLOBAL_STATE()->dump_page){
+		kvm_remove_all_breakpoints(qemu_get_cpu(0));
+		kvm_vcpu_ioctl(qemu_get_cpu(0), KVM_VMX_PT_DISABLE_PAGE_DUMP_CR3);
+		kvm_vcpu_ioctl(qemu_get_cpu(0), KVM_VMX_PT_DISABLE_MTF);
+		failure = true;
+	}
+
+	return failure;
+}
+
+void synchronization_unlock(void){
+	pthread_mutex_lock(&synchronization_lock_mutex);
+	pthread_cond_signal(&synchronization_lock_condition);
+	pthread_mutex_unlock(&synchronization_lock_mutex);
+}
+
+
+uint64_t run_counter = 0;
+bool in_fuzzing_loop = false;
+
+void synchronization_lock_hprintf(void){
+	pthread_mutex_lock(&synchronization_lock_mutex);
+	interface_send_char(KAFL_PING);
+
+	pthread_cond_wait(&synchronization_lock_condition, &synchronization_lock_mutex);
+	pthread_mutex_unlock(&synchronization_lock_mutex);
+
+	flush_hprintf_auxiliary_buffer(GET_GLOBAL_STATE()->auxilary_buffer);
+
+}
+void synchronization_lock(void){
+
+	pthread_mutex_lock(&synchronization_lock_mutex);
+	run_counter++;
+
+	if(qemu_get_cpu(0)->intel_pt_run_trashed){
+		set_pt_overflow_auxiliary_result_buffer(GET_GLOBAL_STATE()->auxilary_buffer);
+	}
+	set_exec_done_auxiliary_result_buffer(GET_GLOBAL_STATE()->auxilary_buffer, GET_GLOBAL_STATE()->timeout_detector.arm_timeout.it_value.tv_sec, (uint32_t)GET_GLOBAL_STATE()->timeout_detector.arm_timeout.it_value.tv_usec);
+	reset_timeout_detector(&(GET_GLOBAL_STATE()->timeout_detector));
+
+	if(synchronization_check_page_not_found()){
+		set_success_auxiliary_result_buffer(GET_GLOBAL_STATE()->auxilary_buffer, 0);
+	}
+
+	if(GET_GLOBAL_STATE()->dump_page){
+		GET_GLOBAL_STATE()->dump_page = false;
+		GET_GLOBAL_STATE()->dump_page_addr = 0x0;
+		kvm_remove_all_breakpoints(qemu_get_cpu(0));
+		kvm_vcpu_ioctl(qemu_get_cpu(0), KVM_VMX_PT_DISABLE_PAGE_DUMP_CR3);
+	}
+
+	if(unlikely(GET_GLOBAL_STATE()->in_redqueen_reload_mode || GET_GLOBAL_STATE()->redqueen_state->trace_mode)){
+		if(GET_GLOBAL_STATE()->redqueen_state->trace_mode){
+			write_trace_result(GET_GLOBAL_STATE()->redqueen_state->trace_state);
+			redqueen_trace_reset(GET_GLOBAL_STATE()->redqueen_state->trace_state);
+		}
+		fsync_all_traces();		
+	}
+
+	interface_send_char(KAFL_PING);
+
+	pthread_cond_wait(&synchronization_lock_condition, &synchronization_lock_mutex);
+	pthread_mutex_unlock(&synchronization_lock_mutex);
+
+	flush_auxiliary_result_buffer(GET_GLOBAL_STATE()->auxilary_buffer);
+	check_auxiliary_config_buffer(GET_GLOBAL_STATE()->auxilary_buffer, &GET_GLOBAL_STATE()->shadow_config);
+	set_success_auxiliary_result_buffer(GET_GLOBAL_STATE()->auxilary_buffer, 1);
+}
+
+void synchronization_lock_crash_found(void){
+	if(!in_fuzzing_loop){
+		fprintf(stderr, "<%d-%ld>\t%s [NOT IN FUZZING LOOP]\n", getpid(), run_counter, __func__);
+	}
+
+	pt_disable(qemu_get_cpu(0), false);
+
+	set_crash_auxiliary_result_buffer(GET_GLOBAL_STATE()->auxilary_buffer);
+	set_reload_auxiliary_result_buffer(GET_GLOBAL_STATE()->auxilary_buffer);
+	
+	qemu_mutex_lock_iothread();
+	fast_reload_restore(get_fast_reload_snapshot());
+	qemu_mutex_unlock_iothread();
+
+	in_fuzzing_loop = false;
+}
+
+void synchronization_lock_asan_found(void){
+	if(!in_fuzzing_loop){
+		fprintf(stderr, "<%d-%ld>\t%s [NOT IN FUZZING LOOP]\n", getpid(), run_counter, __func__);
+		set_success_auxiliary_result_buffer(GET_GLOBAL_STATE()->auxilary_buffer, 0);
+	}
+
+	pt_disable(qemu_get_cpu(0), false);
+
+	set_asan_auxiliary_result_buffer(GET_GLOBAL_STATE()->auxilary_buffer);
+	set_reload_auxiliary_result_buffer(GET_GLOBAL_STATE()->auxilary_buffer);
+
+	qemu_mutex_lock_iothread();
+	fast_reload_restore(get_fast_reload_snapshot());
+	qemu_mutex_unlock_iothread();
+
+	in_fuzzing_loop = false;
+}
+
+void synchronization_lock_timeout_found(void){		
+	
+	if(!in_fuzzing_loop){
+		set_success_auxiliary_result_buffer(GET_GLOBAL_STATE()->auxilary_buffer, 0);
+	}	
+
+	pt_disable(qemu_get_cpu(0), false);
+
+	set_timeout_auxiliary_result_buffer(GET_GLOBAL_STATE()->auxilary_buffer);
+	set_reload_auxiliary_result_buffer(GET_GLOBAL_STATE()->auxilary_buffer);
+
+	reset_timeout_detector(&(GET_GLOBAL_STATE()->timeout_detector));
+
+	qemu_mutex_lock_iothread();
+	fast_reload_restore(get_fast_reload_snapshot());
+	qemu_mutex_unlock_iothread();
+
+	in_fuzzing_loop = false;
+}
+
+void synchronization_lock_shutdown_detected(void){
+	if(!in_fuzzing_loop){
+		fprintf(stderr, "<%d-%ld>\t%s [NOT IN FUZZING LOOP]\n", getpid(), run_counter, __func__);
+		set_success_auxiliary_result_buffer(GET_GLOBAL_STATE()->auxilary_buffer, 0);
+	}
+
+	pt_disable(qemu_get_cpu(0), false);
+
+	qemu_mutex_lock_iothread();
+	fast_reload_restore(get_fast_reload_snapshot());
+	qemu_mutex_unlock_iothread();
+	set_reload_auxiliary_result_buffer(GET_GLOBAL_STATE()->auxilary_buffer);
+
+	in_fuzzing_loop = false;
+	//synchronization_lock();
+}
+
+void synchronization_payload_buffer_write_detected(void){
+	static char reason[1024];
+
+	if(!in_fuzzing_loop){
+			fprintf(stderr, "<%d-%ld>\t%s [NOT IN FUZZING LOOP]\n", getpid(), run_counter, __func__);
+	}
+	
+	pt_disable(qemu_get_cpu(0), false);
+
+	int bytes = snprintf(reason, 1024, "Payload buffer write attempt at RIP: %lx\n", get_rip(qemu_get_cpu(0)));
+	set_payload_buffer_write_reason_auxiliary_buffer(GET_GLOBAL_STATE()->auxilary_buffer, reason, bytes);
+	set_reload_auxiliary_result_buffer(GET_GLOBAL_STATE()->auxilary_buffer);
+
+	qemu_mutex_lock_iothread();
+	fast_reload_restore(get_fast_reload_snapshot());
+	qemu_mutex_unlock_iothread();
+	set_reload_auxiliary_result_buffer(GET_GLOBAL_STATE()->auxilary_buffer);
+
+	in_fuzzing_loop = false;
+}
+
+void synchronization_cow_full_detected(void){
+	if(!in_fuzzing_loop){
+			fprintf(stderr, "<%d-%ld>\t%s [NOT IN FUZZING LOOP]\n", getpid(), run_counter, __func__);
+	}
+
+	pt_disable(qemu_get_cpu(0), false);
+
+	qemu_mutex_lock_iothread();
+	fast_reload_restore(get_fast_reload_snapshot());
+	qemu_mutex_unlock_iothread();
+	set_reload_auxiliary_result_buffer(GET_GLOBAL_STATE()->auxilary_buffer);
+
+	in_fuzzing_loop = false;
+}
+
+void synchronization_disable_pt(CPUState *cpu){
+	if(!in_fuzzing_loop){
+		fprintf(stderr, "<%d-%ld>\t%s [NOT IN FUZZING LOOP]\n", getpid(), run_counter, __func__);
+		set_success_auxiliary_result_buffer(GET_GLOBAL_STATE()->auxilary_buffer, 0);
+	}
+
+	pt_disable(qemu_get_cpu(0), false);
+
+	if(GET_GLOBAL_STATE()->in_reload_mode || GET_GLOBAL_STATE()->in_redqueen_reload_mode || GET_GLOBAL_STATE()->dump_page){
+		qemu_mutex_lock_iothread();
+		fast_reload_restore(get_fast_reload_snapshot());
+		qemu_mutex_unlock_iothread();
+		set_reload_auxiliary_result_buffer(GET_GLOBAL_STATE()->auxilary_buffer);
+	}
+
+	in_fuzzing_loop = false;
+}
+
+void synchronization_enter_fuzzing_loop(CPUState *cpu){
+	if (pt_enable(cpu, false) == 0){
+		cpu->pt_enabled = true;
+	}
+	in_fuzzing_loop = true;
+
+	reset_timeout_detector(&(GET_GLOBAL_STATE()->timeout_detector));
+}
+
diff --new-file -ur qemu/pt/synchronization.h QEMU-PT/pt/synchronization.h
--- qemu/pt/synchronization.h	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/synchronization.h	2021-08-24 21:54:55.938586449 +0200
@@ -0,0 +1,50 @@
+#pragma once
+
+#include "qemu/osdep.h"
+#include <linux/kvm.h>
+
+typedef struct timeout_detector_s{
+	int kvm_tid;
+	volatile bool reload_pending; 
+	volatile bool detection_enabled; 
+
+  time_t      timeout_sec;
+  suseconds_t timeout_usec; 
+
+	struct itimerval arm_timeout;
+	struct itimerval disarm_timeout;
+
+	struct timespec start_time;
+	struct timespec end_time;
+} timeout_detector_t;
+
+void init_timeout_detector(timeout_detector_t* timeout_detector);
+void install_timeout_detector(timeout_detector_t* timeout_detector);
+void reset_timeout_detector(timeout_detector_t* timeout_detector);
+bool arm_sigprof_timer(timeout_detector_t* timeout_detector);
+bool disarm_sigprof_timer(timeout_detector_t* timeout_detector);
+
+void update_itimer(timeout_detector_t* timeout_detector, uint8_t sec, uint32_t usec);
+
+void block_signals(void);
+void unblock_signals(void);
+
+
+void synchronization_unlock(void);
+
+void synchronization_lock_hprintf(void);
+
+
+void synchronization_lock(void);
+void synchronization_lock_crash_found(void);
+void synchronization_lock_asan_found(void);
+void synchronization_lock_timeout_found(void);
+void synchronization_lock_shutdown_detected(void);
+void synchronization_cow_full_detected(void);
+void synchronization_disable_pt(CPUState *cpu);
+void synchronization_enter_fuzzing_loop(CPUState *cpu);
+
+void enable_timeout_detector(timeout_detector_t* timeout_detector);
+void reset_timeout_detector_timeout(timeout_detector_t* timeout_detector);
+
+void synchronization_payload_buffer_write_detected(void);
\ No newline at end of file
diff --new-file -ur qemu/pt/tmp.objs QEMU-PT/pt/tmp.objs
--- qemu/pt/tmp.objs	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/tmp.objs	2021-08-24 21:54:55.938586449 +0200
@@ -0,0 +1,2 @@
+obj-y += decoder.o disassembler.o tnt_cache.o logger.o pt.o memory_access.o kafl_guest.o
+
diff --new-file -ur qemu/pt/tnt_cache.c QEMU-PT/pt/tnt_cache.c
--- qemu/pt/tnt_cache.c	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/tnt_cache.c	2021-08-24 21:54:55.938586449 +0200
@@ -0,0 +1,248 @@
+/*
+
+Copyright (C) 2017 Sergej Schumilo
+
+This file is part of QEMU-PT (kAFL).
+
+QEMU-PT is free software: you can redistribute it and/or modify
+it under the terms of the GNU General Public License as published by
+the Free Software Foundation, either version 2 of the License, or
+(at your option) any later version.
+
+QEMU-PT is distributed in the hope that it will be useful,
+but WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with QEMU-PT.  If not, see <http://www.gnu.org/licenses/>.
+
+*/
+
+#include "tnt_cache.h"
+#include <assert.h>
+#include <sys/mman.h>
+#include <string.h>
+
+//#define DEBUG
+
+#define BIT(x)				(1ULL << (x))
+
+static inline uint8_t asm_bsr(uint64_t x){
+	asm ("bsrq %0, %0" : "=r" (x) : "0" (x));
+	return x;
+}
+
+#define TNT_HASH_SPLIT_VALUE_BITS 58
+#define TNT_HASH_SPLIT_COUNT_BITS 6
+
+uint64_t get_tnt_hash(tnt_cache_t* self){
+	uint64_t count = 0;
+	uint64_t value = 0;
+
+
+	if(self->bl_tnt == 0){
+		return 0;
+	}
+	
+	count = (self->bl_tnt/TNT_HASH_SPLIT_VALUE_BITS) != 0 ? TNT_HASH_SPLIT_VALUE_BITS : self->bl_tnt;
+
+	uint8_t bits_1 = 32 - (self->bl_pos%32); 	
+
+	uint8_t tmp1 = (TNT_HASH_SPLIT_VALUE_BITS-bits_1)%32;
+	uint8_t tmp2 = !!((TNT_HASH_SPLIT_VALUE_BITS-bits_1)/32);
+	uint8_t bits_2 = (32*tmp2) | (tmp1*(!tmp2));
+
+	uint8_t bits_3 = TNT_HASH_SPLIT_VALUE_BITS-(bits_1+bits_2);
+
+	uint32_t value_1 = self->bl_tnt_memory[((self->bl_pos/32)+0)%BL_BUF_ENTRIES];
+	uint32_t value_2 = self->bl_tnt_memory[((self->bl_pos/32)+1)%BL_BUF_ENTRIES];
+	uint32_t value_3 = self->bl_tnt_memory[((self->bl_pos/32)+2)%BL_BUF_ENTRIES];
+
+	/* mask + shift */
+	value = (((uint64_t)value_1) & (0xFFFFFFFF >> (32-bits_1))) << (TNT_HASH_SPLIT_VALUE_BITS-bits_1);
+	value |= ((((uint64_t)value_2) & (0xFFFFFFFF << (32-bits_2))) << (TNT_HASH_SPLIT_VALUE_BITS-bits_2-bits_1)) >> (32-bits_2);  /* done! */
+	value |= ((((uint64_t)value_3) & (0xFFFFFFFF << ((32-bits_3)&0x1F))) >> (32-bits_3)); /* don't shift */
+	value >>= (TNT_HASH_SPLIT_VALUE_BITS-count);
+
+	return (count << TNT_HASH_SPLIT_VALUE_BITS) | value;
+}	
+
+
+#ifdef NON_BRANCH_LESS_CODE
+static inline uint8_t process_tnt_cache_nbl(tnt_cache_t* self){
+	uint8_t result;
+	if (self->tnt){
+		result = self->tnt_memory[self->pos];
+		self->tnt--;
+		self->pos = (self->pos + 1) % BUF_SIZE;
+#ifdef DEBUG
+		printf("-> %d\n", result);
+#endif
+		return result;
+	}
+	return TNT_EMPTY;
+}
+
+static inline void append_tnt_cache_nbl(tnt_cache_t* self, uint8_t data){
+	uint8_t bits = asm_bsr(data)-SHORT_TNT_OFFSET;
+	for(uint8_t i = SHORT_TNT_OFFSET; i < bits+SHORT_TNT_OFFSET; i++){
+#ifdef DEBUG
+		printf("%x\n", ((data) & BIT(i)) >> i);
+#endif
+		self->tnt_memory[((self->max+bits-i)%BUF_SIZE)] = ((data) & BIT(i)) >> i;
+	}
+
+	self->tnt += bits;
+	assert(self->tnt < BUF_SIZE);
+	self->max = (self->max + bits) % BUF_SIZE;
+}
+#endif
+
+#ifdef BRANCH_LESS_CODE
+static inline uint8_t process_tnt_cache_bl(tnt_cache_t* self){
+	if(self->bl_tnt){
+	  uint8_t res =  !!((BIT(31) >> (self->bl_pos%32)) & self->bl_tnt_memory[(self->bl_pos/32)%BL_BUF_ENTRIES]);
+		self->bl_tnt--;
+		self->bl_pos = (self->bl_pos + 1) % (BUF_SIZE);
+		return res;
+	}
+	return TNT_EMPTY;
+}
+
+static inline void append_tnt_cache_bl(tnt_cache_t* self, uint8_t data){
+	uint8_t bits = asm_bsr(data)-SHORT_TNT_OFFSET;
+	uint64_t offset = (self->bl_tnt+self->bl_pos);
+	
+	uint64_t tmp_data = (((uint64_t)data) << (64-bits-SHORT_TNT_OFFSET)) >> (offset%32);
+	uint64_t tmp_value = (((uint64_t)self->bl_tnt_memory[(offset/32)%BL_BUF_ENTRIES]) << 32) | (uint64_t)self->bl_tnt_memory[((offset/32)+1)%BL_BUF_ENTRIES];
+	uint64_t result = (tmp_value & ~(0xFFFFFFFFFFFFFFFFULL >> (offset%32))) | tmp_data;
+
+	self->bl_tnt_memory[(offset/32)%BL_BUF_ENTRIES] = result >> 32;
+	self->bl_tnt_memory[((offset/32)+1)%BL_BUF_ENTRIES] = result & 0xFFFFFFFFULL;
+	self->bl_tnt += bits;
+}
+#endif
+
+
+uint8_t process_tnt_cache(tnt_cache_t* self){
+#if defined(NON_BRANCH_LESS_CODE) && defined(BRANCH_LESS_CODE)
+	assert(self->tnt == self->bl_tnt);
+	uint8_t result_a = process_tnt_cache_nbl(self);
+	uint8_t result_b = process_tnt_cache_bl(self);
+	assert(result_a == result_b);
+	return result_b;
+#endif
+
+#ifdef NON_BRANCH_LESS_CODE
+	return process_tnt_cache_nbl(self);
+#endif
+#ifdef BRANCH_LESS_CODE
+	return process_tnt_cache_bl(self);
+#endif
+}
+
+void append_tnt_cache(tnt_cache_t* self, uint8_t data){
+#ifdef NON_BRANCH_LESS_CODE
+	append_tnt_cache_nbl(self, data);
+#endif
+#ifdef BRANCH_LESS_CODE
+	append_tnt_cache_bl(self, data);
+#endif
+}
+
+void append_tnt_cache_ltnt(tnt_cache_t* self, uint64_t data){
+#ifdef NON_BRANCH_LESS_CODE
+	uint8_t bits = asm_bsr(data)-LONG_TNT_MAX_BITS;
+	for(uint8_t i = LONG_TNT_MAX_BITS; i < bits+LONG_TNT_MAX_BITS; i++){
+		self->tnt_memory[((self->max+bits-i)%BUF_SIZE)] = ((data) & BIT(i)) >> i;
+	}
+
+	self->tnt += bits;
+	assert(self->tnt < BUF_SIZE);
+	self->max = (self->max + bits) % BUF_SIZE;
+#endif
+
+#ifdef BRANCH_LESS_CODE
+	assert(0);
+#endif
+}	
+
+bool is_empty_tnt_cache(tnt_cache_t* self){
+#ifdef NON_BRANCH_LESS_CODE
+	return self->tnt == 0;
+#endif
+#ifdef BRANCH_LESS_CODE
+	return self->bl_tnt == 0;
+#endif
+}
+
+int count_tnt(tnt_cache_t* self){
+#ifdef NON_BRANCH_LESS_CODE
+	return self->tnt;
+#endif
+#ifdef BRANCH_LESS_CODE
+	return self->bl_tnt;
+#endif
+}
+
+tnt_cache_t* tnt_cache_init(void){
+	tnt_cache_t* self = malloc(sizeof(tnt_cache_t));
+#ifdef NON_BRANCH_LESS_CODE
+	self->tnt_memory = (uint8_t*)mmap(NULL, BUF_SIZE, PROT_READ | PROT_WRITE, MAP_ANONYMOUS | MAP_PRIVATE, 0, 0);
+	self->max = 0;
+	self->pos = 0;
+	self->tnt = 0;
+#endif
+
+#ifdef BRANCH_LESS_CODE
+	self->bl_tnt_memory = (uint32_t*)mmap(NULL, BUF_SIZE/8, PROT_READ | PROT_WRITE, MAP_ANONYMOUS | MAP_PRIVATE, 0, 0);
+	self->bl_max = 0;
+	self->bl_pos = 0;
+	self->bl_tnt = 0;
+#endif
+
+	return self;
+}
+
+void tnt_cache_flush(tnt_cache_t* self){
+#ifdef NON_BRANCH_LESS_CODE
+	self->max = 0;
+	self->pos = 0;
+	self->tnt = 0;
+#endif
+
+#ifdef BRANCH_LESS_CODE
+	self->bl_max = 0;
+	self->bl_pos = 0;
+	self->bl_tnt = 0;
+#endif
+}
+
+void tnt_cache_destroy(tnt_cache_t* self){
+#ifdef NON_BRANCH_LESS_CODE
+	munmap(self->tnt_memory, BUF_SIZE);
+	self->max = 0;
+	self->pos = 0;
+	self->tnt = 0;
+#endif
+
+#ifdef BRANCH_LESS_CODE
+	munmap(self->bl_tnt_memory, BUF_SIZE/8);
+	self->bl_max = 0;
+	self->bl_pos = 0;
+	self->bl_tnt = 0;
+#endif
+
+	free(self);
+}
+
+void adjust_tnt_cache(tnt_cache_t* self, uint8_t num){
+	if (num > self->bl_tnt){
+		num = self->bl_tnt;
+	}
+
+	self->bl_tnt -= num;
+	self->bl_pos = (self->bl_pos + num) % (BUF_SIZE);
+}
+
diff --new-file -ur qemu/pt/tnt_cache.h QEMU-PT/pt/tnt_cache.h
--- qemu/pt/tnt_cache.h	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/tnt_cache.h	2021-08-24 21:54:55.942586446 +0200
@@ -0,0 +1,78 @@
+/*
+
+Copyright (C) 2017 Sergej Schumilo
+
+This file is part of QEMU-PT (kAFL).
+
+QEMU-PT is free software: you can redistribute it and/or modify
+it under the terms of the GNU General Public License as published by
+the Free Software Foundation, either version 2 of the License, or
+(at your option) any later version.
+
+QEMU-PT is distributed in the hope that it will be useful,
+but WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with QEMU-PT.  If not, see <http://www.gnu.org/licenses/>.
+
+*/
+
+#ifndef TNT_CACHE_H
+#define TNT_CACHE_H
+
+#include <stdio.h>
+#include <stdint.h>
+#include <stdlib.h>
+#include <stdbool.h>
+
+//#define NON_BRANCH_LESS_CODE
+#define BRANCH_LESS_CODE
+
+#define NOT_TAKEN			0
+#define TAKEN				1
+#define TNT_EMPTY			2
+
+#define SHORT_TNT_OFFSET	1
+#define SHORT_TNT_MAX_BITS	8-1-SHORT_TNT_OFFSET
+
+#define LONG_TNT_OFFSET		16
+#define LONG_TNT_MAX_BITS	64-1-LONG_TNT_OFFSET
+
+#define BUF_SIZE 0x100000000	/* 4G slots */
+#define BL_BUF_ENTRIES ((BUF_SIZE/8)/32)
+
+typedef struct tnt_cache_s{
+#ifdef NON_BRANCH_LESS_CODE
+	uint8_t* tnt_memory;
+	uint64_t pos;
+	uint64_t max;
+	uint64_t tnt;
+#endif
+
+#ifdef BRANCH_LESS_CODE
+	uint32_t* bl_tnt_memory;
+	uint64_t bl_pos;
+	uint64_t bl_max;
+	uint64_t bl_tnt;
+#endif
+} tnt_cache_t;
+
+uint64_t get_tnt_hash(tnt_cache_t* self);
+
+tnt_cache_t* tnt_cache_init(void);
+void tnt_cache_destroy(tnt_cache_t* self);
+void tnt_cache_flush(tnt_cache_t* self);
+
+
+bool is_empty_tnt_cache(tnt_cache_t* self);
+int count_tnt(tnt_cache_t* self);
+uint8_t process_tnt_cache(tnt_cache_t* self);
+
+void append_tnt_cache(tnt_cache_t* self, uint8_t data);
+void append_tnt_cache_ltnt(tnt_cache_t* self, uint64_t data);
+
+void adjust_tnt_cache(tnt_cache_t* self, uint8_t num);
+
+#endif 
diff --new-file -ur qemu/pt/trace_cache.c QEMU-PT/pt/trace_cache.c
--- qemu/pt/trace_cache.c	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/trace_cache.c	2021-08-24 21:54:55.938586449 +0200
@@ -0,0 +1,197 @@
+#include <assert.h>
+#include <string.h>
+#include "trace_cache.h"
+#include "mmh3.h"
+
+/* result: 
+	7-0: value
+	23-7: offset
+*/
+
+tracelet_cache_tmp_t* new_tracelet_cache(size_t bitmap_size){
+
+	tracelet_cache_tmp_t* self = malloc(sizeof(tracelet_cache_tmp_t));
+	self->cache.next_entry_address = 0;
+	self->cache.tnt_bits = 0;
+	self->cache.result_bits = 0;
+	self->cache.result_bits_max = MAX_RESULTS_PER_CACHE;
+	self->cache.bitmap_results = malloc(sizeof(uint32_t)*MAX_RESULTS_PER_CACHE);
+
+	self->lookup_bitmap = malloc(sizeof(uint8_t)*bitmap_size);
+	memset(self->lookup_bitmap, 0x0, bitmap_size);
+
+	return self;
+}
+
+void tracelet_cache_tmp_destroy(tracelet_cache_tmp_t* self){
+	free(self->cache.bitmap_results);
+	free(self->lookup_bitmap);
+	free(self);
+}
+
+void set_next_entry_addres_tracelet_cache(tracelet_cache_t* self, uint64_t next_entry_address){
+	self->next_entry_address = next_entry_address;
+}
+
+
+void reset_tracelet_tmp_cache(tracelet_cache_tmp_t* self){
+	self->cache.next_entry_address = 0xFFFFFFFFFFFFFFFFULL;
+
+	for(uint8_t i = 0; i < self->cache.result_bits; i++){
+		self->lookup_bitmap[self->cache.bitmap_results[i]] = 0;
+	}
+
+	self->cache.tnt_bits = 0;
+	self->cache.result_bits = 0;
+}
+
+
+static inline uint64_t mix_bits(uint64_t v) {
+  v ^= (v >> 31);
+  v *= 0x7fb5d329728ea185;
+  return v;
+}
+
+static uint32_t generate_result_offset(uint64_t from, uint64_t to){
+	uint32_t transition_value = mix_bits(to)^(mix_bits(from)>>1);
+	return transition_value;
+}
+
+
+uint32_t fuzz_bitmap_size = 0x10000;
+uint8_t* fuzz_bitmap = NULL;
+
+void add_result_tracelet_cache(tracelet_cache_tmp_t* self, uint64_t from, uint64_t to){
+	assert(self->cache.result_bits < self->cache.result_bits_max);
+
+	uint32_t offset = generate_result_offset(from, to) & (fuzz_bitmap_size-1);
+
+	if(!self->lookup_bitmap[offset]){
+		self->cache.bitmap_results[self->cache.result_bits++] = offset;
+	}
+
+	self->lookup_bitmap[offset]++;
+	self->cache.tnt_bits++;
+}
+
+tracelet_cache_t* new_from_tracelet_cache_tmp(tracelet_cache_tmp_t* tmp_cache, bool cont_exec){
+	tracelet_cache_t* new = malloc(sizeof(tracelet_cache_t));
+
+	new->next_entry_address = tmp_cache->cache.next_entry_address;
+	new->tnt_bits = tmp_cache->cache.tnt_bits;
+	new->result_bits = tmp_cache->cache.result_bits;
+	new->result_bits_max = new->result_bits;
+	new->bitmap_results = malloc(sizeof(uint32_t)*tmp_cache->cache.result_bits);
+	new->cont_exec = cont_exec;
+
+	for(uint8_t i = 0; i < tmp_cache->cache.result_bits; i++){
+		uint32_t offset = tmp_cache->cache.bitmap_results[i];
+		uint32_t result = tmp_cache->lookup_bitmap[offset] << 24;
+		new->bitmap_results[i] = result | offset;
+	}
+
+
+	return new;
+}
+
+void tracelet_cache_destroy(tracelet_cache_t* self){
+	free(self->bitmap_results);
+	free(self);
+}
+
+uint64_t apply_trace_cache_to_bitmap(tracelet_cache_t* self, tnt_cache_t* tnt_cache_state, bool adjust){
+	
+	for(uint8_t i = 0; i < self->result_bits; i++){
+		
+		uint8_t result = self->bitmap_results[i] >> 24;
+		uint32_t offset = self->bitmap_results[i] & 0xFFFFFF;
+		fuzz_bitmap[offset] += result;
+		
+	}
+	if(adjust){
+		adjust_tnt_cache(tnt_cache_state, self->tnt_bits);
+	}
+	return self->next_entry_address;
+}
+
+void fuzz_bitmap_set_size(uint32_t size){
+	fuzz_bitmap_size = size;
+}
+
+uint32_t fuzz_bitmap_get_size(void){
+	return fuzz_bitmap_size;
+}
+
+void fuzz_bitmap_set_ptr(void* ptr){
+	fuzz_bitmap = (uint8_t*) ptr;
+}
+
+void fuzz_bitmap_reset(void){
+	if(fuzz_bitmap){
+		memset(fuzz_bitmap, 0x00, fuzz_bitmap_size);
+	}
+}
+
+uint64_t fuzz_bitmap_get_hash(void){
+	if(fuzz_bitmap){
+		uint64_t hash[2];
+		mmh3_x64_128(fuzz_bitmap, fuzz_bitmap_size, 0xaaaaaaaa, &hash);
+		return hash[0];
+	}
+	return 0;
+}
+
+void fuzz_bitmap_set(uint64_t from, uint64_t to){
+	uint32_t transition_value = 0;
+	if(fuzz_bitmap){		
+		transition_value = mix_bits(to)^(mix_bits(from)>>1);	
+		fuzz_bitmap[transition_value & (fuzz_bitmap_size-1)]++;
+	}
+}
+
+uint8_t* fuzz_bitmap_get_ptr(void){
+	return fuzz_bitmap;
+}
+
+
+trace_cache_t* trace_cache_new(size_t bitmap_size){
+	trace_cache_t* self = malloc(sizeof(trace_cache_t));
+	self->lookup = kh_init(TRACE_CACHE);
+	self->trace_cache = new_tracelet_cache(bitmap_size);
+	return self;
+}
+
+void trace_cache_destroy(trace_cache_t* self){
+	khiter_t k;
+	for (k = kh_begin(self->lookup); k != kh_end(self->lookup); ++k){
+		if (kh_exist(self->lookup, k)){
+			tracelet_cache_destroy(kh_value(self->lookup, k));
+		}
+	}
+
+	kh_destroy(TRACE_CACHE, self->lookup);
+	tracelet_cache_tmp_destroy(self->trace_cache);
+	free(self);
+}
+
+void trace_cache_add(trace_cache_t* self, trace_cache_key_t key, tracelet_cache_t* tracelet){
+	khiter_t k;
+	int ret;
+	k = kh_get(TRACE_CACHE, self->lookup, key); 
+	if(k == kh_end(self->lookup)){
+		k = kh_put(TRACE_CACHE, self->lookup, key, &ret); 
+		kh_value(self->lookup, k) = tracelet;
+	}
+	return;
+}
+
+tracelet_cache_t* trace_cache_fetch(trace_cache_t* self, trace_cache_key_t key){
+	khiter_t k;
+	k = kh_get(TRACE_CACHE, self->lookup, key); 
+	if(k != kh_end(self->lookup)){
+		return kh_value(self->lookup, k);
+	}
+	return NULL;
+}
+
+
diff --new-file -ur qemu/pt/trace_cache.h QEMU-PT/pt/trace_cache.h
--- qemu/pt/trace_cache.h	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt/trace_cache.h	2021-08-24 21:54:55.938586449 +0200
@@ -0,0 +1,77 @@
+#pragma once
+
+#include <stdint.h>
+#include <stdlib.h>
+#include "khash.h"
+#include "tnt_cache.h"
+#include <immintrin.h>
+
+/* 
+	6 bit of an uint64_t are used to encode the number of tnt bits-
+	The remaining 58 bits are used to store the actual tnt bits. 
+*/
+#define MAX_RESULTS_PER_CACHE 58
+
+
+
+typedef struct tracelet_cache_s{
+	uint64_t next_entry_address;
+	uint8_t tnt_bits;
+
+	uint8_t result_bits; 
+	uint8_t result_bits_max;
+	uint32_t* bitmap_results;
+	bool cont_exec;
+} tracelet_cache_t;
+
+
+typedef struct tracelet_cache_tmp_s{
+	tracelet_cache_t cache;
+	uint8_t* lookup_bitmap;
+} tracelet_cache_tmp_t;
+
+tracelet_cache_tmp_t* new_tracelet_cache(size_t bitmap_size);
+void tracelet_cache_tmp_destroy(tracelet_cache_tmp_t* self);
+void reset_tracelet_tmp_cache(tracelet_cache_tmp_t* self);
+void add_result_tracelet_cache(tracelet_cache_tmp_t* self, uint64_t from, uint64_t to);
+void set_next_entry_addres_tracelet_cache(tracelet_cache_t* self, uint64_t next_entry_address);
+
+
+tracelet_cache_t* new_from_tracelet_cache_tmp(tracelet_cache_tmp_t* tmp_cache, bool cont_exec);
+void tracelet_cache_destroy(tracelet_cache_t* self);
+uint64_t apply_trace_cache_to_bitmap(tracelet_cache_t* self, tnt_cache_t* tnt_cache_state, bool adjust);
+
+/* singleton implementation */
+uint32_t fuzz_bitmap_get_size(void);
+void fuzz_bitmap_set_size(uint32_t size);
+void fuzz_bitmap_set_ptr(void* ptr);
+void fuzz_bitmap_reset(void);
+uint64_t fuzz_bitmap_get_hash(void);
+void fuzz_bitmap_set(uint64_t from, uint64_t to);
+uint8_t* fuzz_bitmap_get_ptr(void);
+
+typedef struct trace_cache_key_s{
+	uint64_t tnt_hash;
+	uint64_t entry;
+	uint64_t limit;
+} trace_cache_key_t;
+
+
+#define kh_trace_cache_key_t_hash_func(key)  (khint32_t)(((key.entry)>>33^(key.entry)^(key.entry)<<11) ^ ((key.limit)>>33^(key.limit)^(key.limit)<<11) ^ ((key.tnt_hash)>>33^(key.tnt_hash)^(key.tnt_hash)<<11))
+
+//static inline int kh_trace_cache_key_t_equal(trace_cache_key_t k1, trace_cache_key_t k2) { return !memcmp(&k1, &k2, sizeof(k1)); }
+static inline int kh_trace_cache_key_t_equal(trace_cache_key_t k1, trace_cache_key_t k2) { return k1.tnt_hash == k2.tnt_hash && k1.entry == k2.entry && k1.limit == k2.limit ; }
+
+KHASH_INIT(TRACE_CACHE, trace_cache_key_t, tracelet_cache_t*, 1, kh_trace_cache_key_t_hash_func, kh_trace_cache_key_t_equal)
+
+
+typedef struct trace_cache_s{
+	khash_t(TRACE_CACHE) *lookup;
+	tracelet_cache_tmp_t* trace_cache;
+} trace_cache_t;
+
+trace_cache_t* trace_cache_new(size_t bitmap_size);
+void trace_cache_destroy(trace_cache_t* self);
+void trace_cache_add(trace_cache_t* self, trace_cache_key_t key, tracelet_cache_t* tracelet);
+tracelet_cache_t* trace_cache_fetch(trace_cache_t* self, trace_cache_key_t key);
+
diff --new-file -ur qemu/pt.c QEMU-PT/pt.c
--- qemu/pt.c	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt.c	2021-08-24 21:54:55.930586453 +0200
@@ -0,0 +1,388 @@
+/*
+
+Copyright (C) 2017 Sergej Schumilo
+
+This file is part of QEMU-PT (kAFL).
+
+QEMU-PT is free software: you can redistribute it and/or modify
+it under the terms of the GNU General Public License as published by
+the Free Software Foundation, either version 2 of the License, or
+(at your option) any later version.
+
+QEMU-PT is distributed in the hope that it will be useful,
+but WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with QEMU-PT.  If not, see <http://www.gnu.org/licenses/>.
+
+*/
+
+#include "qemu/osdep.h"
+#include <linux/kvm.h>
+#include <sys/ioctl.h>
+#include <sys/mman.h>
+#include "qemu-common.h"
+#include "cpu.h"
+#include "pt.h"
+#include "pt/decoder.h"
+#include "exec/memory.h"
+#include "sysemu/kvm_int.h"
+#include "sysemu/kvm.h"
+#include "sysemu/cpus.h"
+#include "pt/hypercall.h"
+#include "pt/logger.h"
+#include "pt/memory_access.h"
+#include "pt/interface.h"
+#include "pt/debug.h"
+#ifdef CONFIG_REDQUEEN
+#include "pt/redqueen.h"
+#include "pt/redqueen_patch.h"
+#include "pt/patcher.h"
+#endif
+#include "pt/page_cache.h"
+#include "pt/state.h"
+
+#define PT_BUFFER_MMAP_ADDR 0x3ffff0000000
+
+
+uint32_t state_byte = 0;
+uint32_t last = 0;
+
+int pt_trace_dump_fd = 0;
+bool should_dump_pt_trace= false;
+
+void pt_open_pt_trace_file(char* filename){
+  pt_trace_dump_fd = open(filename, O_WRONLY);
+  should_dump_pt_trace = true;
+  assert(pt_trace_dump_fd >= 0);
+}
+
+void pt_trucate_pt_trace_file(void){
+  if(should_dump_pt_trace){
+    assert(lseek(pt_trace_dump_fd, 0, SEEK_SET) == 0);
+    assert(ftruncate(pt_trace_dump_fd, 0)==0);
+  }
+}
+
+static void pt_set(CPUState *cpu, run_on_cpu_data arg){
+	asm volatile("" ::: "memory");
+}
+
+static inline int pt_cmd_hmp_context(CPUState *cpu, uint64_t cmd){
+	cpu->pt_ret = -1;
+	if(pt_hypercalls_enabled()){
+		QEMU_PT_PRINTF(PT_PREFIX, "Error: HMP commands are ignored if kafl tracing mode is enabled (-kafl)!");
+	}
+	else{
+		cpu->pt_cmd = cmd;
+		run_on_cpu(cpu, pt_set, RUN_ON_CPU_NULL);
+	}
+	return cpu->pt_ret;
+}
+
+static int pt_cmd(CPUState *cpu, uint64_t cmd, bool hmp_mode){
+	if (hmp_mode){
+		return pt_cmd_hmp_context(cpu, cmd);
+	}
+	else {
+		cpu->pt_cmd = cmd;
+		pt_pre_kvm_run(cpu);
+		return cpu->pt_ret;
+	}
+}
+
+static inline int pt_ioctl(int fd, unsigned long request, unsigned long arg){
+	if (!fd){
+		return -EINVAL;
+	}
+	return ioctl(fd, request, arg);
+}
+
+static inline uint64_t mix_bits(uint64_t v) {
+  v ^= (v >> 31);
+  v *= 0x7fb5d329728ea185;
+  return v;
+}
+
+void pt_dump(CPUState *cpu, int bytes){
+
+  if(should_dump_pt_trace){
+    assert(bytes == write(pt_trace_dump_fd, cpu->pt_mmap, bytes));
+  }
+#ifdef SAMPLE_RAW
+	sample_raw(cpu->pt_mmap, bytes);
+#endif
+#ifdef SAMPLE_RAW_SINGLE
+	sample_raw_single(cpu->pt_mmap, bytes);
+#endif
+#ifdef CONFIG_REDQUEEN	
+	if(!(GET_GLOBAL_STATE()->redqueen_state && GET_GLOBAL_STATE()->redqueen_state->intercept_mode)){
+#endif
+		if (GET_GLOBAL_STATE()->in_fuzzing_mode && GET_GLOBAL_STATE()->decoder_page_fault == false && cpu->pt_decoder_state && !GET_GLOBAL_STATE()->dump_page){
+			decoder_result_t result = decode_buffer(cpu->pt_decoder_state, cpu->pt_mmap, bytes);
+			switch(result){
+				case decoder_success:
+					break;
+				case decoder_success_pt_overflow:
+					cpu->intel_pt_run_trashed = true;
+					break;
+				case decoder_page_fault:
+					GET_GLOBAL_STATE()->decoder_page_fault = true;
+					GET_GLOBAL_STATE()->decoder_page_fault_addr = pt_decoder_get_page_fault_addr(cpu->pt_decoder_state);
+					break;
+				case decoder_error:
+					fprintf(stderr, "WARNING: decode_buffer returned decoder_error\n");
+					break;
+			}
+		}
+#ifdef CONFIG_REDQUEEN			
+		}
+#endif
+}
+
+
+int pt_enable(CPUState *cpu, bool hmp_mode){
+#ifdef SAMPLE_RAW
+	init_sample_raw();
+#endif
+#ifdef SAMPLE_RAW_SINGLE
+	init_sample_raw_single(getpid());
+#endif
+#ifdef SAMPLE_DECODED
+	init_sample_decoded();
+#endif
+#ifdef SAMPLE_DECODED_DETAILED
+	init_sample_decoded_detailed();
+#endif
+	fuzz_bitmap_reset();
+  pt_trucate_pt_trace_file();
+	return pt_cmd(cpu, KVM_VMX_PT_ENABLE, hmp_mode);
+}
+	
+int pt_disable(CPUState *cpu, bool hmp_mode){
+	int r = pt_cmd(cpu, KVM_VMX_PT_DISABLE, hmp_mode);
+	if(GET_GLOBAL_STATE()->pt_ip_filter_enabled && cpu->pt_decoder_state){
+		pt_decoder_flush(cpu->pt_decoder_state);
+	}
+	return r;
+}
+
+int pt_set_cr3(CPUState *cpu, uint64_t val, bool hmp_mode){
+	if (val == GET_GLOBAL_STATE()->pt_c3_filter){
+		return 0; // nothing changed  
+	}
+	int r = 0;
+	
+	if (cpu->pt_enabled){
+		return -EINVAL;
+	}
+	if (GET_GLOBAL_STATE()->pt_c3_filter && GET_GLOBAL_STATE()->pt_c3_filter != val){
+		GET_GLOBAL_STATE()->pt_c3_filter = val;
+		r += pt_cmd(cpu, KVM_VMX_PT_CONFIGURE_CR3, hmp_mode);
+		r += pt_cmd(cpu, KVM_VMX_PT_ENABLE_CR3, hmp_mode);
+		return r;
+	}
+	GET_GLOBAL_STATE()->pt_c3_filter = val;
+	r += pt_cmd(cpu, KVM_VMX_PT_CONFIGURE_CR3, hmp_mode);
+	r += pt_cmd(cpu, KVM_VMX_PT_ENABLE_CR3, hmp_mode);
+	return r;
+}
+
+#ifdef CONFIG_REDQUEEN
+int pt_enable_ip_filtering(CPUState *cpu, uint8_t addrn, bool redqueen, bool hmp_mode){
+#else
+int pt_enable_ip_filtering(CPUState *cpu, uint8_t addrn, uint64_t filter[4][2], bool hmp_mode){
+#endif
+	int r = 0;
+
+	uint64_t filters[4][2] = {0};
+
+	if(addrn > 3){
+		return -1;
+	}
+
+	if (cpu->pt_enabled){
+		return -EINVAL;
+	}
+		
+	if(GET_GLOBAL_STATE()->pt_ip_filter_a[addrn] > GET_GLOBAL_STATE()->pt_ip_filter_b[addrn]){
+		QEMU_PT_PRINTF(PT_PREFIX, "Error (ip_a > ip_b) 0x%lx-0x%lx", GET_GLOBAL_STATE()->pt_ip_filter_a[addrn] , GET_GLOBAL_STATE()->pt_ip_filter_b[addrn]);
+		return -EINVAL;
+	}
+
+	if(GET_GLOBAL_STATE()->pt_ip_filter_enabled[addrn]){
+		pt_disable_ip_filtering(cpu, addrn, hmp_mode);
+	}
+
+	QEMU_PT_PRINTF(PT_PREFIX, "Configuring new trace region (addr%d, 0x%lx-0x%lx)", addrn, GET_GLOBAL_STATE()->pt_ip_filter_a[addrn] , GET_GLOBAL_STATE()->pt_ip_filter_b[addrn]);
+	
+#ifdef CONFIG_REDQUEEN
+
+	filters[0][0] = GET_GLOBAL_STATE()->pt_ip_filter_a[0];
+	filters[0][1] = GET_GLOBAL_STATE()->pt_ip_filter_b[0];
+	filters[1][0] = GET_GLOBAL_STATE()->pt_ip_filter_a[1];
+	filters[1][1] = GET_GLOBAL_STATE()->pt_ip_filter_b[1];
+	filters[2][0] = GET_GLOBAL_STATE()->pt_ip_filter_a[2];
+	filters[2][1] = GET_GLOBAL_STATE()->pt_ip_filter_b[2];
+	filters[3][0] = GET_GLOBAL_STATE()->pt_ip_filter_a[3];
+	filters[3][1] = GET_GLOBAL_STATE()->pt_ip_filter_b[3];
+
+	cpu->pt_decoder_state = pt_decoder_init(filters, GET_GLOBAL_STATE()->disassembler_word_width, get_redqueen_state(), get_page_cache());
+#else		
+	cpu->pt_decoder_state = pt_decoder_init(filters, cpu->disassembler_word_width, page_cache);
+#endif
+
+	if(GET_GLOBAL_STATE()->pt_ip_filter_configured[addrn] && GET_GLOBAL_STATE()->pt_ip_filter_a[addrn] != 0 && GET_GLOBAL_STATE()->pt_ip_filter_b[addrn] != 0){
+			r += pt_cmd(cpu, KVM_VMX_PT_CONFIGURE_ADDR0+addrn, hmp_mode);
+			r += pt_cmd(cpu, KVM_VMX_PT_ENABLE_ADDR0+addrn, hmp_mode);
+			GET_GLOBAL_STATE()->pt_ip_filter_enabled[addrn] = true;
+	}
+	return r;
+}
+
+int pt_disable_ip_filtering(CPUState *cpu, uint8_t addrn, bool hmp_mode){
+	int r = 0;
+	switch(addrn){
+		case 0:
+		case 1:
+		case 2:
+		case 3:
+			r = pt_cmd(cpu, KVM_VMX_PT_DISABLE_ADDR0+addrn, hmp_mode);
+			if(GET_GLOBAL_STATE()->pt_ip_filter_enabled[addrn]){
+				GET_GLOBAL_STATE()->pt_ip_filter_enabled[addrn] = false;
+			}
+			break;
+		default:
+			r = -EINVAL;
+	}
+	return r;
+}
+
+void pt_kvm_init(CPUState *cpu){
+	cpu->pt_cmd = 0;
+	cpu->pt_enabled = false;
+	cpu->pt_fd = 0;
+
+	cpu->pt_decoder_state = NULL;
+	cpu->reload_pending = false;
+	cpu->intel_pt_run_trashed = false;
+}
+
+struct vmx_pt_filter_iprs {
+	__u64 a;
+	__u64 b;
+};
+
+pthread_mutex_t pt_dump_mutex = PTHREAD_MUTEX_INITIALIZER;
+
+void pt_pre_kvm_run(CPUState *cpu){
+	pthread_mutex_lock(&pt_dump_mutex);
+	int ret;
+	struct vmx_pt_filter_iprs filter_iprs;
+#ifdef CONFIG_REDQUEEN
+
+	if(GET_GLOBAL_STATE()->patches_disable_pending){
+		patcher_t* patcher = get_redqueen_patch_state();
+		pt_disable_patches(patcher);
+		GET_GLOBAL_STATE()->patches_disable_pending = false;
+	}
+
+	if(GET_GLOBAL_STATE()->patches_enable_pending){
+		patcher_t* patcher = get_redqueen_patch_state();
+		pt_enable_patches(patcher);
+		GET_GLOBAL_STATE()->patches_enable_pending = false;
+	}
+
+	if(GET_GLOBAL_STATE()->redqueen_enable_pending){
+		if (GET_GLOBAL_STATE()->redqueen_state){
+			enable_rq_intercept_mode(GET_GLOBAL_STATE()->redqueen_state);
+		}
+		GET_GLOBAL_STATE()->redqueen_enable_pending = false;
+	}
+
+	if(GET_GLOBAL_STATE()->redqueen_disable_pending){
+		if (GET_GLOBAL_STATE()->redqueen_state){
+			disable_rq_intercept_mode(GET_GLOBAL_STATE()->redqueen_state);
+		}
+		GET_GLOBAL_STATE()->redqueen_disable_pending = false;
+	}
+#endif
+	if (!cpu->pt_fd) {
+		cpu->pt_fd = kvm_vcpu_ioctl(cpu, KVM_VMX_PT_SETUP_FD, (unsigned long)0);
+		ret = ioctl(cpu->pt_fd, KVM_VMX_PT_GET_TOPA_SIZE, (unsigned long)0x0);
+		
+		cpu->pt_mmap = mmap((void*)PT_BUFFER_MMAP_ADDR, ret, PROT_READ|PROT_WRITE, MAP_SHARED, cpu->pt_fd, 0);
+		assert(cpu->pt_mmap != (void*)0xFFFFFFFFFFFFFFFF);
+		assert(mmap(cpu->pt_mmap+ret, 0x1000, PROT_READ|PROT_WRITE, MAP_ANONYMOUS | MAP_FIXED | MAP_PRIVATE, -1, 0) == (void*)(cpu->pt_mmap+ret)); //;!= (void*)0xFFFFFFFFFFFFFFFF); // add an extra page to have enough space for an additional PT_TRACE_END byte  
+		    
+		memset(cpu->pt_mmap+ret, 0x55, 0x1000);
+	}
+	
+	if (cpu->pt_cmd){
+		switch(cpu->pt_cmd){
+			case KVM_VMX_PT_ENABLE:
+				if (cpu->pt_fd){
+					ioctl(cpu->pt_fd, KVM_VMX_PT_CHECK_TOPA_OVERFLOW, (unsigned long)0);
+					if (!ioctl(cpu->pt_fd, cpu->pt_cmd, 0)){
+						cpu->pt_enabled = true;
+					}
+				}
+				break;
+			case KVM_VMX_PT_DISABLE:
+				if (cpu->pt_fd){
+					ret = ioctl(cpu->pt_fd, cpu->pt_cmd, 0);
+					if (ret > 0){
+						pt_dump(cpu, ret);
+						cpu->pt_enabled = false;
+					}
+				}
+				break;
+			
+			/* ip filtering configuration */	
+			case KVM_VMX_PT_CONFIGURE_ADDR0:
+			case KVM_VMX_PT_CONFIGURE_ADDR1:
+			case KVM_VMX_PT_CONFIGURE_ADDR2:
+			case KVM_VMX_PT_CONFIGURE_ADDR3:
+				filter_iprs.a = GET_GLOBAL_STATE()->pt_ip_filter_a[(cpu->pt_cmd)-KVM_VMX_PT_CONFIGURE_ADDR0];
+	   		filter_iprs.b = GET_GLOBAL_STATE()->pt_ip_filter_b[(cpu->pt_cmd)-KVM_VMX_PT_CONFIGURE_ADDR0];
+				ret = pt_ioctl(cpu->pt_fd, cpu->pt_cmd, (unsigned long)&filter_iprs);
+				break;
+			case KVM_VMX_PT_ENABLE_ADDR0:
+			case KVM_VMX_PT_ENABLE_ADDR1:
+			case KVM_VMX_PT_ENABLE_ADDR2:
+			case KVM_VMX_PT_ENABLE_ADDR3:
+				ret = pt_ioctl(cpu->pt_fd, cpu->pt_cmd, (unsigned long)0);
+				break;
+			case KVM_VMX_PT_CONFIGURE_CR3:
+				ret = pt_ioctl(cpu->pt_fd, cpu->pt_cmd, GET_GLOBAL_STATE()->pt_c3_filter);
+				break;
+			case KVM_VMX_PT_ENABLE_CR3:
+				ret = pt_ioctl(cpu->pt_fd, cpu->pt_cmd, (unsigned long)0);
+				break;
+			default:
+				if (cpu->pt_fd){
+					ioctl(cpu->pt_fd, cpu->pt_cmd, 0);  
+				}
+				break;
+			}
+		cpu->pt_cmd = 0;
+		cpu->pt_ret = 0;
+	}
+	pthread_mutex_unlock(&pt_dump_mutex);
+}
+
+void pt_handle_overflow(CPUState *cpu){
+	pthread_mutex_lock(&pt_dump_mutex);
+	int overflow = ioctl(cpu->pt_fd, KVM_VMX_PT_CHECK_TOPA_OVERFLOW, (unsigned long)0);
+	if (overflow > 0){
+		pt_dump(cpu, overflow);
+	}  
+	
+	pthread_mutex_unlock(&pt_dump_mutex);
+}
+
+void pt_post_kvm_run(CPUState *cpu){
+	pt_handle_overflow(cpu);
+}
diff --new-file -ur qemu/pt.h QEMU-PT/pt.h
--- qemu/pt.h	1970-01-01 01:00:00.000000000 +0100
+++ QEMU-PT/pt.h	2021-08-24 21:54:56.038586388 +0200
@@ -0,0 +1,50 @@
+/*
+
+Copyright (C) 2017 Sergej Schumilo
+
+This file is part of QEMU-PT (kAFL).
+
+QEMU-PT is free software: you can redistribute it and/or modify
+it under the terms of the GNU General Public License as published by
+the Free Software Foundation, either version 2 of the License, or
+(at your option) any later version.
+
+QEMU-PT is distributed in the hope that it will be useful,
+but WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with QEMU-PT.  If not, see <http://www.gnu.org/licenses/>.
+
+*/
+
+#ifndef PT_H
+#define PT_H
+
+void pt_sync(void);
+void pt_reset_bitmap(void);
+void pt_setup_bitmap(void* ptr);
+
+int pt_enable(CPUState *cpu, bool hmp_mode);
+int pt_disable(CPUState *cpu, bool hmp_mode);
+#ifdef CONFIG_REDQUEEN
+int pt_enable_ip_filtering(CPUState *cpu, uint8_t addrn, bool redqueen, bool hmp_mode);
+#else
+int pt_enable_ip_filtering(CPUState *cpu, uint8_t addrn, uint64_t filter[4][2], bool hmp_mode);
+#endif
+int pt_disable_ip_filtering(CPUState *cpu, uint8_t addrn, bool hmp_mode);
+int pt_set_cr3(CPUState *cpu, uint64_t val, bool hmp_mode);
+
+void pt_kvm_init(CPUState *cpu);
+void pt_pre_kvm_run(CPUState *cpu);
+void pt_post_kvm_run(CPUState *cpu);
+
+void pt_handle_overflow(CPUState *cpu);
+void pt_dump(CPUState *cpu, int bytes);
+void pt_bitmap(uint64_t from, uint64_t to);
+
+void pt_open_pt_trace_file(char* filename);
+void pt_trucate_pt_trace_file(void);
+#endif
+
diff --new-file -ur qemu/qemu-img.c QEMU-PT/qemu-img.c
--- qemu/qemu-img.c	2021-08-24 12:37:27.000000000 +0200
+++ QEMU-PT/qemu-img.c	2021-08-24 21:54:55.686586600 +0200
@@ -49,6 +49,21 @@
 #include "crypto/init.h"
 #include "trace/control.h"
 
+#include "pt/block_cow.h"
+
+cow_cache_t* cow_cache_new(const char* filename){
+    cow_cache_t* self = malloc(sizeof(cow_cache_t));
+    self->enabled = false;
+    return self;
+}
+
+void cow_cache_read_entry(void* opaque){
+}
+
+void cow_cache_write_entry(void* opaque){
+}
+
+
 #define QEMU_IMG_VERSION "qemu-img version " QEMU_FULL_VERSION \
                           "\n" QEMU_COPYRIGHT "\n"
 
diff --new-file -ur qemu/qemu-io.c QEMU-PT/qemu-io.c
--- qemu/qemu-io.c	2021-08-24 12:37:27.000000000 +0200
+++ QEMU-PT/qemu-io.c	2021-08-24 21:54:55.938586449 +0200
@@ -34,6 +34,20 @@
 #include "crypto/init.h"
 #include "qemu-version.h"
 
+#include "pt/block_cow.h"
+
+cow_cache_t* cow_cache_new(const char* filename){
+    cow_cache_t* self = malloc(sizeof(cow_cache_t));
+    self->enabled = false;
+    return self;
+}
+
+void cow_cache_read_entry(void* opaque){
+}
+
+void cow_cache_write_entry(void* opaque){
+}
+
 #define CMD_NOFILE_OK   0x01
 
 static BlockBackend *qemuio_blk;
diff --new-file -ur qemu/qemu-nbd.c QEMU-PT/qemu-nbd.c
--- qemu/qemu-nbd.c	2021-08-24 12:37:27.000000000 +0200
+++ QEMU-PT/qemu-nbd.c	2021-08-24 21:54:55.686586600 +0200
@@ -45,6 +45,20 @@
 #include "trace/control.h"
 #include "qemu-version.h"
 
+#include "pt/block_cow.h"
+
+cow_cache_t* cow_cache_new(const char* filename){
+    cow_cache_t* self = malloc(sizeof(cow_cache_t));
+    self->enabled = false;
+    return self;
+}
+
+void cow_cache_read_entry(void* opaque){
+}
+
+void cow_cache_write_entry(void* opaque){
+}
+
 #ifdef __linux__
 #define HAVE_NBD_DEVICE 1
 #else
diff --new-file -ur qemu/qemu-options.hx QEMU-PT/qemu-options.hx
--- qemu/qemu-options.hx	2021-08-24 12:37:27.000000000 +0200
+++ QEMU-PT/qemu-options.hx	2021-08-24 21:54:56.038586388 +0200
@@ -4254,6 +4254,16 @@
 Enable FIPS 140-2 compliance mode.
 ETEXI
 
+#ifdef CONFIG_PROCESSOR_TRACE
+DEF("fast_vm_reload", HAS_ARG, QEMU_OPTION_fast_vm_reload,
+    "-fast_vm_reload    BLA BLA\n", QEMU_ARCH_ALL)
+STEXI
+@item -fast_vm_reload
+@findex -fast_vm_reload
+fast_vm_reload.
+ETEXI
+#endif
+
 HXCOMM Deprecated by -accel tcg
 DEF("no-kvm", 0, QEMU_OPTION_no_kvm, "", QEMU_ARCH_I386)
 
diff --new-file -ur qemu/roms/vgabios/config.mak QEMU-PT/roms/vgabios/config.mak
--- qemu/roms/vgabios/config.mak	2021-08-24 12:37:27.000000000 +0200
+++ QEMU-PT/roms/vgabios/config.mak	2021-08-24 21:54:55.866586492 +0200
@@ -1,5 +1,5 @@
 # Automatically generated by configure - do not modify
-SRC_PATH=/home/kafl/QEMU-PT_4.2.0/roms/vgabios
+SRC_PATH=/home/user/nyx_release/QEMU-PT/roms/vgabios
 AS=as
 CCAS=cc
 CC=cc
diff --new-file -ur qemu/target/i386/cpu.c QEMU-PT/target/i386/cpu.c
--- qemu/target/i386/cpu.c	2021-08-24 12:37:27.000000000 +0200
+++ QEMU-PT/target/i386/cpu.c	2021-08-24 21:54:55.734586571 +0200
@@ -1961,6 +1961,98 @@
         .model_id = "Common KVM processor"
     },
     {
+        .name = "kAFL64-Hypervisor",
+        .level = 0xd,
+        .vendor = CPUID_VENDOR_INTEL,
+        .family = 6,
+        .model = 60,
+        .stepping = 4,
+        .features[FEAT_1_EDX] =
+            CPUID_VME | CPUID_SSE2 | CPUID_SSE | CPUID_FXSR | CPUID_MMX |
+            CPUID_CLFLUSH | CPUID_PSE36 | CPUID_PAT | CPUID_CMOV | CPUID_MCA |
+            CPUID_PGE | CPUID_MTRR | CPUID_SEP | CPUID_APIC | CPUID_CX8 |
+            CPUID_MCE | CPUID_PAE | CPUID_MSR | CPUID_TSC | CPUID_PSE |
+            CPUID_DE | CPUID_FP87,
+        .features[FEAT_1_ECX] = 
+            CPUID_EXT_AVX | CPUID_EXT_XSAVE | CPUID_EXT_AES |
+            CPUID_EXT_POPCNT | CPUID_EXT_X2APIC | CPUID_EXT_SSE42 |
+            CPUID_EXT_SSE41 | CPUID_EXT_CX16 | CPUID_EXT_SSSE3 |
+            CPUID_EXT_PCLMULQDQ | CPUID_EXT_SSE3 |
+            CPUID_EXT_TSC_DEADLINE_TIMER | CPUID_EXT_FMA | CPUID_EXT_MOVBE |
+            CPUID_EXT_PCID | CPUID_EXT_F16C, // | CPUID_EXT_RDRAND, /* RDRAND breaks perl fuzzing (don't know why) */
+        .features[FEAT_8000_0001_EDX] =
+            CPUID_EXT2_LM | CPUID_EXT2_RDTSCP | CPUID_EXT2_NX |
+            CPUID_EXT2_SYSCALL,
+        .features[FEAT_8000_0001_ECX] =
+            CPUID_EXT3_ABM | CPUID_EXT3_LAHF_LM,
+        .features[FEAT_7_0_EBX] =
+            CPUID_7_0_EBX_FSGSBASE | CPUID_7_0_EBX_BMI1 |
+            CPUID_7_0_EBX_HLE | CPUID_7_0_EBX_AVX2 | CPUID_7_0_EBX_SMEP |
+            CPUID_7_0_EBX_BMI2 | CPUID_7_0_EBX_ERMS | CPUID_7_0_EBX_INVPCID |
+            CPUID_7_0_EBX_RTM,
+        .features[FEAT_XSAVE] =
+            CPUID_XSAVE_XSAVEOPT,
+        .features[FEAT_6_EAX] =
+            CPUID_6_EAX_ARAT,
+        .features[FEAT_VMX_BASIC] = MSR_VMX_BASIC_INS_OUTS |
+             MSR_VMX_BASIC_TRUE_CTLS,
+        .features[FEAT_VMX_ENTRY_CTLS] = VMX_VM_ENTRY_IA32E_MODE |
+            VMX_VM_ENTRY_LOAD_IA32_PAT |
+             VMX_VM_ENTRY_LOAD_DEBUG_CONTROLS | VMX_VM_ENTRY_LOAD_IA32_EFER,
+        .features[FEAT_VMX_EPT_VPID_CAPS] = MSR_VMX_EPT_EXECONLY |
+             MSR_VMX_EPT_PAGE_WALK_LENGTH_4 | MSR_VMX_EPT_WB | MSR_VMX_EPT_2MB |
+             MSR_VMX_EPT_1GB | MSR_VMX_EPT_INVEPT |
+             MSR_VMX_EPT_INVEPT_SINGLE_CONTEXT | MSR_VMX_EPT_INVEPT_ALL_CONTEXT |
+             MSR_VMX_EPT_INVVPID | MSR_VMX_EPT_INVVPID_SINGLE_ADDR |
+             MSR_VMX_EPT_INVVPID_SINGLE_CONTEXT | MSR_VMX_EPT_INVVPID_ALL_CONTEXT |
+             MSR_VMX_EPT_INVVPID_SINGLE_CONTEXT_NOGLOBALS | MSR_VMX_EPT_AD_BITS,
+        .features[FEAT_VMX_EXIT_CTLS] =
+             VMX_VM_EXIT_ACK_INTR_ON_EXIT | VMX_VM_EXIT_SAVE_DEBUG_CONTROLS |
+             VMX_VM_EXIT_LOAD_IA32_PAT | VMX_VM_EXIT_LOAD_IA32_EFER |
+             VMX_VM_EXIT_SAVE_IA32_PAT | VMX_VM_EXIT_SAVE_IA32_EFER |
+             VMX_VM_EXIT_SAVE_VMX_PREEMPTION_TIMER,
+        .features[FEAT_VMX_MISC] = MSR_VMX_MISC_ACTIVITY_HLT |
+             MSR_VMX_MISC_STORE_LMA | MSR_VMX_MISC_VMWRITE_VMEXIT,
+        .features[FEAT_VMX_PINBASED_CTLS] = VMX_PIN_BASED_EXT_INTR_MASK |
+             VMX_PIN_BASED_NMI_EXITING | VMX_PIN_BASED_VIRTUAL_NMIS |
+             VMX_PIN_BASED_VMX_PREEMPTION_TIMER ,
+        .features[FEAT_VMX_PROCBASED_CTLS] = 
+            VMX_CPU_BASED_VIRTUAL_INTR_PENDING |
+             VMX_CPU_BASED_USE_TSC_OFFSETING | VMX_CPU_BASED_HLT_EXITING |
+             VMX_CPU_BASED_INVLPG_EXITING | VMX_CPU_BASED_MWAIT_EXITING |
+             VMX_CPU_BASED_RDPMC_EXITING | VMX_CPU_BASED_RDTSC_EXITING |
+             VMX_CPU_BASED_CR8_LOAD_EXITING | VMX_CPU_BASED_CR8_STORE_EXITING |
+             VMX_CPU_BASED_TPR_SHADOW | VMX_CPU_BASED_MOV_DR_EXITING |
+             VMX_CPU_BASED_UNCOND_IO_EXITING | VMX_CPU_BASED_USE_IO_BITMAPS |
+             VMX_CPU_BASED_MONITOR_EXITING | VMX_CPU_BASED_PAUSE_EXITING |
+             VMX_CPU_BASED_VIRTUAL_NMI_PENDING | VMX_CPU_BASED_USE_MSR_BITMAPS |
+             VMX_CPU_BASED_CR3_LOAD_EXITING | VMX_CPU_BASED_CR3_STORE_EXITING |
+             VMX_CPU_BASED_MONITOR_TRAP_FLAG |
+             VMX_CPU_BASED_ACTIVATE_SECONDARY_CONTROLS,
+        .features[FEAT_VMX_SECONDARY_CTLS] =
+             VMX_SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES |
+             VMX_SECONDARY_EXEC_WBINVD_EXITING | VMX_SECONDARY_EXEC_ENABLE_EPT |
+             VMX_SECONDARY_EXEC_DESC | VMX_SECONDARY_EXEC_RDTSCP |
+             VMX_SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE |
+             VMX_SECONDARY_EXEC_ENABLE_VPID | VMX_SECONDARY_EXEC_UNRESTRICTED_GUEST |
+             VMX_SECONDARY_EXEC_RDRAND_EXITING | VMX_SECONDARY_EXEC_ENABLE_INVPCID |
+             VMX_SECONDARY_EXEC_ENABLE_VMFUNC | VMX_SECONDARY_EXEC_SHADOW_VMCS,
+        .features[FEAT_VMX_VMFUNC] = MSR_VMX_VMFUNC_EPT_SWITCHING,
+        .xlevel = 0x80000008,
+        .model_id = "Intel Core (Haswell) [Hypervisor Fuzzing]",
+        .versions = (X86CPUVersionDefinition[]) {
+            { .version = 1,
+              .props = (PropValue[]) {
+                  { "vmx", "on" },
+                  { "vmx-ept", "on" },
+                  { /* end of list */ }
+              },
+            },
+            { /* end of list */ }
+        }    
+
+    },
+    {
         .name = "qemu32",
         .level = 4,
         .vendor = CPUID_VENDOR_INTEL,
diff --new-file -ur qemu/target/i386/kvm.c QEMU-PT/target/i386/kvm.c
--- qemu/target/i386/kvm.c	2021-08-24 12:37:27.000000000 +0200
+++ QEMU-PT/target/i386/kvm.c	2021-08-24 21:54:55.734586571 +0200
@@ -780,10 +780,12 @@
                        kvm_vcpu_ioctl(cs, KVM_GET_TSC_KHZ) :
                        -ENOTSUP;
         if (cur_freq <= 0 || cur_freq != env->tsc_khz) {
+            /*
             warn_report("TSC frequency mismatch between "
                         "VM (%" PRId64 " kHz) and host (%d kHz), "
                         "and TSC scaling unavailable",
                         env->tsc_khz, cur_freq);
+            */
             return r;
         }
     }
@@ -2938,6 +2940,7 @@
         return ret;
     }
 
+#ifndef CONFIG_PROCESSOR_TRACE
     if (ret < cpu->kvm_msr_buf->nmsrs) {
         struct kvm_msr_entry *e = &cpu->kvm_msr_buf->entries[ret];
         error_report("error: failed to set MSR 0x%" PRIx32 " to 0x%" PRIx64,
@@ -2945,6 +2948,7 @@
     }
 
     assert(ret == cpu->kvm_msr_buf->nmsrs);
+#endif
     return 0;
 }
 
@@ -3806,6 +3810,7 @@
 
 static int kvm_put_nested_state(X86CPU *cpu)
 {
+    //fprintf(stderr, ">>>    %s     <<<\n", __func__);
     CPUX86State *env = &cpu->env;
     int max_nested_state_len = kvm_max_nested_state_length();
 
@@ -3918,10 +3923,15 @@
     if (ret < 0) {
         return ret;
     }
-    ret = kvm_put_debugregs(x86_cpu);
+
+#ifdef CONFIG_PROCESSOR_TRACE
+   if(level < KVM_PUT_FULL_STATE_FAST){
+   // ret = kvm_put_debugregs(x86_cpu);
     if (ret < 0) {
         return ret;
     }
+   }
+#endif
     /* must be last */
     ret = kvm_guest_debug_workarounds(x86_cpu);
     if (ret < 0) {
@@ -3930,6 +3940,63 @@
     return 0;
 }
 
+#ifdef CONFIG_PROCESSOR_TRACE
+int kvm_arch_get_registers_fast(CPUState *cpu)
+{
+    X86CPU *x86_cpu = X86_CPU(cpu);
+    int ret = 0;
+    //fprintf(stderr, "%s - kvm_getput_regs\n", __func__);
+    ret = kvm_getput_regs(x86_cpu, 0);
+    if (ret < 0) {
+        fprintf(stderr, "%s - WARNING: kvm_getput_regs failed!\n", __func__);
+        return ret;
+    }
+
+    //fprintf(stderr, "%s - kvm_get_xsave\n", __func__);
+    ret = kvm_get_xsave(x86_cpu);
+    if (ret < 0) {
+        fprintf(stderr, "%s - WARNING: kvm_get_xsave failed!\n", __func__);
+        return ret;
+    }
+
+    //fprintf(stderr, "%s - kvm_get_xcrs\n", __func__);
+    ret = kvm_get_xcrs(x86_cpu);
+    if (ret < 0) {
+        fprintf(stderr, "%s - WARNING: kvm_get_xcrs failed!\n", __func__);
+        return ret;
+    }
+
+    //fprintf(stderr, "%s - kvm_get_sregs\n", __func__);
+    ret = kvm_get_sregs(x86_cpu);
+    if (ret < 0) {
+        fprintf(stderr, "%s - WARNING: kvm_get_sregs failed!\n", __func__);
+        return ret;
+    }
+
+    //fprintf(stderr, "%s - kvm_get_msrs\n", __func__);
+    ret = kvm_get_msrs(x86_cpu);
+    if (ret < 0) {
+        fprintf(stderr, "%s - WARNING: kvm_get_msrs failed!\n", __func__);
+        return ret;
+    }
+
+    //fprintf(stderr, "%s - kvm_get_mp_state\n", __func__);
+    ret = kvm_get_mp_state(x86_cpu);
+    if (ret < 0) {
+        fprintf(stderr, "%s - WARNING: kvm_get_mp_state failed!\n", __func__);
+        return ret;
+    }
+
+    //fprintf(stderr, "%s - kvm_get_apic\n", __func__);
+    ret = kvm_get_apic(x86_cpu);
+    if (ret < 0) {
+        fprintf(stderr, "%s - WARNING: kvm_get_apic failed!\n", __func__);
+        return ret;
+    }
+    return ret;
+}
+#endif
+
 int kvm_arch_get_registers(CPUState *cs)
 {
     X86CPU *cpu = X86_CPU(cs);
diff --new-file -ur qemu/util/bitmap.c QEMU-PT/util/bitmap.c
--- qemu/util/bitmap.c	2021-08-24 12:37:27.000000000 +0200
+++ QEMU-PT/util/bitmap.c	2021-08-24 21:54:55.686586600 +0200
@@ -287,6 +287,55 @@
     return dirty != 0;
 }
 
+#ifdef CONFIG_PROCESSOR_TRACE
+bool bitmap_test_atomic(unsigned long *map, long start, long nr)
+{
+    unsigned long *p = map + BIT_WORD(start);
+    const long size = start + nr;
+    int bits_to_clear = BITS_PER_LONG - (start % BITS_PER_LONG);
+    unsigned long mask_to_clear = BITMAP_FIRST_WORD_MASK(start);
+    unsigned long dirty = 0;
+    unsigned long old_bits;
+
+    assert(start >= 0 && nr >= 0);
+
+    /* First word */
+    if (nr - bits_to_clear > 0) {
+        old_bits = atomic_fetch_and(p, ULONG_MAX);
+        dirty |= old_bits & mask_to_clear;
+        nr -= bits_to_clear;
+        bits_to_clear = BITS_PER_LONG;
+        mask_to_clear = ~0UL;
+        p++;
+    }
+
+    /* Full words */
+    if (bits_to_clear == BITS_PER_LONG) {
+        while (nr >= BITS_PER_LONG) {
+            if (*p) {
+                old_bits = atomic_xchg(p, 0);
+                dirty |= old_bits;
+            }
+            nr -= BITS_PER_LONG;
+            p++;
+        }
+    }
+
+    /* Last word */
+    if (nr) {
+        mask_to_clear &= BITMAP_LAST_WORD_MASK(size);
+        old_bits = atomic_fetch_and(p, ULONG_MAX);
+        dirty |= old_bits & mask_to_clear;
+    } else {
+        if (!dirty) {
+            smp_mb();
+        }
+    }
+
+    return dirty != 0;
+}
+#endif
+
 void bitmap_copy_and_clear_atomic(unsigned long *dst, unsigned long *src,
                                   long nr)
 {
diff --new-file -ur qemu/util/main-loop.c QEMU-PT/util/main-loop.c
--- qemu/util/main-loop.c	2021-08-24 12:37:27.000000000 +0200
+++ QEMU-PT/util/main-loop.c	2021-08-24 21:54:55.686586600 +0200
@@ -88,7 +88,7 @@
     sigemptyset(&set);
     sigaddset(&set, SIG_IPI);
     sigaddset(&set, SIGIO);
-    sigaddset(&set, SIGALRM);
+    //sigaddset(&set, SIGALRM);
     sigaddset(&set, SIGBUS);
     /* SIGINT cannot be handled via signalfd, so that ^C can be used
      * to interrupt QEMU when it is being run under gdb.  SIGHUP and
diff --new-file -ur qemu/vl.c QEMU-PT/vl.c
--- qemu/vl.c	2021-08-24 12:37:27.000000000 +0200
+++ QEMU-PT/vl.c	2021-08-24 21:54:55.938586449 +0200
@@ -130,6 +130,14 @@
 #include "sysemu/iothread.h"
 #include "qemu/guest-random.h"
 
+#ifdef CONFIG_PROCESSOR_TRACE
+#include "pt.h"
+#include "pt/hypercall.h"
+#include "pt/synchronization.h"
+#include "pt/fast_vm_reload.h"
+#include "pt/state.h"
+#endif
+
 #define MAX_VIRTIO_CONSOLES 1
 
 static const char *data_dir[16];
@@ -238,6 +246,28 @@
     { .driver = "vhost-user-vga",       .flag = &default_vga       },
 };
 
+#ifdef CONFIG_PROCESSOR_TRACE
+static QemuOptsList qemu_fast_vm_reloads_opts = {
+    .name = "fast_vm_reload-opts",
+    .implied_opt_name = "order",
+    .head = QTAILQ_HEAD_INITIALIZER(qemu_fast_vm_reloads_opts.head),
+    .merge_lists = true,
+    .desc = {
+        {
+            .name = "path",
+            .type = QEMU_OPT_STRING,
+        },{
+            .name = "load",
+            .type = QEMU_OPT_BOOL,
+        },{
+            .name = "pre_path",
+            .type = QEMU_OPT_STRING,
+        },
+        {  }
+    },
+};
+#endif
+
 static QemuOptsList qemu_rtc_opts = {
     .name = "rtc",
     .head = QTAILQ_HEAD_INITIALIZER(qemu_rtc_opts.head),
@@ -1296,6 +1326,10 @@
     }
 }
 
+#ifdef CONFIG_PROCESSOR_TRACE
+char* loadvm_global = NULL;
+#endif
+
 static ShutdownCause reset_requested;
 static ShutdownCause shutdown_requested;
 static int shutdown_signal;
@@ -1470,6 +1504,14 @@
 
 void qemu_system_reset_request(ShutdownCause reason)
 {
+#ifdef CONFIG_PROCESSOR_TRACE
+   if(GET_GLOBAL_STATE()->in_fuzzing_mode){
+        fprintf(stderr, "%s!\n", __func__);
+        GET_GLOBAL_STATE()->shutdown_requested = true;
+        return;
+    }
+#endif
+
     if (no_reboot && reason != SHUTDOWN_CAUSE_SUBSYSTEM_RESET) {
         shutdown_requested = reason;
     } else {
@@ -1489,6 +1531,13 @@
 
 void qemu_system_suspend_request(void)
 {
+#ifdef CONFIG_PROCESSOR_TRACE
+    if(GET_GLOBAL_STATE()->in_fuzzing_mode){
+        fprintf(stderr, "%s!\n", __func__);
+        GET_GLOBAL_STATE()->shutdown_requested = true;
+        return;
+    }
+#endif
     if (runstate_check(RUN_STATE_SUSPENDED)) {
         return;
     }
@@ -1558,6 +1607,13 @@
 
 void qemu_system_shutdown_request(ShutdownCause reason)
 {
+#ifdef CONFIG_PROCESSOR_TRACE
+    if(GET_GLOBAL_STATE()->in_fuzzing_mode){
+        fprintf(stderr, "%s!\n", __func__);
+        GET_GLOBAL_STATE()->shutdown_requested = true;
+        return;
+    }
+#endif
     trace_qemu_system_shutdown_request(reason);
     replay_shutdown_request(reason);
     shutdown_requested = reason;
@@ -1578,6 +1634,13 @@
 
 void qemu_system_powerdown_request(void)
 {
+#ifdef CONFIG_PROCESSOR_TRACE
+    if(GET_GLOBAL_STATE()->in_fuzzing_mode){
+        fprintf(stderr, "%s!\n", __func__);
+        GET_GLOBAL_STATE()->shutdown_requested = true;
+        return;
+    }
+#endif
     trace_qemu_system_powerdown_request();
     powerdown_requested = 1;
     qemu_notify_event();
@@ -1675,8 +1738,13 @@
 
 static void version(void)
 {
+#ifdef CONFIG_PROCESSOR_TRACE
+    printf("QEMU-PT emulator version " QEMU_VERSION QEMU_PKGVERSION "  (kAFL)\n"
+           QEMU_COPYRIGHT "\n");
+#else
     printf("QEMU emulator version " QEMU_FULL_VERSION "\n"
            QEMU_COPYRIGHT "\n");
+#endif
 }
 
 static void help(int exitcode)
@@ -2597,6 +2665,27 @@
     return !object_create_initial(type, opts);
 }
 
+#ifdef CONFIG_PROCESSOR_TRACE
+static bool verifiy_snapshot_folder(const char* folder){
+    struct stat s;
+
+    if(!folder){
+        return false;
+    }
+    if(-1 != stat(folder, &s)) {
+        if(S_ISDIR(s.st_mode)) {
+            return true;
+        }
+        else{
+            error_report("fast_vm_reload: path is not a folder");
+            exit(1);
+        }
+    }
+    error_report("fast_vm_reload: path does not exist");
+    exit(1);
+}
+#endif
+
 
 static void set_memory_options(uint64_t *ram_slots, ram_addr_t *maxram_size,
                                MachineClass *mc)
@@ -2822,6 +2911,12 @@
 
 int main(int argc, char **argv, char **envp)
 {
+
+#ifdef CONFIG_PROCESSOR_TRACE
+    bool fast_vm_reload = false;
+    state_init_global();
+#endif
+
     int i;
     int snapshot, linux_boot;
     const char *initrd_filename;
@@ -2882,6 +2977,9 @@
     qemu_add_opts(&qemu_netdev_opts);
     qemu_add_opts(&qemu_nic_opts);
     qemu_add_opts(&qemu_net_opts);
+#ifdef CONFIG_PROCESSOR_TRACE
+    qemu_add_opts(&qemu_fast_vm_reloads_opts);
+#endif
     qemu_add_opts(&qemu_rtc_opts);
     qemu_add_opts(&qemu_global_opts);
     qemu_add_opts(&qemu_mon_opts);
@@ -2969,6 +3067,15 @@
                 exit(1);
             }
             switch(popt->index) {
+#ifdef CONFIG_PROCESSOR_TRACE
+            case QEMU_OPTION_fast_vm_reload:
+                opts = qemu_opts_parse_noisily(qemu_find_opts("fast_vm_reload-opts"),
+                                               optarg, true);                if (!opts) {
+                    exit(1);
+                }
+                fast_vm_reload = true;
+                break;
+#endif
             case QEMU_OPTION_cpu:
                 /* hw initialization will check this */
                 cpu_option = optarg;
@@ -3394,6 +3501,9 @@
                 break;
             case QEMU_OPTION_loadvm:
                 loadvm = optarg;
+#ifdef CONFIG_PROCESSOR_TRACE
+                loadvm_global = (char*)optarg;
+#endif
                 break;
             case QEMU_OPTION_full_screen:
                 dpy.has_full_screen = true;
@@ -3828,6 +3938,10 @@
         exit(1);
     }
 
+#ifdef CONFIG_PROCESSOR_TRACE
+    block_signals();
+#endif
+
 #ifdef CONFIG_SECCOMP
     olist = qemu_find_opts_err("sandbox", NULL);
     if (olist) {
@@ -4378,6 +4492,74 @@
     replay_checkpoint(CHECKPOINT_RESET);
     qemu_system_reset(SHUTDOWN_CAUSE_NONE);
     register_global_state();
+
+#ifdef CONFIG_PROCESSOR_TRACE
+    if (fast_vm_reload){
+        QemuOpts *opts = qemu_opts_parse_noisily(qemu_find_opts("fast_vm_reload-opts"), optarg, true);
+        const char* snapshot_path = qemu_opt_get(opts, "path");
+        const char* pre_snapshot_path = qemu_opt_get(opts, "pre_path");
+
+        /* valid arguments 
+            -> path=foo,pre_path=bar,load=off // ALLOWED
+            -> path=foo,pre_path=bar,load=on // INVALID
+            -> path=foo,load=off // ALLOWED
+            -> path=foo,load=on // ALLOWED
+            
+            -> pre_path=bar,load=off // ALLOWED
+            -> pre_path=bar,load=on // INVALID
+
+            -> load=off // ALLOWED but useless
+            -> load=on // INVALID
+        */
+
+        bool snapshot_used = verifiy_snapshot_folder(snapshot_path); 
+        bool pre_snapshot_used = verifiy_snapshot_folder(pre_snapshot_path); 
+        bool load_mode = qemu_opt_get_bool(opts, "load", false);
+
+
+        if(pre_snapshot_used && load_mode){
+            fprintf(stderr, "Invalid argument (pre_snapshot_used && load_mode)!\n");
+            exit(1);
+        }
+
+        if((!snapshot_used && !pre_snapshot_used) && load_mode){
+            fprintf(stderr, "Invalid argument ((!pre_snapshot_used && !pre_snapshot_used) && load_mode)!\n");
+            exit(1);
+        }
+
+        if(pre_snapshot_used && snapshot_used){
+            fprintf(stderr, "Loading pre image to start fuzzing...\n");
+            set_fast_reload_mode(false);
+            set_fast_reload_path(snapshot_path);
+            enable_fast_reloads();
+            fast_reload_create_from_file_pre_image(get_fast_reload_snapshot(), pre_snapshot_path, false);
+            fast_reload_destroy(get_fast_reload_snapshot());
+            GET_GLOBAL_STATE()->fast_reload_snapshot = fast_reload_new();
+        }
+        else{
+            if(pre_snapshot_used){
+                fprintf(stderr, "Preparing to create pre image...\n");
+                set_fast_reload_pre_path(pre_snapshot_path);
+                set_fast_reload_pre_image();
+            }
+            else if(snapshot_used){
+                set_fast_reload_path(snapshot_path);
+                enable_fast_reloads();
+                if (load_mode){
+                    set_fast_reload_mode(true);
+                    fprintf(stderr, "Waiting for snapshot to start fuzzing...\n");
+                    fast_reload_create_from_file(get_fast_reload_snapshot(), snapshot_path, false);
+                    cpu_synchronize_all_post_reset();
+                }
+                else{
+                    fprintf(stderr, "Booting to start fuzzing...\n");
+                    set_fast_reload_mode(false);
+                }
+            }
+        }
+    }
+#endif
+
     if (loadvm) {
         Error *local_err = NULL;
         if (load_snapshot(loadvm, &local_err) < 0) {
